{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "718aa2a7",
   "metadata": {},
   "source": [
    "# Sentiment Analysis of Donald Trump Rally Speeches\n",
    "\n",
    "This notebook performs comprehensive sentiment analysis on Donald Trump's rally speeches from 2019-2020 using **FinBERT**, a BERT model fine-tuned for financial sentiment analysis. While originally designed for financial text, FinBERT's sentiment classification (positive, negative, neutral) works well for political speech analysis.\n",
    "\n",
    "## Analysis Overview\n",
    "- **Model**: ProsusAI/finbert - Pre-trained BERT for sentiment classification\n",
    "- **Approach**: Chunk long speeches into manageable segments for BERT processing\n",
    "- **Output**: Sentiment scores (positive, negative, neutral) for each speech\n",
    "- **Insights**: Temporal trends, location-based patterns, and aggregate sentiment metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2da79cf",
   "metadata": {},
   "source": [
    "## Import Libraries\n",
    "\n",
    "Loading required libraries for deep learning, NLP, and visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6af38e03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\bokr4002\\OneDrive - NIQ\\Desktop\\GitReposLocal\\Donald-Trump-Rally-Speeches-NLP\\.venv\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "TensorFlow version: 2.20.0\n",
      "GPU Available: []\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "import math\n",
    "import tensorflow as tf\n",
    "from tqdm.notebook import tqdm\n",
    "from typing import List, Tuple, Dict\n",
    "\n",
    "from transformers import pipeline, AutoTokenizer, TFBertForSequenceClassification\n",
    "from scipy.special import softmax\n",
    "from tensorflow.python.ops.numpy_ops import np_config\n",
    "\n",
    "# Enable NumPy behavior for TensorFlow\n",
    "np_config.enable_numpy_behavior()\n",
    "\n",
    "# Set visualization styles\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['figure.dpi'] = 100\n",
    "\n",
    "print(f\"TensorFlow version: {tf.__version__}\")\n",
    "print(f\"GPU Available: {tf.config.list_physical_devices('GPU')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26318174",
   "metadata": {},
   "source": [
    "## Loading and overview of stored dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "05de699d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 35 speeches\n",
      "Date range: Jul 2019 - Sep 2020\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset prepared in Word Clouds notebook\n",
    "%store -r DT_rally_speaches_dataset\n",
    "df = DT_rally_speaches_dataset.copy()\n",
    "\n",
    "print(f\"Loaded {len(df)} speeches\")\n",
    "print(f\"Date range: {df['Month'].iloc[0]} {df['Year'].iloc[0]} - {df['Month'].iloc[-1]} {df['Year'].iloc[-1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2c9cab8c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Location</th>\n",
       "      <th>Month</th>\n",
       "      <th>Year</th>\n",
       "      <th>filename</th>\n",
       "      <th>content</th>\n",
       "      <th>Month_Num</th>\n",
       "      <th>Date</th>\n",
       "      <th>word_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Greenville</td>\n",
       "      <td>Jul</td>\n",
       "      <td>2019</td>\n",
       "      <td>GreenvilleJul17_2019.txt</td>\n",
       "      <td>Thank you very much. Thank you. Thank you. Tha...</td>\n",
       "      <td>7</td>\n",
       "      <td>2019-07-15</td>\n",
       "      <td>10605</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Cincinnati</td>\n",
       "      <td>Aug</td>\n",
       "      <td>2019</td>\n",
       "      <td>CincinnatiAug1_2019.txt</td>\n",
       "      <td>Thank you all. Thank you very much. Thank you ...</td>\n",
       "      <td>8</td>\n",
       "      <td>2019-08-15</td>\n",
       "      <td>8170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>New Hampshire</td>\n",
       "      <td>Aug</td>\n",
       "      <td>2019</td>\n",
       "      <td>NewHampshireAug15_2019.txt</td>\n",
       "      <td>Thank you very much everybody. Thank you. Wow...</td>\n",
       "      <td>8</td>\n",
       "      <td>2019-08-15</td>\n",
       "      <td>10141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Texas</td>\n",
       "      <td>Sep</td>\n",
       "      <td>2019</td>\n",
       "      <td>TexasSep23_2019.txt</td>\n",
       "      <td>Hello, Houston. I am so thrilled to be here in...</td>\n",
       "      <td>9</td>\n",
       "      <td>2019-09-15</td>\n",
       "      <td>2487</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>New Mexico</td>\n",
       "      <td>Sep</td>\n",
       "      <td>2019</td>\n",
       "      <td>NewMexicoSep16_2019.txt</td>\n",
       "      <td>Wow, thank you. Thank you, New Mexico. Thank ...</td>\n",
       "      <td>9</td>\n",
       "      <td>2019-09-15</td>\n",
       "      <td>11498</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Location Month  Year                    filename  \\\n",
       "0     Greenville   Jul  2019    GreenvilleJul17_2019.txt   \n",
       "1     Cincinnati   Aug  2019     CincinnatiAug1_2019.txt   \n",
       "2  New Hampshire   Aug  2019  NewHampshireAug15_2019.txt   \n",
       "3          Texas   Sep  2019         TexasSep23_2019.txt   \n",
       "4     New Mexico   Sep  2019     NewMexicoSep16_2019.txt   \n",
       "\n",
       "                                             content  Month_Num       Date  \\\n",
       "0  Thank you very much. Thank you. Thank you. Tha...          7 2019-07-15   \n",
       "1  Thank you all. Thank you very much. Thank you ...          8 2019-08-15   \n",
       "2   Thank you very much everybody. Thank you. Wow...          8 2019-08-15   \n",
       "3  Hello, Houston. I am so thrilled to be here in...          9 2019-09-15   \n",
       "4   Wow, thank you. Thank you, New Mexico. Thank ...          9 2019-09-15   \n",
       "\n",
       "   word_count  \n",
       "0       10605  \n",
       "1        8170  \n",
       "2       10141  \n",
       "3        2487  \n",
       "4       11498  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "490e8d61",
   "metadata": {},
   "source": [
    "## Model and tokenizer setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a21e00d2",
   "metadata": {},
   "source": [
    "We're using **FinBERT** (ProsusAI/finbert), a BERT model fine-tuned for sentiment analysis. It classifies text into three categories:\n",
    "- **Positive**: Optimistic, confident language\n",
    "- **Negative**: Critical, pessimistic language  \n",
    "- **Neutral**: Factual, balanced statements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1146d0b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ssl\n",
    "ssl._create_default_https_context = ssl._create_unverified_context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8371855d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading FinBERT model and tokenizer...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'(MaxRetryError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Max retries exceeded with url: /ProsusAI/finbert/resolve/main/tokenizer_config.json (Caused by SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1000)')))\"), '(Request ID: c1d42ebb-08ec-4f12-b9b5-83dbc8017653)')' thrown while requesting HEAD https://huggingface.co/ProsusAI/finbert/resolve/main/tokenizer_config.json\n",
      "Retrying in 1s [Retry 1/5].\n",
      "'(MaxRetryError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Max retries exceeded with url: /ProsusAI/finbert/resolve/main/tokenizer_config.json (Caused by SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1000)')))\"), '(Request ID: abbb226a-28a6-4660-b563-c8055a3ea437)')' thrown while requesting HEAD https://huggingface.co/ProsusAI/finbert/resolve/main/tokenizer_config.json\n",
      "Retrying in 2s [Retry 2/5].\n",
      "'(MaxRetryError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Max retries exceeded with url: /ProsusAI/finbert/resolve/main/tokenizer_config.json (Caused by SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1000)')))\"), '(Request ID: 6ae5b2de-c9f9-40b5-b31a-e288a5abd43d)')' thrown while requesting HEAD https://huggingface.co/ProsusAI/finbert/resolve/main/tokenizer_config.json\n",
      "Retrying in 4s [Retry 3/5].\n",
      "'(MaxRetryError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Max retries exceeded with url: /ProsusAI/finbert/resolve/main/tokenizer_config.json (Caused by SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1000)')))\"), '(Request ID: 8fc20fa7-6819-4e08-af16-81dd8b990145)')' thrown while requesting HEAD https://huggingface.co/ProsusAI/finbert/resolve/main/tokenizer_config.json\n",
      "Retrying in 8s [Retry 4/5].\n",
      "'(MaxRetryError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Max retries exceeded with url: /ProsusAI/finbert/resolve/main/tokenizer_config.json (Caused by SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1000)')))\"), '(Request ID: 46e86252-dc3d-4ec7-ba53-c50be7c48988)')' thrown while requesting HEAD https://huggingface.co/ProsusAI/finbert/resolve/main/tokenizer_config.json\n",
      "Retrying in 8s [Retry 5/5].\n",
      "'(MaxRetryError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Max retries exceeded with url: /ProsusAI/finbert/resolve/main/tokenizer_config.json (Caused by SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1000)')))\"), '(Request ID: 494186cc-5c43-4866-a31c-f2c1dd15352d)')' thrown while requesting HEAD https://huggingface.co/ProsusAI/finbert/resolve/main/tokenizer_config.json\n"
     ]
    },
    {
     "ename": "SSLError",
     "evalue": "(MaxRetryError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Max retries exceeded with url: /ProsusAI/finbert/resolve/main/tokenizer_config.json (Caused by SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1000)')))\"), '(Request ID: 494186cc-5c43-4866-a31c-f2c1dd15352d)')",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mSSLCertVerificationError\u001b[39m                  Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\bokr4002\\OneDrive - NIQ\\Desktop\\GitReposLocal\\Donald-Trump-Rally-Speeches-NLP\\.venv\\Lib\\site-packages\\urllib3\\connectionpool.py:464\u001b[39m, in \u001b[36mHTTPConnectionPool._make_request\u001b[39m\u001b[34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[39m\n\u001b[32m    463\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m464\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_validate_conn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    465\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m (SocketTimeout, BaseSSLError) \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\bokr4002\\OneDrive - NIQ\\Desktop\\GitReposLocal\\Donald-Trump-Rally-Speeches-NLP\\.venv\\Lib\\site-packages\\urllib3\\connectionpool.py:1093\u001b[39m, in \u001b[36mHTTPSConnectionPool._validate_conn\u001b[39m\u001b[34m(self, conn)\u001b[39m\n\u001b[32m   1092\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m conn.is_closed:\n\u001b[32m-> \u001b[39m\u001b[32m1093\u001b[39m     \u001b[43mconn\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1095\u001b[39m \u001b[38;5;66;03m# TODO revise this, see https://github.com/urllib3/urllib3/issues/2791\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\bokr4002\\OneDrive - NIQ\\Desktop\\GitReposLocal\\Donald-Trump-Rally-Speeches-NLP\\.venv\\Lib\\site-packages\\urllib3\\connection.py:790\u001b[39m, in \u001b[36mHTTPSConnection.connect\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    788\u001b[39m server_hostname_rm_dot = server_hostname.rstrip(\u001b[33m\"\u001b[39m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m790\u001b[39m sock_and_verified = \u001b[43m_ssl_wrap_socket_and_match_hostname\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    791\u001b[39m \u001b[43m    \u001b[49m\u001b[43msock\u001b[49m\u001b[43m=\u001b[49m\u001b[43msock\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    792\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcert_reqs\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcert_reqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    793\u001b[39m \u001b[43m    \u001b[49m\u001b[43mssl_version\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mssl_version\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    794\u001b[39m \u001b[43m    \u001b[49m\u001b[43mssl_minimum_version\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mssl_minimum_version\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    795\u001b[39m \u001b[43m    \u001b[49m\u001b[43mssl_maximum_version\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mssl_maximum_version\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    796\u001b[39m \u001b[43m    \u001b[49m\u001b[43mca_certs\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mca_certs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    797\u001b[39m \u001b[43m    \u001b[49m\u001b[43mca_cert_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mca_cert_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    798\u001b[39m \u001b[43m    \u001b[49m\u001b[43mca_cert_data\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mca_cert_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    799\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcert_file\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcert_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    800\u001b[39m \u001b[43m    \u001b[49m\u001b[43mkey_file\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mkey_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    801\u001b[39m \u001b[43m    \u001b[49m\u001b[43mkey_password\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mkey_password\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    802\u001b[39m \u001b[43m    \u001b[49m\u001b[43mserver_hostname\u001b[49m\u001b[43m=\u001b[49m\u001b[43mserver_hostname_rm_dot\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    803\u001b[39m \u001b[43m    \u001b[49m\u001b[43mssl_context\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mssl_context\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    804\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtls_in_tls\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtls_in_tls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    805\u001b[39m \u001b[43m    \u001b[49m\u001b[43massert_hostname\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43massert_hostname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    806\u001b[39m \u001b[43m    \u001b[49m\u001b[43massert_fingerprint\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43massert_fingerprint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    807\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    808\u001b[39m \u001b[38;5;28mself\u001b[39m.sock = sock_and_verified.socket\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\bokr4002\\OneDrive - NIQ\\Desktop\\GitReposLocal\\Donald-Trump-Rally-Speeches-NLP\\.venv\\Lib\\site-packages\\urllib3\\connection.py:969\u001b[39m, in \u001b[36m_ssl_wrap_socket_and_match_hostname\u001b[39m\u001b[34m(sock, cert_reqs, ssl_version, ssl_minimum_version, ssl_maximum_version, cert_file, key_file, key_password, ca_certs, ca_cert_dir, ca_cert_data, assert_hostname, assert_fingerprint, server_hostname, ssl_context, tls_in_tls)\u001b[39m\n\u001b[32m    967\u001b[39m         server_hostname = normalized\n\u001b[32m--> \u001b[39m\u001b[32m969\u001b[39m ssl_sock = \u001b[43mssl_wrap_socket\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    970\u001b[39m \u001b[43m    \u001b[49m\u001b[43msock\u001b[49m\u001b[43m=\u001b[49m\u001b[43msock\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    971\u001b[39m \u001b[43m    \u001b[49m\u001b[43mkeyfile\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkey_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    972\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcertfile\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcert_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    973\u001b[39m \u001b[43m    \u001b[49m\u001b[43mkey_password\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkey_password\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    974\u001b[39m \u001b[43m    \u001b[49m\u001b[43mca_certs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mca_certs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    975\u001b[39m \u001b[43m    \u001b[49m\u001b[43mca_cert_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mca_cert_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    976\u001b[39m \u001b[43m    \u001b[49m\u001b[43mca_cert_data\u001b[49m\u001b[43m=\u001b[49m\u001b[43mca_cert_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    977\u001b[39m \u001b[43m    \u001b[49m\u001b[43mserver_hostname\u001b[49m\u001b[43m=\u001b[49m\u001b[43mserver_hostname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    978\u001b[39m \u001b[43m    \u001b[49m\u001b[43mssl_context\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    979\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtls_in_tls\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtls_in_tls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    980\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    982\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\bokr4002\\OneDrive - NIQ\\Desktop\\GitReposLocal\\Donald-Trump-Rally-Speeches-NLP\\.venv\\Lib\\site-packages\\urllib3\\util\\ssl_.py:480\u001b[39m, in \u001b[36mssl_wrap_socket\u001b[39m\u001b[34m(sock, keyfile, certfile, cert_reqs, ca_certs, server_hostname, ssl_version, ciphers, ssl_context, ca_cert_dir, key_password, ca_cert_data, tls_in_tls)\u001b[39m\n\u001b[32m    478\u001b[39m context.set_alpn_protocols(ALPN_PROTOCOLS)\n\u001b[32m--> \u001b[39m\u001b[32m480\u001b[39m ssl_sock = \u001b[43m_ssl_wrap_socket_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43msock\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtls_in_tls\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mserver_hostname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    481\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m ssl_sock\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\bokr4002\\OneDrive - NIQ\\Desktop\\GitReposLocal\\Donald-Trump-Rally-Speeches-NLP\\.venv\\Lib\\site-packages\\urllib3\\util\\ssl_.py:524\u001b[39m, in \u001b[36m_ssl_wrap_socket_impl\u001b[39m\u001b[34m(sock, ssl_context, tls_in_tls, server_hostname)\u001b[39m\n\u001b[32m    522\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m SSLTransport(sock, ssl_context, server_hostname)\n\u001b[32m--> \u001b[39m\u001b[32m524\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mssl_context\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwrap_socket\u001b[49m\u001b[43m(\u001b[49m\u001b[43msock\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mserver_hostname\u001b[49m\u001b[43m=\u001b[49m\u001b[43mserver_hostname\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\Program Files\\Python312\\Lib\\ssl.py:455\u001b[39m, in \u001b[36mSSLContext.wrap_socket\u001b[39m\u001b[34m(self, sock, server_side, do_handshake_on_connect, suppress_ragged_eofs, server_hostname, session)\u001b[39m\n\u001b[32m    449\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mwrap_socket\u001b[39m(\u001b[38;5;28mself\u001b[39m, sock, server_side=\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m    450\u001b[39m                 do_handshake_on_connect=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m    451\u001b[39m                 suppress_ragged_eofs=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m    452\u001b[39m                 server_hostname=\u001b[38;5;28;01mNone\u001b[39;00m, session=\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m    453\u001b[39m     \u001b[38;5;66;03m# SSLSocket class handles server_hostname encoding before it calls\u001b[39;00m\n\u001b[32m    454\u001b[39m     \u001b[38;5;66;03m# ctx._wrap_socket()\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m455\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msslsocket_class\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_create\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    456\u001b[39m \u001b[43m        \u001b[49m\u001b[43msock\u001b[49m\u001b[43m=\u001b[49m\u001b[43msock\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    457\u001b[39m \u001b[43m        \u001b[49m\u001b[43mserver_side\u001b[49m\u001b[43m=\u001b[49m\u001b[43mserver_side\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    458\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdo_handshake_on_connect\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdo_handshake_on_connect\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    459\u001b[39m \u001b[43m        \u001b[49m\u001b[43msuppress_ragged_eofs\u001b[49m\u001b[43m=\u001b[49m\u001b[43msuppress_ragged_eofs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    460\u001b[39m \u001b[43m        \u001b[49m\u001b[43mserver_hostname\u001b[49m\u001b[43m=\u001b[49m\u001b[43mserver_hostname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    461\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    462\u001b[39m \u001b[43m        \u001b[49m\u001b[43msession\u001b[49m\u001b[43m=\u001b[49m\u001b[43msession\u001b[49m\n\u001b[32m    463\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\Program Files\\Python312\\Lib\\ssl.py:1041\u001b[39m, in \u001b[36mSSLSocket._create\u001b[39m\u001b[34m(cls, sock, server_side, do_handshake_on_connect, suppress_ragged_eofs, server_hostname, context, session)\u001b[39m\n\u001b[32m   1040\u001b[39m                 \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mdo_handshake_on_connect should not be specified for non-blocking sockets\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1041\u001b[39m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdo_handshake\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1042\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\Program Files\\Python312\\Lib\\ssl.py:1319\u001b[39m, in \u001b[36mSSLSocket.do_handshake\u001b[39m\u001b[34m(self, block)\u001b[39m\n\u001b[32m   1318\u001b[39m         \u001b[38;5;28mself\u001b[39m.settimeout(\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m-> \u001b[39m\u001b[32m1319\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sslobj\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdo_handshake\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1320\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
      "\u001b[31mSSLCertVerificationError\u001b[39m: [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1000)",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mSSLError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\bokr4002\\OneDrive - NIQ\\Desktop\\GitReposLocal\\Donald-Trump-Rally-Speeches-NLP\\.venv\\Lib\\site-packages\\urllib3\\connectionpool.py:787\u001b[39m, in \u001b[36mHTTPConnectionPool.urlopen\u001b[39m\u001b[34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[39m\n\u001b[32m    786\u001b[39m \u001b[38;5;66;03m# Make the request on the HTTPConnection object\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m787\u001b[39m response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_make_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    788\u001b[39m \u001b[43m    \u001b[49m\u001b[43mconn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    789\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    790\u001b[39m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    791\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    792\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    793\u001b[39m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    794\u001b[39m \u001b[43m    \u001b[49m\u001b[43mchunked\u001b[49m\u001b[43m=\u001b[49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    795\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretries\u001b[49m\u001b[43m=\u001b[49m\u001b[43mretries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    796\u001b[39m \u001b[43m    \u001b[49m\u001b[43mresponse_conn\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresponse_conn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    797\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    798\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    799\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mresponse_kw\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    800\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    802\u001b[39m \u001b[38;5;66;03m# Everything went great!\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\bokr4002\\OneDrive - NIQ\\Desktop\\GitReposLocal\\Donald-Trump-Rally-Speeches-NLP\\.venv\\Lib\\site-packages\\urllib3\\connectionpool.py:488\u001b[39m, in \u001b[36mHTTPConnectionPool._make_request\u001b[39m\u001b[34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[39m\n\u001b[32m    487\u001b[39m         new_e = _wrap_proxy_error(new_e, conn.proxy.scheme)\n\u001b[32m--> \u001b[39m\u001b[32m488\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m new_e\n\u001b[32m    490\u001b[39m \u001b[38;5;66;03m# conn.request() calls http.client.*.request, not the method in\u001b[39;00m\n\u001b[32m    491\u001b[39m \u001b[38;5;66;03m# urllib3.request. It also calls makefile (recv) on the socket.\u001b[39;00m\n",
      "\u001b[31mSSLError\u001b[39m: [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1000)",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mMaxRetryError\u001b[39m                             Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\bokr4002\\OneDrive - NIQ\\Desktop\\GitReposLocal\\Donald-Trump-Rally-Speeches-NLP\\.venv\\Lib\\site-packages\\requests\\adapters.py:644\u001b[39m, in \u001b[36mHTTPAdapter.send\u001b[39m\u001b[34m(self, request, stream, timeout, verify, cert, proxies)\u001b[39m\n\u001b[32m    643\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m644\u001b[39m     resp = \u001b[43mconn\u001b[49m\u001b[43m.\u001b[49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    645\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    646\u001b[39m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[43m=\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    647\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    648\u001b[39m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m.\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    649\u001b[39m \u001b[43m        \u001b[49m\u001b[43mredirect\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    650\u001b[39m \u001b[43m        \u001b[49m\u001b[43massert_same_host\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    651\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    652\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    653\u001b[39m \u001b[43m        \u001b[49m\u001b[43mretries\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    654\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    655\u001b[39m \u001b[43m        \u001b[49m\u001b[43mchunked\u001b[49m\u001b[43m=\u001b[49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    656\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    658\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m (ProtocolError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\bokr4002\\OneDrive - NIQ\\Desktop\\GitReposLocal\\Donald-Trump-Rally-Speeches-NLP\\.venv\\Lib\\site-packages\\urllib3\\connectionpool.py:841\u001b[39m, in \u001b[36mHTTPConnectionPool.urlopen\u001b[39m\u001b[34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[39m\n\u001b[32m    839\u001b[39m     new_e = ProtocolError(\u001b[33m\"\u001b[39m\u001b[33mConnection aborted.\u001b[39m\u001b[33m\"\u001b[39m, new_e)\n\u001b[32m--> \u001b[39m\u001b[32m841\u001b[39m retries = \u001b[43mretries\u001b[49m\u001b[43m.\u001b[49m\u001b[43mincrement\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    842\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merror\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnew_e\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_pool\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_stacktrace\u001b[49m\u001b[43m=\u001b[49m\u001b[43msys\u001b[49m\u001b[43m.\u001b[49m\u001b[43mexc_info\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[32m    843\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    844\u001b[39m retries.sleep()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\bokr4002\\OneDrive - NIQ\\Desktop\\GitReposLocal\\Donald-Trump-Rally-Speeches-NLP\\.venv\\Lib\\site-packages\\urllib3\\util\\retry.py:519\u001b[39m, in \u001b[36mRetry.increment\u001b[39m\u001b[34m(self, method, url, response, error, _pool, _stacktrace)\u001b[39m\n\u001b[32m    518\u001b[39m     reason = error \u001b[38;5;129;01mor\u001b[39;00m ResponseError(cause)\n\u001b[32m--> \u001b[39m\u001b[32m519\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m MaxRetryError(_pool, url, reason) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mreason\u001b[39;00m  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[32m    521\u001b[39m log.debug(\u001b[33m\"\u001b[39m\u001b[33mIncremented Retry for (url=\u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m): \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[33m\"\u001b[39m, url, new_retry)\n",
      "\u001b[31mMaxRetryError\u001b[39m: HTTPSConnectionPool(host='huggingface.co', port=443): Max retries exceeded with url: /ProsusAI/finbert/resolve/main/tokenizer_config.json (Caused by SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1000)')))",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mSSLError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      2\u001b[39m MODEL_CHECKPOINT = \u001b[33m'\u001b[39m\u001b[33mProsusAI/finbert\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m      4\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mLoading FinBERT model and tokenizer...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m tokenizer = \u001b[43mAutoTokenizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mMODEL_CHECKPOINT\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      6\u001b[39m model = TFBertForSequenceClassification.from_pretrained(MODEL_CHECKPOINT)\n\u001b[32m      8\u001b[39m \u001b[38;5;66;03m# Display model configuration\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\bokr4002\\OneDrive - NIQ\\Desktop\\GitReposLocal\\Donald-Trump-Rally-Speeches-NLP\\.venv\\Lib\\site-packages\\transformers\\models\\auto\\tokenization_auto.py:1073\u001b[39m, in \u001b[36mAutoTokenizer.from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, *inputs, **kwargs)\u001b[39m\n\u001b[32m   1070\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m tokenizer_class.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)\n\u001b[32m   1072\u001b[39m \u001b[38;5;66;03m# Next, let's try to use the tokenizer_config file to get the tokenizer class.\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1073\u001b[39m tokenizer_config = \u001b[43mget_tokenizer_config\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1074\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33m_commit_hash\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m tokenizer_config:\n\u001b[32m   1075\u001b[39m     kwargs[\u001b[33m\"\u001b[39m\u001b[33m_commit_hash\u001b[39m\u001b[33m\"\u001b[39m] = tokenizer_config[\u001b[33m\"\u001b[39m\u001b[33m_commit_hash\u001b[39m\u001b[33m\"\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\bokr4002\\OneDrive - NIQ\\Desktop\\GitReposLocal\\Donald-Trump-Rally-Speeches-NLP\\.venv\\Lib\\site-packages\\transformers\\models\\auto\\tokenization_auto.py:905\u001b[39m, in \u001b[36mget_tokenizer_config\u001b[39m\u001b[34m(pretrained_model_name_or_path, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, **kwargs)\u001b[39m\n\u001b[32m    902\u001b[39m     token = use_auth_token\n\u001b[32m    904\u001b[39m commit_hash = kwargs.get(\u001b[33m\"\u001b[39m\u001b[33m_commit_hash\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m905\u001b[39m resolved_config_file = \u001b[43mcached_file\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    906\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    907\u001b[39m \u001b[43m    \u001b[49m\u001b[43mTOKENIZER_CONFIG_FILE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    908\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    909\u001b[39m \u001b[43m    \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m=\u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    910\u001b[39m \u001b[43m    \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    911\u001b[39m \u001b[43m    \u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m=\u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    912\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    913\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    914\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    915\u001b[39m \u001b[43m    \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m=\u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    916\u001b[39m \u001b[43m    \u001b[49m\u001b[43m_raise_exceptions_for_gated_repo\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    917\u001b[39m \u001b[43m    \u001b[49m\u001b[43m_raise_exceptions_for_missing_entries\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    918\u001b[39m \u001b[43m    \u001b[49m\u001b[43m_raise_exceptions_for_connection_errors\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    919\u001b[39m \u001b[43m    \u001b[49m\u001b[43m_commit_hash\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcommit_hash\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    920\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    921\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m resolved_config_file \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    922\u001b[39m     logger.info(\u001b[33m\"\u001b[39m\u001b[33mCould not locate the tokenizer configuration file, will try to use the model config instead.\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\bokr4002\\OneDrive - NIQ\\Desktop\\GitReposLocal\\Donald-Trump-Rally-Speeches-NLP\\.venv\\Lib\\site-packages\\transformers\\utils\\hub.py:322\u001b[39m, in \u001b[36mcached_file\u001b[39m\u001b[34m(path_or_repo_id, filename, **kwargs)\u001b[39m\n\u001b[32m    264\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcached_file\u001b[39m(\n\u001b[32m    265\u001b[39m     path_or_repo_id: Union[\u001b[38;5;28mstr\u001b[39m, os.PathLike],\n\u001b[32m    266\u001b[39m     filename: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m    267\u001b[39m     **kwargs,\n\u001b[32m    268\u001b[39m ) -> Optional[\u001b[38;5;28mstr\u001b[39m]:\n\u001b[32m    269\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    270\u001b[39m \u001b[33;03m    Tries to locate a file in a local folder and repo, downloads and cache it if necessary.\u001b[39;00m\n\u001b[32m    271\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    320\u001b[39m \u001b[33;03m    ```\u001b[39;00m\n\u001b[32m    321\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m322\u001b[39m     file = \u001b[43mcached_files\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath_or_repo_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpath_or_repo_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilenames\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    323\u001b[39m     file = file[\u001b[32m0\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m file\n\u001b[32m    324\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m file\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\bokr4002\\OneDrive - NIQ\\Desktop\\GitReposLocal\\Donald-Trump-Rally-Speeches-NLP\\.venv\\Lib\\site-packages\\transformers\\utils\\hub.py:567\u001b[39m, in \u001b[36mcached_files\u001b[39m\u001b[34m(path_or_repo_id, filenames, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[39m\n\u001b[32m    564\u001b[39m     \u001b[38;5;66;03m# Any other Exception type should now be re-raised, in order to provide helpful error messages and break the execution flow\u001b[39;00m\n\u001b[32m    565\u001b[39m     \u001b[38;5;66;03m# (EntryNotFoundError will be treated outside this block and correctly re-raised if needed)\u001b[39;00m\n\u001b[32m    566\u001b[39m     \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e, EntryNotFoundError):\n\u001b[32m--> \u001b[39m\u001b[32m567\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[32m    569\u001b[39m resolved_files = [\n\u001b[32m    570\u001b[39m     _get_cache_file_to_return(path_or_repo_id, filename, cache_dir, revision) \u001b[38;5;28;01mfor\u001b[39;00m filename \u001b[38;5;129;01min\u001b[39;00m full_filenames\n\u001b[32m    571\u001b[39m ]\n\u001b[32m    572\u001b[39m \u001b[38;5;66;03m# If there are any missing file and the flag is active, raise\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\bokr4002\\OneDrive - NIQ\\Desktop\\GitReposLocal\\Donald-Trump-Rally-Speeches-NLP\\.venv\\Lib\\site-packages\\transformers\\utils\\hub.py:479\u001b[39m, in \u001b[36mcached_files\u001b[39m\u001b[34m(path_or_repo_id, filenames, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[39m\n\u001b[32m    476\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    477\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(full_filenames) == \u001b[32m1\u001b[39m:\n\u001b[32m    478\u001b[39m         \u001b[38;5;66;03m# This is slightly better for only 1 file\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m479\u001b[39m         \u001b[43mhf_hub_download\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    480\u001b[39m \u001b[43m            \u001b[49m\u001b[43mpath_or_repo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    481\u001b[39m \u001b[43m            \u001b[49m\u001b[43mfilenames\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    482\u001b[39m \u001b[43m            \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m==\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    483\u001b[39m \u001b[43m            \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    484\u001b[39m \u001b[43m            \u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    485\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    486\u001b[39m \u001b[43m            \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[43m=\u001b[49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    487\u001b[39m \u001b[43m            \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m=\u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    488\u001b[39m \u001b[43m            \u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m=\u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    489\u001b[39m \u001b[43m            \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    490\u001b[39m \u001b[43m            \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    491\u001b[39m \u001b[43m            \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    492\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    493\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    494\u001b[39m         snapshot_download(\n\u001b[32m    495\u001b[39m             path_or_repo_id,\n\u001b[32m    496\u001b[39m             allow_patterns=full_filenames,\n\u001b[32m   (...)\u001b[39m\u001b[32m    505\u001b[39m             local_files_only=local_files_only,\n\u001b[32m    506\u001b[39m         )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\bokr4002\\OneDrive - NIQ\\Desktop\\GitReposLocal\\Donald-Trump-Rally-Speeches-NLP\\.venv\\Lib\\site-packages\\huggingface_hub\\utils\\_validators.py:114\u001b[39m, in \u001b[36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    111\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m check_use_auth_token:\n\u001b[32m    112\u001b[39m     kwargs = smoothly_deprecate_use_auth_token(fn_name=fn.\u001b[34m__name__\u001b[39m, has_token=has_token, kwargs=kwargs)\n\u001b[32m--> \u001b[39m\u001b[32m114\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\bokr4002\\OneDrive - NIQ\\Desktop\\GitReposLocal\\Donald-Trump-Rally-Speeches-NLP\\.venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:1007\u001b[39m, in \u001b[36mhf_hub_download\u001b[39m\u001b[34m(repo_id, filename, subfolder, repo_type, revision, library_name, library_version, cache_dir, local_dir, user_agent, force_download, proxies, etag_timeout, token, local_files_only, headers, endpoint, resume_download, force_filename, local_dir_use_symlinks)\u001b[39m\n\u001b[32m    987\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m _hf_hub_download_to_local_dir(\n\u001b[32m    988\u001b[39m         \u001b[38;5;66;03m# Destination\u001b[39;00m\n\u001b[32m    989\u001b[39m         local_dir=local_dir,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1004\u001b[39m         local_files_only=local_files_only,\n\u001b[32m   1005\u001b[39m     )\n\u001b[32m   1006\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1007\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_hf_hub_download_to_cache_dir\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1008\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Destination\u001b[39;49;00m\n\u001b[32m   1009\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1010\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# File info\u001b[39;49;00m\n\u001b[32m   1011\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrepo_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrepo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1012\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1013\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1014\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1015\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# HTTP info\u001b[39;49;00m\n\u001b[32m   1016\u001b[39m \u001b[43m        \u001b[49m\u001b[43mendpoint\u001b[49m\u001b[43m=\u001b[49m\u001b[43mendpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1017\u001b[39m \u001b[43m        \u001b[49m\u001b[43metag_timeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43metag_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1018\u001b[39m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhf_headers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1019\u001b[39m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m=\u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1020\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1021\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Additional options\u001b[39;49;00m\n\u001b[32m   1022\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1023\u001b[39m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m=\u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1024\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\bokr4002\\OneDrive - NIQ\\Desktop\\GitReposLocal\\Donald-Trump-Rally-Speeches-NLP\\.venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:1070\u001b[39m, in \u001b[36m_hf_hub_download_to_cache_dir\u001b[39m\u001b[34m(cache_dir, repo_id, filename, repo_type, revision, endpoint, etag_timeout, headers, proxies, token, local_files_only, force_download)\u001b[39m\n\u001b[32m   1066\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m pointer_path\n\u001b[32m   1068\u001b[39m \u001b[38;5;66;03m# Try to get metadata (etag, commit_hash, url, size) from the server.\u001b[39;00m\n\u001b[32m   1069\u001b[39m \u001b[38;5;66;03m# If we can't, a HEAD request error is returned.\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1070\u001b[39m (url_to_download, etag, commit_hash, expected_size, xet_file_data, head_call_error) = \u001b[43m_get_metadata_or_catch_error\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1071\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrepo_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrepo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1072\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1073\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1074\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1075\u001b[39m \u001b[43m    \u001b[49m\u001b[43mendpoint\u001b[49m\u001b[43m=\u001b[49m\u001b[43mendpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1076\u001b[39m \u001b[43m    \u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m=\u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1077\u001b[39m \u001b[43m    \u001b[49m\u001b[43metag_timeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43metag_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1078\u001b[39m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1079\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1080\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1081\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstorage_folder\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstorage_folder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1082\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrelative_filename\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrelative_filename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1083\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1085\u001b[39m \u001b[38;5;66;03m# etag can be None for several reasons:\u001b[39;00m\n\u001b[32m   1086\u001b[39m \u001b[38;5;66;03m# 1. we passed local_files_only.\u001b[39;00m\n\u001b[32m   1087\u001b[39m \u001b[38;5;66;03m# 2. we don't have a connection\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1093\u001b[39m \u001b[38;5;66;03m# If the specified revision is a commit hash, look inside \"snapshots\".\u001b[39;00m\n\u001b[32m   1094\u001b[39m \u001b[38;5;66;03m# If the specified revision is a branch or tag, look inside \"refs\".\u001b[39;00m\n\u001b[32m   1095\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m head_call_error \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1096\u001b[39m     \u001b[38;5;66;03m# Couldn't make a HEAD call => let's try to find a local file\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\bokr4002\\OneDrive - NIQ\\Desktop\\GitReposLocal\\Donald-Trump-Rally-Speeches-NLP\\.venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:1543\u001b[39m, in \u001b[36m_get_metadata_or_catch_error\u001b[39m\u001b[34m(repo_id, filename, repo_type, revision, endpoint, proxies, etag_timeout, headers, token, local_files_only, relative_filename, storage_folder)\u001b[39m\n\u001b[32m   1541\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1542\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1543\u001b[39m         metadata = \u001b[43mget_hf_file_metadata\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1544\u001b[39m \u001b[43m            \u001b[49m\u001b[43murl\u001b[49m\u001b[43m=\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m=\u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43metag_timeout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mendpoint\u001b[49m\u001b[43m=\u001b[49m\u001b[43mendpoint\u001b[49m\n\u001b[32m   1545\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1546\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m EntryNotFoundError \u001b[38;5;28;01mas\u001b[39;00m http_error:\n\u001b[32m   1547\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m storage_folder \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m relative_filename \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1548\u001b[39m             \u001b[38;5;66;03m# Cache the non-existence of the file\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\bokr4002\\OneDrive - NIQ\\Desktop\\GitReposLocal\\Donald-Trump-Rally-Speeches-NLP\\.venv\\Lib\\site-packages\\huggingface_hub\\utils\\_validators.py:114\u001b[39m, in \u001b[36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    111\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m check_use_auth_token:\n\u001b[32m    112\u001b[39m     kwargs = smoothly_deprecate_use_auth_token(fn_name=fn.\u001b[34m__name__\u001b[39m, has_token=has_token, kwargs=kwargs)\n\u001b[32m--> \u001b[39m\u001b[32m114\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\bokr4002\\OneDrive - NIQ\\Desktop\\GitReposLocal\\Donald-Trump-Rally-Speeches-NLP\\.venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:1460\u001b[39m, in \u001b[36mget_hf_file_metadata\u001b[39m\u001b[34m(url, token, proxies, timeout, library_name, library_version, user_agent, headers, endpoint)\u001b[39m\n\u001b[32m   1457\u001b[39m hf_headers[\u001b[33m\"\u001b[39m\u001b[33mAccept-Encoding\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[33m\"\u001b[39m\u001b[33midentity\u001b[39m\u001b[33m\"\u001b[39m  \u001b[38;5;66;03m# prevent any compression => we want to know the real size of the file\u001b[39;00m\n\u001b[32m   1459\u001b[39m \u001b[38;5;66;03m# Retrieve metadata\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1460\u001b[39m r = \u001b[43m_request_wrapper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1461\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mHEAD\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1462\u001b[39m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[43m=\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1463\u001b[39m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhf_headers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1464\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_redirects\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   1465\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfollow_relative_redirects\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   1466\u001b[39m \u001b[43m    \u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m=\u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1467\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1468\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1469\u001b[39m hf_raise_for_status(r)\n\u001b[32m   1471\u001b[39m \u001b[38;5;66;03m# Return\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\bokr4002\\OneDrive - NIQ\\Desktop\\GitReposLocal\\Donald-Trump-Rally-Speeches-NLP\\.venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:283\u001b[39m, in \u001b[36m_request_wrapper\u001b[39m\u001b[34m(method, url, follow_relative_redirects, **params)\u001b[39m\n\u001b[32m    281\u001b[39m \u001b[38;5;66;03m# Recursively follow relative redirects\u001b[39;00m\n\u001b[32m    282\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m follow_relative_redirects:\n\u001b[32m--> \u001b[39m\u001b[32m283\u001b[39m     response = \u001b[43m_request_wrapper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    284\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    285\u001b[39m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[43m=\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    286\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfollow_relative_redirects\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    287\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    288\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    290\u001b[39m     \u001b[38;5;66;03m# If redirection, we redirect only relative paths.\u001b[39;00m\n\u001b[32m    291\u001b[39m     \u001b[38;5;66;03m# This is useful in case of a renamed repository.\u001b[39;00m\n\u001b[32m    292\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[32m300\u001b[39m <= response.status_code <= \u001b[32m399\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\bokr4002\\OneDrive - NIQ\\Desktop\\GitReposLocal\\Donald-Trump-Rally-Speeches-NLP\\.venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:306\u001b[39m, in \u001b[36m_request_wrapper\u001b[39m\u001b[34m(method, url, follow_relative_redirects, **params)\u001b[39m\n\u001b[32m    303\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m response\n\u001b[32m    305\u001b[39m \u001b[38;5;66;03m# Perform request and return if status_code is not in the retry list.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m306\u001b[39m response = \u001b[43mhttp_backoff\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m=\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    307\u001b[39m hf_raise_for_status(response)\n\u001b[32m    308\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\bokr4002\\OneDrive - NIQ\\Desktop\\GitReposLocal\\Donald-Trump-Rally-Speeches-NLP\\.venv\\Lib\\site-packages\\huggingface_hub\\utils\\_http.py:325\u001b[39m, in \u001b[36mhttp_backoff\u001b[39m\u001b[34m(method, url, max_retries, base_wait_time, max_wait_time, retry_on_exceptions, retry_on_status_codes, **kwargs)\u001b[39m\n\u001b[32m    322\u001b[39m         reset_sessions()  \u001b[38;5;66;03m# In case of SSLError it's best to reset the shared requests.Session objects\u001b[39;00m\n\u001b[32m    324\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m nb_tries > max_retries:\n\u001b[32m--> \u001b[39m\u001b[32m325\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m err\n\u001b[32m    327\u001b[39m \u001b[38;5;66;03m# Sleep for X seconds\u001b[39;00m\n\u001b[32m    328\u001b[39m logger.warning(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mRetrying in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msleep_time\u001b[38;5;132;01m}\u001b[39;00m\u001b[33ms [Retry \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnb_tries\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmax_retries\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m].\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\bokr4002\\OneDrive - NIQ\\Desktop\\GitReposLocal\\Donald-Trump-Rally-Speeches-NLP\\.venv\\Lib\\site-packages\\huggingface_hub\\utils\\_http.py:306\u001b[39m, in \u001b[36mhttp_backoff\u001b[39m\u001b[34m(method, url, max_retries, base_wait_time, max_wait_time, retry_on_exceptions, retry_on_status_codes, **kwargs)\u001b[39m\n\u001b[32m    303\u001b[39m     kwargs[\u001b[33m\"\u001b[39m\u001b[33mdata\u001b[39m\u001b[33m\"\u001b[39m].seek(io_obj_initial_pos)\n\u001b[32m    305\u001b[39m \u001b[38;5;66;03m# Perform request and return if status_code is not in the retry list.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m306\u001b[39m response = \u001b[43msession\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m=\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    307\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m response.status_code \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m retry_on_status_codes:\n\u001b[32m    308\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\bokr4002\\OneDrive - NIQ\\Desktop\\GitReposLocal\\Donald-Trump-Rally-Speeches-NLP\\.venv\\Lib\\site-packages\\requests\\sessions.py:589\u001b[39m, in \u001b[36mSession.request\u001b[39m\u001b[34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[39m\n\u001b[32m    584\u001b[39m send_kwargs = {\n\u001b[32m    585\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mtimeout\u001b[39m\u001b[33m\"\u001b[39m: timeout,\n\u001b[32m    586\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mallow_redirects\u001b[39m\u001b[33m\"\u001b[39m: allow_redirects,\n\u001b[32m    587\u001b[39m }\n\u001b[32m    588\u001b[39m send_kwargs.update(settings)\n\u001b[32m--> \u001b[39m\u001b[32m589\u001b[39m resp = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43msend_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    591\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\bokr4002\\OneDrive - NIQ\\Desktop\\GitReposLocal\\Donald-Trump-Rally-Speeches-NLP\\.venv\\Lib\\site-packages\\requests\\sessions.py:703\u001b[39m, in \u001b[36mSession.send\u001b[39m\u001b[34m(self, request, **kwargs)\u001b[39m\n\u001b[32m    700\u001b[39m start = preferred_clock()\n\u001b[32m    702\u001b[39m \u001b[38;5;66;03m# Send the request\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m703\u001b[39m r = \u001b[43madapter\u001b[49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    705\u001b[39m \u001b[38;5;66;03m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[32m    706\u001b[39m elapsed = preferred_clock() - start\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\bokr4002\\OneDrive - NIQ\\Desktop\\GitReposLocal\\Donald-Trump-Rally-Speeches-NLP\\.venv\\Lib\\site-packages\\huggingface_hub\\utils\\_http.py:95\u001b[39m, in \u001b[36mUniqueRequestIdAdapter.send\u001b[39m\u001b[34m(self, request, *args, **kwargs)\u001b[39m\n\u001b[32m     93\u001b[39m     logger.debug(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mSend: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m_curlify(request)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     94\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m95\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     96\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m requests.RequestException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m     97\u001b[39m     request_id = request.headers.get(X_AMZN_TRACE_ID)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\bokr4002\\OneDrive - NIQ\\Desktop\\GitReposLocal\\Donald-Trump-Rally-Speeches-NLP\\.venv\\Lib\\site-packages\\requests\\adapters.py:675\u001b[39m, in \u001b[36mHTTPAdapter.send\u001b[39m\u001b[34m(self, request, stream, timeout, verify, cert, proxies)\u001b[39m\n\u001b[32m    671\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m ProxyError(e, request=request)\n\u001b[32m    673\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e.reason, _SSLError):\n\u001b[32m    674\u001b[39m         \u001b[38;5;66;03m# This branch is for urllib3 v1.22 and later.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m675\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m SSLError(e, request=request)\n\u001b[32m    677\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(e, request=request)\n\u001b[32m    679\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m ClosedPoolError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[31mSSLError\u001b[39m: (MaxRetryError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Max retries exceeded with url: /ProsusAI/finbert/resolve/main/tokenizer_config.json (Caused by SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1000)')))\"), '(Request ID: 494186cc-5c43-4866-a31c-f2c1dd15352d)')"
     ]
    }
   ],
   "source": [
    "# Load FinBERT model and tokenizer\n",
    "MODEL_CHECKPOINT = 'ProsusAI/finbert'\n",
    "\n",
    "print(\"Loading FinBERT model and tokenizer...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_CHECKPOINT)\n",
    "model = TFBertForSequenceClassification.from_pretrained(MODEL_CHECKPOINT)\n",
    "\n",
    "# Display model configuration\n",
    "print(f\"\\nModel: {MODEL_CHECKPOINT}\")\n",
    "print(f\"Labels: {model.config.id2label}\")\n",
    "print(f\"Max sequence length: {tokenizer.model_max_length}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46e799cf",
   "metadata": {},
   "source": [
    "## Text Processing Functions\n",
    "\n",
    "BERT models have a maximum sequence length (512 tokens). We need to split long speeches into manageable chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9f9d596",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_text_for_bert(text: str, tokenizer, max_length: int = 510) -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Split text into chunks that fit within BERT's token limits.\n",
    "    \n",
    "    Parameters:\n",
    "        text: Input text to chunk\n",
    "        tokenizer: HuggingFace tokenizer\n",
    "        max_length: Maximum tokens per chunk (510 to leave room for [CLS] and [SEP])\n",
    "        \n",
    "    Returns:\n",
    "        List of encoded chunks ready for model input\n",
    "    \"\"\"\n",
    "    # Tokenize the full text\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    \n",
    "    # Split into chunks\n",
    "    chunks = []\n",
    "    for i in range(0, len(tokens), max_length):\n",
    "        chunk_tokens = tokens[i:i + max_length]\n",
    "        chunk_text = tokenizer.convert_tokens_to_string(chunk_tokens)\n",
    "        \n",
    "        # Encode with special tokens\n",
    "        encoding = tokenizer.encode_plus(\n",
    "            chunk_text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=max_length + 2,  # +2 for [CLS] and [SEP]\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='tf'\n",
    "        )\n",
    "        chunks.append(encoding)\n",
    "    \n",
    "    return chunks\n",
    "\n",
    "\n",
    "def analyze_sentiment(chunks: List[Dict], model) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Run sentiment analysis on text chunks.\n",
    "    \n",
    "    Parameters:\n",
    "        chunks: List of encoded text chunks\n",
    "        model: Loaded sentiment analysis model\n",
    "        \n",
    "    Returns:\n",
    "        Tuple of (all_predictions, mean_sentiment) as numpy arrays\n",
    "    \"\"\"\n",
    "    all_predictions = []\n",
    "    \n",
    "    for chunk in chunks:\n",
    "        # Get model predictions\n",
    "        outputs = model(chunk)\n",
    "        \n",
    "        # Convert logits to probabilities\n",
    "        probs = tf.nn.softmax(outputs.logits, axis=-1)\n",
    "        all_predictions.append(probs.numpy())\n",
    "    \n",
    "    # Stack all predictions\n",
    "    all_predictions = np.vstack(all_predictions)\n",
    "    \n",
    "    # Calculate mean sentiment across all chunks\n",
    "    mean_sentiment = np.mean(all_predictions, axis=0)\n",
    "    \n",
    "    return all_predictions, mean_sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aad9b431",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions for text 1 shape: (48, 3)\n",
      "Predictions for text 2 shape: (46, 3)\n",
      "Predictions for text 3 shape: (25, 3)\n",
      "Predictions for text 4 shape: (18, 3)\n",
      "Predictions for text 5 shape: (22, 3)\n",
      "Predictions for text 6 shape: (32, 3)\n",
      "Predictions for text 7 shape: (28, 3)\n",
      "Predictions for text 8 shape: (32, 3)\n",
      "Predictions for text 9 shape: (45, 3)\n",
      "Predictions for text 10 shape: (24, 3)\n",
      "Predictions for text 11 shape: (27, 3)\n",
      "Predictions for text 12 shape: (29, 3)\n",
      "Predictions for text 13 shape: (24, 3)\n",
      "Predictions for text 14 shape: (26, 3)\n",
      "Predictions for text 15 shape: (37, 3)\n",
      "Predictions for text 16 shape: (33, 3)\n",
      "Predictions for text 17 shape: (24, 3)\n",
      "Predictions for text 18 shape: (25, 3)\n",
      "Predictions for text 19 shape: (38, 3)\n",
      "Predictions for text 20 shape: (31, 3)\n",
      "Predictions for text 21 shape: (39, 3)\n",
      "Predictions for text 22 shape: (26, 3)\n",
      "Predictions for text 23 shape: (24, 3)\n",
      "Predictions for text 24 shape: (17, 3)\n",
      "Predictions for text 25 shape: (31, 3)\n",
      "Predictions for text 26 shape: (29, 3)\n",
      "Predictions for text 27 shape: (26, 3)\n",
      "Predictions for text 28 shape: (32, 3)\n",
      "Predictions for text 29 shape: (6, 3)\n",
      "Predictions for text 30 shape: (29, 3)\n",
      "Predictions for text 31 shape: (30, 3)\n",
      "Predictions for text 32 shape: (25, 3)\n",
      "Predictions for text 33 shape: (18, 3)\n",
      "Predictions for text 34 shape: (29, 3)\n",
      "Predictions for text 35 shape: (17, 3)\n"
     ]
    }
   ],
   "source": [
    "# Process all speeches with progress tracking\n",
    "print(\"🔄 Processing all speeches for sentiment analysis...\\n\")\n",
    "\n",
    "sentiment_results = []\n",
    "\n",
    "for idx, row in tqdm(df.iterrows(), total=len(df), desc=\"Analyzing speeches\"):\n",
    "    try:\n",
    "        # Chunk the speech text\n",
    "        chunks = chunk_text_for_bert(row['content'], tokenizer, max_length=510)\n",
    "        \n",
    "        # Analyze sentiment\n",
    "        chunk_predictions, mean_sentiment = analyze_sentiment(chunks, model)\n",
    "        \n",
    "        # Store results\n",
    "        sentiment_results.append({\n",
    "            'speech_idx': idx,\n",
    "            'location': row['Location'],\n",
    "            'month': row['Month'],\n",
    "            'year': row['Year'],\n",
    "            'num_chunks': len(chunks),\n",
    "            'positive': mean_sentiment[0],\n",
    "            'negative': mean_sentiment[1],\n",
    "            'neutral': mean_sentiment[2],\n",
    "            'chunk_predictions': chunk_predictions,\n",
    "            'dominant_sentiment': model.config.id2label[np.argmax(mean_sentiment)]\n",
    "        })\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\n⚠️  Error processing speech {idx} ({row['Location']}): {e}\")\n",
    "        continue\n",
    "\n",
    "print(f\"\\n✅ Successfully analyzed {len(sentiment_results)} speeches!\")\n",
    "\n",
    "# Create results DataFrame\n",
    "sentiment_df = pd.DataFrame([{k: v for k, v in r.items() if k != 'chunk_predictions'} \n",
    "                              for r in sentiment_results])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bea136f6",
   "metadata": {},
   "source": [
    "## Sentiment Analysis Results\n",
    "\n",
    "Let's examine the sentiment scores for each speech."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "379de42f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display sentiment scores\n",
    "print(\"Sentiment Scores by Speech:\\n\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for idx, row in sentiment_df.iterrows():\n",
    "    print(f\"{row['location']:.<30} ({row['month']} {row['year']})\")\n",
    "    print(f\"   Positive: {row['positive']:.3f} | Negative: {row['negative']:.3f} | Neutral: {row['neutral']:.3f}\")\n",
    "    print(f\"   Dominant: {row['dominant_sentiment']} | Chunks: {row['num_chunks']}\")\n",
    "    print()\n",
    "\n",
    "# Show DataFrame\n",
    "sentiment_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d492feb",
   "metadata": {},
   "source": [
    "## Interactive Visualizations\n",
    "\n",
    "Creating interactive charts to explore sentiment patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0c20093",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive sentiment visualization\n",
    "fig = make_subplots(\n",
    "    rows=2, cols=2,\n",
    "    subplot_titles=('Sentiment Distribution Across All Speeches',\n",
    "                    'Average Sentiment by Year',\n",
    "                    'Dominant Sentiment Count',\n",
    "                    'Sentiment Trends Over Time'),\n",
    "    specs=[[{\"type\": \"bar\"}, {\"type\": \"bar\"}],\n",
    "           [{\"type\": \"pie\"}, {\"type\": \"scatter\"}]]\n",
    ")\n",
    "\n",
    "# 1. Overall sentiment distribution (stacked bar)\n",
    "fig.add_trace(\n",
    "    go.Bar(name='Positive', x=sentiment_df['location'], y=sentiment_df['positive'],\n",
    "           marker_color='#2ecc71', showlegend=True),\n",
    "    row=1, col=1\n",
    ")\n",
    "fig.add_trace(\n",
    "    go.Bar(name='Negative', x=sentiment_df['location'], y=sentiment_df['negative'],\n",
    "           marker_color='#e74c3c', showlegend=True),\n",
    "    row=1, col=1\n",
    ")\n",
    "fig.add_trace(\n",
    "    go.Bar(name='Neutral', x=sentiment_df['location'], y=sentiment_df['neutral'],\n",
    "           marker_color='#95a5a6', showlegend=True),\n",
    "    row=1, col=1\n",
    ")\n",
    "\n",
    "# 2. Average sentiment by year\n",
    "year_avg = sentiment_df.groupby('year')[['positive', 'negative', 'neutral']].mean()\n",
    "fig.add_trace(\n",
    "    go.Bar(name='Positive', x=year_avg.index, y=year_avg['positive'],\n",
    "           marker_color='#2ecc71', showlegend=False),\n",
    "    row=1, col=2\n",
    ")\n",
    "fig.add_trace(\n",
    "    go.Bar(name='Negative', x=year_avg.index, y=year_avg['negative'],\n",
    "           marker_color='#e74c3c', showlegend=False),\n",
    "    row=1, col=2\n",
    ")\n",
    "fig.add_trace(\n",
    "    go.Bar(name='Neutral', x=year_avg.index, y=year_avg['neutral'],\n",
    "           marker_color='#95a5a6', showlegend=False),\n",
    "    row=1, col=2\n",
    ")\n",
    "\n",
    "# 3. Dominant sentiment pie chart\n",
    "sentiment_counts = sentiment_df['dominant_sentiment'].value_counts()\n",
    "fig.add_trace(\n",
    "    go.Pie(labels=sentiment_counts.index, values=sentiment_counts.values,\n",
    "           marker=dict(colors=['#2ecc71', '#e74c3c', '#95a5a6']),\n",
    "           showlegend=False),\n",
    "    row=2, col=1\n",
    ")\n",
    "\n",
    "# 4. Sentiment timeline\n",
    "fig.add_trace(\n",
    "    go.Scatter(x=sentiment_df['speech_idx'], y=sentiment_df['positive'],\n",
    "               mode='lines+markers', name='Positive',\n",
    "               line=dict(color='#2ecc71', width=2), showlegend=False),\n",
    "    row=2, col=2\n",
    ")\n",
    "fig.add_trace(\n",
    "    go.Scatter(x=sentiment_df['speech_idx'], y=sentiment_df['negative'],\n",
    "               mode='lines+markers', name='Negative',\n",
    "               line=dict(color='#e74c3c', width=2), showlegend=False),\n",
    "    row=2, col=2\n",
    ")\n",
    "fig.add_trace(\n",
    "    go.Scatter(x=sentiment_df['speech_idx'], y=sentiment_df['neutral'],\n",
    "               mode='lines+markers', name='Neutral',\n",
    "               line=dict(color='#95a5a6', width=2), showlegend=False),\n",
    "    row=2, col=2\n",
    ")\n",
    "\n",
    "# Update layout\n",
    "fig.update_layout(\n",
    "    height=900,\n",
    "    title_text=\"Comprehensive Sentiment Analysis Dashboard\",\n",
    "    showlegend=True,\n",
    "    barmode='group',\n",
    "    template='plotly_white'\n",
    ")\n",
    "\n",
    "fig.update_xaxes(tickangle=-45, row=1, col=1)\n",
    "fig.update_yaxes(title_text=\"Probability\", row=1, col=1)\n",
    "fig.update_yaxes(title_text=\"Probability\", row=1, col=2)\n",
    "fig.update_yaxes(title_text=\"Sentiment Score\", row=2, col=2)\n",
    "fig.update_xaxes(title_text=\"Speech Index\", row=2, col=2)\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa17f9a8",
   "metadata": {},
   "source": [
    "## Sentiment Heatmap\n",
    "\n",
    "Visualizing sentiment patterns across speeches and time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "210041bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sentiment heatmap\n",
    "heatmap_data = sentiment_df[['positive', 'negative', 'neutral']].T\n",
    "heatmap_data.columns = [f\"{row['location'][:15]}...\" if len(row['location']) > 15 \n",
    "                        else row['location'] \n",
    "                        for _, row in sentiment_df.iterrows()]\n",
    "\n",
    "fig = go.Figure(data=go.Heatmap(\n",
    "    z=heatmap_data.values,\n",
    "    x=heatmap_data.columns,\n",
    "    y=['Positive', 'Negative', 'Neutral'],\n",
    "    colorscale='RdYlGn',\n",
    "    text=heatmap_data.values,\n",
    "    texttemplate='%{text:.2f}',\n",
    "    textfont={\"size\": 10},\n",
    "    colorbar=dict(title=\"Probability\")\n",
    "))\n",
    "\n",
    "fig.update_layout(\n",
    "    title='Sentiment Heatmap: All Speeches',\n",
    "    xaxis_title='Speech Location',\n",
    "    yaxis_title='Sentiment Type',\n",
    "    height=400,\n",
    "    template='plotly_white'\n",
    ")\n",
    "\n",
    "fig.update_xaxes(tickangle=-45)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e2918e7",
   "metadata": {},
   "source": [
    "## Chunk-Level Sentiment Analysis\n",
    "\n",
    "Examining sentiment variation within individual speeches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7de657e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select a few interesting speeches to examine in detail\n",
    "selected_speeches = [0, 10, 20, 30]  # First, middle, and later speeches\n",
    "\n",
    "fig = make_subplots(\n",
    "    rows=2, cols=2,\n",
    "    subplot_titles=[f\"{sentiment_results[i]['location']} ({sentiment_results[i]['month']} {sentiment_results[i]['year']})\" \n",
    "                    for i in selected_speeches]\n",
    ")\n",
    "\n",
    "positions = [(1, 1), (1, 2), (2, 1), (2, 2)]\n",
    "\n",
    "for idx, (speech_idx, pos) in enumerate(zip(selected_speeches, positions)):\n",
    "    result = sentiment_results[speech_idx]\n",
    "    chunks = result['chunk_predictions']\n",
    "    \n",
    "    chunk_indices = list(range(len(chunks)))\n",
    "    \n",
    "    # Add traces for each sentiment\n",
    "    fig.add_trace(\n",
    "        go.Scatter(x=chunk_indices, y=chunks[:, 0],\n",
    "                   mode='lines+markers', name='Positive',\n",
    "                   line=dict(color='#2ecc71'), showlegend=(idx == 0)),\n",
    "        row=pos[0], col=pos[1]\n",
    "    )\n",
    "    fig.add_trace(\n",
    "        go.Scatter(x=chunk_indices, y=chunks[:, 1],\n",
    "                   mode='lines+markers', name='Negative',\n",
    "                   line=dict(color='#e74c3c'), showlegend=(idx == 0)),\n",
    "        row=pos[0], col=pos[1]\n",
    "    )\n",
    "    fig.add_trace(\n",
    "        go.Scatter(x=chunk_indices, y=chunks[:, 2],\n",
    "                   mode='lines+markers', name='Neutral',\n",
    "                   line=dict(color='#95a5a6'), showlegend=(idx == 0)),\n",
    "        row=pos[0], col=pos[1]\n",
    "    )\n",
    "    \n",
    "    fig.update_xaxes(title_text=\"Chunk Index\", row=pos[0], col=pos[1])\n",
    "    fig.update_yaxes(title_text=\"Sentiment Score\", row=pos[0], col=pos[1])\n",
    "\n",
    "fig.update_layout(\n",
    "    height=800,\n",
    "    title_text=\"Sentiment Variation Within Individual Speeches\",\n",
    "    template='plotly_white',\n",
    "    showlegend=True\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a2ae2c4",
   "metadata": {},
   "source": [
    "## Temporal Analysis: Sentiment Over Time\n",
    "\n",
    "Analyzing how sentiment evolved throughout 2019 and 2020."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33016fbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add chronological date information to sentiment_df\n",
    "month_map = {'Jan': 1, 'Feb': 2, 'Mar': 3, 'Apr': 4, 'May': 5, 'Jun': 6,\n",
    "             'Jul': 7, 'Aug': 8, 'Sep': 9, 'Oct': 10, 'Nov': 11, 'Dec': 12}\n",
    "sentiment_df['month_num'] = sentiment_df['month'].map(month_map)\n",
    "sentiment_df['date'] = pd.to_datetime(sentiment_df['year'] + '-' + \n",
    "                                       sentiment_df['month_num'].astype(str) + '-15')\n",
    "sentiment_df = sentiment_df.sort_values('date')\n",
    "\n",
    "# Create temporal visualization\n",
    "fig = go.Figure()\n",
    "\n",
    "# Add sentiment traces\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=sentiment_df['date'],\n",
    "    y=sentiment_df['positive'],\n",
    "    mode='lines+markers',\n",
    "    name='Positive',\n",
    "    line=dict(color='#2ecc71', width=3),\n",
    "    marker=dict(size=8),\n",
    "    hovertemplate='<b>%{text}</b><br>Positive: %{y:.3f}<extra></extra>',\n",
    "    text=sentiment_df['location']\n",
    "))\n",
    "\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=sentiment_df['date'],\n",
    "    y=sentiment_df['negative'],\n",
    "    mode='lines+markers',\n",
    "    name='Negative',\n",
    "    line=dict(color='#e74c3c', width=3),\n",
    "    marker=dict(size=8),\n",
    "    hovertemplate='<b>%{text}</b><br>Negative: %{y:.3f}<extra></extra>',\n",
    "    text=sentiment_df['location']\n",
    "))\n",
    "\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=sentiment_df['date'],\n",
    "    y=sentiment_df['neutral'],\n",
    "    mode='lines+markers',\n",
    "    name='Neutral',\n",
    "    line=dict(color='#95a5a6', width=3),\n",
    "    marker=dict(size=8),\n",
    "    hovertemplate='<b>%{text}</b><br>Neutral: %{y:.3f}<extra></extra>',\n",
    "    text=sentiment_df['location']\n",
    "))\n",
    "\n",
    "# Add vertical line to separate years\n",
    "fig.add_vline(x=pd.Timestamp('2020-01-01'), line_dash=\"dash\", \n",
    "              line_color=\"gray\", annotation_text=\"2020 Begins\")\n",
    "\n",
    "fig.update_layout(\n",
    "    title='Sentiment Evolution Over Time (2019-2020)',\n",
    "    xaxis_title='Date',\n",
    "    yaxis_title='Sentiment Score',\n",
    "    height=500,\n",
    "    template='plotly_white',\n",
    "    hovermode='x unified'\n",
    ")\n",
    "\n",
    "fig.show()\n",
    "\n",
    "# Calculate rolling average\n",
    "window = 3\n",
    "sentiment_df['positive_ma'] = sentiment_df['positive'].rolling(window=window, center=True).mean()\n",
    "sentiment_df['negative_ma'] = sentiment_df['negative'].rolling(window=window, center=True).mean()\n",
    "sentiment_df['neutral_ma'] = sentiment_df['neutral'].rolling(window=window, center=True).mean()\n",
    "\n",
    "# Plot with moving average\n",
    "fig2 = go.Figure()\n",
    "\n",
    "# Raw data (lighter)\n",
    "fig2.add_trace(go.Scatter(x=sentiment_df['date'], y=sentiment_df['positive'],\n",
    "                          mode='markers', name='Positive (raw)',\n",
    "                          marker=dict(color='#2ecc71', size=6, opacity=0.3),\n",
    "                          showlegend=True))\n",
    "fig2.add_trace(go.Scatter(x=sentiment_df['date'], y=sentiment_df['negative'],\n",
    "                          mode='markers', name='Negative (raw)',\n",
    "                          marker=dict(color='#e74c3c', size=6, opacity=0.3),\n",
    "                          showlegend=True))\n",
    "\n",
    "# Moving averages (bold)\n",
    "fig2.add_trace(go.Scatter(x=sentiment_df['date'], y=sentiment_df['positive_ma'],\n",
    "                          mode='lines', name=f'Positive ({window}-speech avg)',\n",
    "                          line=dict(color='#2ecc71', width=4)))\n",
    "fig2.add_trace(go.Scatter(x=sentiment_df['date'], y=sentiment_df['negative_ma'],\n",
    "                          mode='lines', name=f'Negative ({window}-speech avg)',\n",
    "                          line=dict(color='#e74c3c', width=4)))\n",
    "\n",
    "fig2.add_vline(x=pd.Timestamp('2020-01-01'), line_dash=\"dash\", \n",
    "               line_color=\"gray\", annotation_text=\"2020 Begins\")\n",
    "\n",
    "fig2.update_layout(\n",
    "    title=f'Sentiment Trends with {window}-Speech Moving Average',\n",
    "    xaxis_title='Date',\n",
    "    yaxis_title='Sentiment Score',\n",
    "    height=500,\n",
    "    template='plotly_white'\n",
    ")\n",
    "\n",
    "fig2.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0864966",
   "metadata": {},
   "source": [
    "## Year-over-Year Comparison\n",
    "\n",
    "Comparing sentiment patterns between 2019 and 2020."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74adf837",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare sentiment statistics by year\n",
    "year_stats = sentiment_df.groupby('year').agg({\n",
    "    'positive': ['mean', 'std', 'min', 'max'],\n",
    "    'negative': ['mean', 'std', 'min', 'max'],\n",
    "    'neutral': ['mean', 'std', 'min', 'max'],\n",
    "    'speech_idx': 'count'\n",
    "}).round(3)\n",
    "\n",
    "print(\"Year-over-Year Sentiment Statistics:\")\n",
    "print(\"=\"*80)\n",
    "print(year_stats)\n",
    "print()\n",
    "\n",
    "# Create box plots for sentiment distribution by year\n",
    "fig = make_subplots(\n",
    "    rows=1, cols=3,\n",
    "    subplot_titles=('Positive Sentiment', 'Negative Sentiment', 'Neutral Sentiment')\n",
    ")\n",
    "\n",
    "for year in sentiment_df['year'].unique():\n",
    "    year_data = sentiment_df[sentiment_df['year'] == year]\n",
    "    \n",
    "    fig.add_trace(\n",
    "        go.Box(y=year_data['positive'], name=year, showlegend=True,\n",
    "               marker_color='#2ecc71' if year == '2019' else '#27ae60'),\n",
    "        row=1, col=1\n",
    "    )\n",
    "    fig.add_trace(\n",
    "        go.Box(y=year_data['negative'], name=year, showlegend=False,\n",
    "               marker_color='#e74c3c' if year == '2019' else '#c0392b'),\n",
    "        row=1, col=2\n",
    "    )\n",
    "    fig.add_trace(\n",
    "        go.Box(y=year_data['neutral'], name=year, showlegend=False,\n",
    "               marker_color='#95a5a6' if year == '2019' else '#7f8c8d'),\n",
    "        row=1, col=3\n",
    "    )\n",
    "\n",
    "fig.update_layout(\n",
    "    title_text='Sentiment Distribution by Year',\n",
    "    height=400,\n",
    "    template='plotly_white',\n",
    "    showlegend=True\n",
    ")\n",
    "\n",
    "fig.update_yaxes(title_text=\"Sentiment Score\", row=1, col=1)\n",
    "fig.update_yaxes(title_text=\"Sentiment Score\", row=1, col=2)\n",
    "fig.update_yaxes(title_text=\"Sentiment Score\", row=1, col=3)\n",
    "\n",
    "fig.show()\n",
    "\n",
    "# Statistical comparison\n",
    "print(\"\\n📊 Key Insights:\")\n",
    "print(\"=\"*80)\n",
    "for year in sorted(sentiment_df['year'].unique()):\n",
    "    year_data = sentiment_df[sentiment_df['year'] == year]\n",
    "    print(f\"\\n{year}:\")\n",
    "    print(f\"  • Average Positive: {year_data['positive'].mean():.3f} (±{year_data['positive'].std():.3f})\")\n",
    "    print(f\"  • Average Negative: {year_data['negative'].mean():.3f} (±{year_data['negative'].std():.3f})\")\n",
    "    print(f\"  • Average Neutral:  {year_data['neutral'].mean():.3f} (±{year_data['neutral'].std():.3f})\")\n",
    "    print(f\"  • Speeches: {len(year_data)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "851c8a55",
   "metadata": {},
   "source": [
    "## Summary Statistics and Insights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48608a58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprehensive summary\n",
    "print(\"=\" * 80)\n",
    "print(\"📊 SENTIMENT ANALYSIS SUMMARY\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Overall statistics\n",
    "print(f\"\\n🎤 Dataset Overview:\")\n",
    "print(f\"   Total Speeches Analyzed: {len(sentiment_df)}\")\n",
    "print(f\"   Time Period: {sentiment_df['date'].min().strftime('%B %Y')} - {sentiment_df['date'].max().strftime('%B %Y')}\")\n",
    "print(f\"   Total Text Chunks Processed: {sentiment_df['num_chunks'].sum():,}\")\n",
    "print(f\"   Average Chunks per Speech: {sentiment_df['num_chunks'].mean():.1f}\")\n",
    "\n",
    "# Overall sentiment averages\n",
    "print(f\"\\n📈 Overall Sentiment Scores:\")\n",
    "print(f\"   Positive: {sentiment_df['positive'].mean():.3f} (±{sentiment_df['positive'].std():.3f})\")\n",
    "print(f\"   Negative: {sentiment_df['negative'].mean():.3f} (±{sentiment_df['negative'].std():.3f})\")\n",
    "print(f\"   Neutral:  {sentiment_df['neutral'].mean():.3f} (±{sentiment_df['neutral'].std():.3f})\")\n",
    "\n",
    "# Dominant sentiment\n",
    "dominant_counts = sentiment_df['dominant_sentiment'].value_counts()\n",
    "print(f\"\\n🎯 Dominant Sentiment Distribution:\")\n",
    "for sentiment, count in dominant_counts.items():\n",
    "    percentage = (count / len(sentiment_df)) * 100\n",
    "    print(f\"   {sentiment}: {count} speeches ({percentage:.1f}%)\")\n",
    "\n",
    "# Most/least positive speeches\n",
    "most_positive = sentiment_df.nlargest(3, 'positive')\n",
    "most_negative = sentiment_df.nlargest(3, 'negative')\n",
    "\n",
    "print(f\"\\n✨ Most Positive Speeches:\")\n",
    "for _, row in most_positive.iterrows():\n",
    "    print(f\"   • {row['location']} ({row['month']} {row['year']}): {row['positive']:.3f}\")\n",
    "\n",
    "print(f\"\\n⚠️  Most Negative Speeches:\")\n",
    "for _, row in most_negative.iterrows():\n",
    "    print(f\"   • {row['location']} ({row['month']} {row['year']}): {row['negative']:.3f}\")\n",
    "\n",
    "# Sentiment volatility (speeches with high variance in chunks)\n",
    "print(f\"\\n📊 Sentiment Variation:\")\n",
    "chunk_variances = []\n",
    "for result in sentiment_results:\n",
    "    chunks = result['chunk_predictions']\n",
    "    variance = np.var(chunks, axis=0).mean()\n",
    "    chunk_variances.append((result['location'], result['month'], result['year'], variance))\n",
    "\n",
    "chunk_variances.sort(key=lambda x: x[3], reverse=True)\n",
    "print(f\"   Speeches with Most Sentiment Variation:\")\n",
    "for location, month, year, var in chunk_variances[:3]:\n",
    "    print(f\"   • {location} ({month} {year}): variance = {var:.4f}\")\n",
    "\n",
    "print(f\"\\n   Speeches with Most Consistent Sentiment:\")\n",
    "for location, month, year, var in chunk_variances[-3:]:\n",
    "    print(f\"   • {location} ({month} {year}): variance = {var:.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e67e1749",
   "metadata": {},
   "source": [
    "## Save Results to DataFrame\n",
    "\n",
    "Adding sentiment scores to the original dataset for further analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4f3ff23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge sentiment scores back into original DataFrame\n",
    "df_with_sentiment = df.copy()\n",
    "df_with_sentiment['sentiment_positive'] = sentiment_df['positive'].values\n",
    "df_with_sentiment['sentiment_negative'] = sentiment_df['negative'].values\n",
    "df_with_sentiment['sentiment_neutral'] = sentiment_df['neutral'].values\n",
    "df_with_sentiment['dominant_sentiment'] = sentiment_df['dominant_sentiment'].values\n",
    "\n",
    "# Store the enhanced dataset\n",
    "DT_rally_speeches_with_sentiment = df_with_sentiment\n",
    "%store DT_rally_speeches_with_sentiment\n",
    "\n",
    "print(\"✅ Sentiment scores added to DataFrame!\")\n",
    "print(f\"\\nNew columns: sentiment_positive, sentiment_negative, sentiment_neutral, dominant_sentiment\")\n",
    "print(f\"\\nDataFrame shape: {df_with_sentiment.shape}\")\n",
    "print(\"\\n📁 Dataset stored as 'DT_rally_speeches_with_sentiment' for use in other notebooks\")\n",
    "\n",
    "# Display sample\n",
    "df_with_sentiment[['Location', 'Month', 'Year', 'sentiment_positive', \n",
    "                    'sentiment_negative', 'sentiment_neutral', 'dominant_sentiment']].head(10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
