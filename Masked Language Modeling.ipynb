{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b1e61527",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d9797544",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import collections\n",
    "import math\n",
    "\n",
    "from datasets import Dataset\n",
    "from transformers import AutoTokenizer, TFAutoModelForMaskedLM, DataCollatorForLanguageModeling, create_optimizer, pipeline\n",
    "from transformers.data.data_collator import tf_default_data_collator\n",
    "from transformers.keras_callbacks import PushToHubCallback\n",
    "from huggingface_hub import notebook_login"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b172c36b",
   "metadata": {},
   "source": [
    "## Dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35131fcb",
   "metadata": {},
   "source": [
    "Importing dataframe from a previous notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3b6c7ffc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Location</th>\n",
       "      <th>Month</th>\n",
       "      <th>Year</th>\n",
       "      <th>filename</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Battle Creek</td>\n",
       "      <td>Dec</td>\n",
       "      <td>2019</td>\n",
       "      <td>BattleCreekDec19_2019.txt</td>\n",
       "      <td>Thank you. Thank you. Thank you to Vice Presid...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Bemidji</td>\n",
       "      <td>Sep</td>\n",
       "      <td>2020</td>\n",
       "      <td>BemidjiSep18_2020.txt</td>\n",
       "      <td>There's a lot of people. That's great. Thank y...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Charleston</td>\n",
       "      <td>Feb</td>\n",
       "      <td>2020</td>\n",
       "      <td>CharlestonFeb28_2020.txt</td>\n",
       "      <td>Thank you. Thank you. Thank you. All I can say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Charlotte</td>\n",
       "      <td>Mar</td>\n",
       "      <td>2020</td>\n",
       "      <td>CharlotteMar2_2020.txt</td>\n",
       "      <td>I want to thank you very much. North Carolina,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Cincinnati</td>\n",
       "      <td>Aug</td>\n",
       "      <td>2019</td>\n",
       "      <td>CincinnatiAug1_2019.txt</td>\n",
       "      <td>Thank you all. Thank you very much. Thank you ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Colorador Springs</td>\n",
       "      <td>Feb</td>\n",
       "      <td>2020</td>\n",
       "      <td>ColoradorSpringsFeb20_2020.txt</td>\n",
       "      <td>Hello Colorado. We love Colorado, most beautif...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Dallas</td>\n",
       "      <td>Oct</td>\n",
       "      <td>2019</td>\n",
       "      <td>DallasOct17_2019.txt</td>\n",
       "      <td>Thank you. Thank you very much. Hello Dallas. ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Des Moines</td>\n",
       "      <td>Jan</td>\n",
       "      <td>2020</td>\n",
       "      <td>DesMoinesJan30_2020.txt</td>\n",
       "      <td>I worked so hard for this state. I worked so h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Fayetteville</td>\n",
       "      <td>Sep</td>\n",
       "      <td>2020</td>\n",
       "      <td>FayettevilleSep19_2020.txt</td>\n",
       "      <td>What a crowd, what a crowd. Get those people o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Fayetteville</td>\n",
       "      <td>Sep</td>\n",
       "      <td>2019</td>\n",
       "      <td>FayettevilleSep9_2019.txt</td>\n",
       "      <td>Thank you everybody. Thank you and Vice Presi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Freeland</td>\n",
       "      <td>Sep</td>\n",
       "      <td>2020</td>\n",
       "      <td>FreelandSep10_2020.txt</td>\n",
       "      <td>We brought you a lot of car plants, Michigan. ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Greenville</td>\n",
       "      <td>Jul</td>\n",
       "      <td>2019</td>\n",
       "      <td>GreenvilleJul17_2019.txt</td>\n",
       "      <td>Thank you very much. Thank you. Thank you. Tha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Henderson</td>\n",
       "      <td>Sep</td>\n",
       "      <td>2020</td>\n",
       "      <td>HendersonSep13_2020.txt</td>\n",
       "      <td>Thank you, thank you. Wow. Wow, and I'm thrill...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Hershey</td>\n",
       "      <td>Dec</td>\n",
       "      <td>2019</td>\n",
       "      <td>HersheyDec10_2019.txt</td>\n",
       "      <td>Well, thank you to Vice President Pence. Than...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Las Vegas</td>\n",
       "      <td>Feb</td>\n",
       "      <td>2020</td>\n",
       "      <td>LasVegasFeb21_2020.txt</td>\n",
       "      <td>Well, thank you very much. And hello Las Vegas...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Latrobe</td>\n",
       "      <td>Sep</td>\n",
       "      <td>2020</td>\n",
       "      <td>LatrobeSep3_2020.txt</td>\n",
       "      <td>So thank you Pennsylvania, very much. I'm thri...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Lexington</td>\n",
       "      <td>Nov</td>\n",
       "      <td>2019</td>\n",
       "      <td>LexingtonNov4_2019.txt</td>\n",
       "      <td>Thank you very much and thank you to the origi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Milwaukee</td>\n",
       "      <td>Jan</td>\n",
       "      <td>2020</td>\n",
       "      <td>MilwaukeeJan14_2020.txt</td>\n",
       "      <td>Well thank you very much. And I'm thrilled to ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Minden</td>\n",
       "      <td>Sep</td>\n",
       "      <td>2020</td>\n",
       "      <td>MindenSep12_2020.txt</td>\n",
       "      <td>Well, I thank you very much. So I want to star...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Minneapolis</td>\n",
       "      <td>Oct</td>\n",
       "      <td>2019</td>\n",
       "      <td>MinneapolisOct10_2019.txt</td>\n",
       "      <td>Thank you very much. Thank you, Minnesota. Thi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Mosinee</td>\n",
       "      <td>Sep</td>\n",
       "      <td>2020</td>\n",
       "      <td>MosineeSep17_2020.txt</td>\n",
       "      <td>Thank you, thank you very much. Thank you very...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>New Hampshire</td>\n",
       "      <td>Aug</td>\n",
       "      <td>2019</td>\n",
       "      <td>NewHampshireAug15_2019.txt</td>\n",
       "      <td>Thank you very much everybody. Thank you. Wow...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>New Hampshire</td>\n",
       "      <td>Aug</td>\n",
       "      <td>2020</td>\n",
       "      <td>NewHampshireAug28_2020.txt</td>\n",
       "      <td>Hello, everybody. Hello, everybody. Wow. Hello...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>New Hampshire</td>\n",
       "      <td>Feb</td>\n",
       "      <td>2020</td>\n",
       "      <td>NewHampshireFeb10_2020.txt</td>\n",
       "      <td>Hello, Manchester, and I am thrilled to be in ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>New Mexico</td>\n",
       "      <td>Sep</td>\n",
       "      <td>2019</td>\n",
       "      <td>NewMexicoSep16_2019.txt</td>\n",
       "      <td>Wow, thank you. Thank you, New Mexico. Thank ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Ohio</td>\n",
       "      <td>Sep</td>\n",
       "      <td>2020</td>\n",
       "      <td>OhioSep21_2020.txt</td>\n",
       "      <td>Wow, that's a big crowd. This is a big crowd. ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>Phoenix</td>\n",
       "      <td>Feb</td>\n",
       "      <td>2020</td>\n",
       "      <td>PhoenixFeb19_2020.txt</td>\n",
       "      <td>Thank you very much, Phoenix. We love to be b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Pittsburgh</td>\n",
       "      <td>Sep</td>\n",
       "      <td>2020</td>\n",
       "      <td>PittsburghSep22_2020.txt</td>\n",
       "      <td>Doesn't have the power. Doesn't have the stayi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>Texas</td>\n",
       "      <td>Sep</td>\n",
       "      <td>2019</td>\n",
       "      <td>TexasSep23_2019.txt</td>\n",
       "      <td>Hello, Houston. I am so thrilled to be here in...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>Toledo</td>\n",
       "      <td>Jan</td>\n",
       "      <td>2020</td>\n",
       "      <td>ToledoJan9_2020.txt</td>\n",
       "      <td>Well, thank you very much. Vice President Mike...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>Tulsa</td>\n",
       "      <td>Jun</td>\n",
       "      <td>2020</td>\n",
       "      <td>TulsaJun20_2020.txt</td>\n",
       "      <td>Thank you, thank you. So we begin, Oklahoma, w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>Tupelo</td>\n",
       "      <td>Nov</td>\n",
       "      <td>2019</td>\n",
       "      <td>TupeloNov1_2019.txt</td>\n",
       "      <td>ell, thank you very much. And hello, Tupelo. T...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>Wildwood</td>\n",
       "      <td>Jan</td>\n",
       "      <td>2020</td>\n",
       "      <td>WildwoodJan28_2020.txt</td>\n",
       "      <td>Thank you. Thank you. I love New Jersey and I'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>Salem</td>\n",
       "      <td>Sep</td>\n",
       "      <td>2020</td>\n",
       "      <td>Winston-SalemSep8_2020.txt</td>\n",
       "      <td>Well, thank you very much. Thank you. Thank yo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>Yuma</td>\n",
       "      <td>Aug</td>\n",
       "      <td>2020</td>\n",
       "      <td>YumaAug18_2020.txt</td>\n",
       "      <td>Oh, thank you very much, everybody. Thank you....</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Location Month  Year                        filename  \\\n",
       "0         Battle Creek   Dec  2019       BattleCreekDec19_2019.txt   \n",
       "1              Bemidji   Sep  2020           BemidjiSep18_2020.txt   \n",
       "2           Charleston   Feb  2020        CharlestonFeb28_2020.txt   \n",
       "3            Charlotte   Mar  2020          CharlotteMar2_2020.txt   \n",
       "4           Cincinnati   Aug  2019         CincinnatiAug1_2019.txt   \n",
       "5    Colorador Springs   Feb  2020  ColoradorSpringsFeb20_2020.txt   \n",
       "6               Dallas   Oct  2019            DallasOct17_2019.txt   \n",
       "7           Des Moines   Jan  2020         DesMoinesJan30_2020.txt   \n",
       "8         Fayetteville   Sep  2020      FayettevilleSep19_2020.txt   \n",
       "9         Fayetteville   Sep  2019       FayettevilleSep9_2019.txt   \n",
       "10            Freeland   Sep  2020          FreelandSep10_2020.txt   \n",
       "11          Greenville   Jul  2019        GreenvilleJul17_2019.txt   \n",
       "12           Henderson   Sep  2020         HendersonSep13_2020.txt   \n",
       "13             Hershey   Dec  2019           HersheyDec10_2019.txt   \n",
       "14           Las Vegas   Feb  2020          LasVegasFeb21_2020.txt   \n",
       "15             Latrobe   Sep  2020            LatrobeSep3_2020.txt   \n",
       "16           Lexington   Nov  2019          LexingtonNov4_2019.txt   \n",
       "17           Milwaukee   Jan  2020         MilwaukeeJan14_2020.txt   \n",
       "18              Minden   Sep  2020            MindenSep12_2020.txt   \n",
       "19         Minneapolis   Oct  2019       MinneapolisOct10_2019.txt   \n",
       "20             Mosinee   Sep  2020           MosineeSep17_2020.txt   \n",
       "21       New Hampshire   Aug  2019      NewHampshireAug15_2019.txt   \n",
       "22       New Hampshire   Aug  2020      NewHampshireAug28_2020.txt   \n",
       "23       New Hampshire   Feb  2020      NewHampshireFeb10_2020.txt   \n",
       "24          New Mexico   Sep  2019         NewMexicoSep16_2019.txt   \n",
       "25                Ohio   Sep  2020              OhioSep21_2020.txt   \n",
       "26             Phoenix   Feb  2020           PhoenixFeb19_2020.txt   \n",
       "27          Pittsburgh   Sep  2020        PittsburghSep22_2020.txt   \n",
       "28               Texas   Sep  2019             TexasSep23_2019.txt   \n",
       "29              Toledo   Jan  2020             ToledoJan9_2020.txt   \n",
       "30               Tulsa   Jun  2020             TulsaJun20_2020.txt   \n",
       "31              Tupelo   Nov  2019             TupeloNov1_2019.txt   \n",
       "32            Wildwood   Jan  2020          WildwoodJan28_2020.txt   \n",
       "33               Salem   Sep  2020      Winston-SalemSep8_2020.txt   \n",
       "34                Yuma   Aug  2020              YumaAug18_2020.txt   \n",
       "\n",
       "                                              content  \n",
       "0   Thank you. Thank you. Thank you to Vice Presid...  \n",
       "1   There's a lot of people. That's great. Thank y...  \n",
       "2   Thank you. Thank you. Thank you. All I can say...  \n",
       "3   I want to thank you very much. North Carolina,...  \n",
       "4   Thank you all. Thank you very much. Thank you ...  \n",
       "5   Hello Colorado. We love Colorado, most beautif...  \n",
       "6   Thank you. Thank you very much. Hello Dallas. ...  \n",
       "7   I worked so hard for this state. I worked so h...  \n",
       "8   What a crowd, what a crowd. Get those people o...  \n",
       "9    Thank you everybody. Thank you and Vice Presi...  \n",
       "10  We brought you a lot of car plants, Michigan. ...  \n",
       "11  Thank you very much. Thank you. Thank you. Tha...  \n",
       "12  Thank you, thank you. Wow. Wow, and I'm thrill...  \n",
       "13   Well, thank you to Vice President Pence. Than...  \n",
       "14  Well, thank you very much. And hello Las Vegas...  \n",
       "15  So thank you Pennsylvania, very much. I'm thri...  \n",
       "16  Thank you very much and thank you to the origi...  \n",
       "17  Well thank you very much. And I'm thrilled to ...  \n",
       "18  Well, I thank you very much. So I want to star...  \n",
       "19  Thank you very much. Thank you, Minnesota. Thi...  \n",
       "20  Thank you, thank you very much. Thank you very...  \n",
       "21   Thank you very much everybody. Thank you. Wow...  \n",
       "22  Hello, everybody. Hello, everybody. Wow. Hello...  \n",
       "23  Hello, Manchester, and I am thrilled to be in ...  \n",
       "24   Wow, thank you. Thank you, New Mexico. Thank ...  \n",
       "25  Wow, that's a big crowd. This is a big crowd. ...  \n",
       "26   Thank you very much, Phoenix. We love to be b...  \n",
       "27  Doesn't have the power. Doesn't have the stayi...  \n",
       "28  Hello, Houston. I am so thrilled to be here in...  \n",
       "29  Well, thank you very much. Vice President Mike...  \n",
       "30  Thank you, thank you. So we begin, Oklahoma, w...  \n",
       "31  ell, thank you very much. And hello, Tupelo. T...  \n",
       "32  Thank you. Thank you. I love New Jersey and I'...  \n",
       "33  Well, thank you very much. Thank you. Thank yo...  \n",
       "34  Oh, thank you very much, everybody. Thank you....  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%store -r DT_rally_speaches_dataset\n",
    "df = DT_rally_speaches_dataset\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0b52ea6",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "91fabcbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at distilbert-base-uncased were not used when initializing TFDistilBertForMaskedLM: ['activation_13']\n",
      "- This IS expected if you are initializing TFDistilBertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFDistilBertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFDistilBertForMaskedLM were initialized from the model checkpoint at distilbert-base-uncased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFDistilBertForMaskedLM for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"tf_distil_bert_for_masked_lm\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " distilbert (TFDistilBertMai  multiple                 66362880  \n",
      " nLayer)                                                         \n",
      "                                                                 \n",
      " vocab_transform (Dense)     multiple                  590592    \n",
      "                                                                 \n",
      " vocab_layer_norm (LayerNorm  multiple                 1536      \n",
      " alization)                                                      \n",
      "                                                                 \n",
      " vocab_projector (TFDistilBe  multiple                 23866170  \n",
      " rtLMHead)                                                       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 66,985,530\n",
      "Trainable params: 66,985,530\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_checkpoint = \"distilbert-base-uncased\"\n",
    "model = TFAutoModelForMaskedLM.from_pretrained(model_checkpoint)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "445f7290",
   "metadata": {},
   "source": [
    "## Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4cafc44a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac13193c",
   "metadata": {},
   "source": [
    "Let's pick a text to test the base model on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a4d6d7f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# text = \"This is a great [MASK].\"\n",
    "text = \"Make [MASK] great\"\n",
    "# text = \"[MASK] virus\"\n",
    "# text = \"kung [MASK]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3130c8b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at distilbert-base-uncased were not used when initializing TFDistilBertForMaskedLM: ['activation_13']\n",
      "- This IS expected if you are initializing TFDistilBertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFDistilBertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFDistilBertForMaskedLM were initialized from the model checkpoint at distilbert-base-uncased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFDistilBertForMaskedLM for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "base_model = model = TFAutoModelForMaskedLM.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "db7e3136",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Make yourself great\n",
      ">>> Make it great\n",
      ">>> Make thee great\n",
      ">>> Make me great\n",
      ">>> Make yourselves great\n"
     ]
    }
   ],
   "source": [
    "inputs = tokenizer(text, return_tensors=\"np\")\n",
    "token_logits = base_model(**inputs).logits\n",
    "# Find the location of [MASK] and extract its logits\n",
    "mask_token_index = np.argwhere(inputs[\"input_ids\"] == tokenizer.mask_token_id)[0, 1]\n",
    "mask_token_logits = token_logits[0, mask_token_index, :]\n",
    "# Pick the [MASK] candidates with the highest logits\n",
    "# We negate the array before argsort to get the largest, not the smallest, logits\n",
    "top_5_tokens = np.argsort(-mask_token_logits)[:5].tolist()\n",
    "\n",
    "for token in top_5_tokens:\n",
    "    print(f\">>> {text.replace(tokenizer.mask_token, tokenizer.decode([token]))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66de50d5",
   "metadata": {},
   "source": [
    "The tokenizer works best using a dataset so let's convert the pandas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6bacd68a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['Location', 'Month', 'Year', 'filename', 'content'],\n",
       "    num_rows: 35\n",
       "})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = Dataset.from_pandas(df)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d87a7441",
   "metadata": {},
   "source": [
    "Let's set up a tokenize function that can then be mapped onto the dataset. If using a fast tokenizer we can also use the word ids for whole word masking later on. We can also drop the column that will not be required for this task. \n",
    "\n",
    "Since we are working with very long texts we cannot truncate the excess since that will lose us most of the dataset. Instead we can split the texts into batches small enough to fit the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8ad1462c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/35 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (24291 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input_ids', 'attention_mask', 'word_ids'],\n",
       "    num_rows: 35\n",
       "})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def tokenize_function(examples):\n",
    "    result = tokenizer(examples[\"content\"])\n",
    "    if tokenizer.is_fast:\n",
    "        result[\"word_ids\"] = [result.word_ids(i) for i in range(len(result[\"input_ids\"]))]\n",
    "    return result\n",
    "\n",
    "\n",
    "# Use batched=True to activate fast multithreading!\n",
    "tokenized_dataset = dataset.map(\n",
    "    tokenize_function, batched=True, remove_columns=['Location', 'Month', 'Year', 'filename', 'content']\n",
    ")\n",
    "tokenized_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bca01e5e",
   "metadata": {},
   "source": [
    "Let's check the model's max context length in order to determine the size of the chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "96913d1c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "512"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.model_max_length"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec48a604",
   "metadata": {},
   "source": [
    "The capabilities of your machine is also a factor when picking the chunk size. If the machine is lacking in memmory it might be better to pick a smaller number than what the model is capable of handling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0675e8a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_size = 256"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e2e3aa7",
   "metadata": {},
   "source": [
    "Let's check the number of tokens per speech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "473546d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'>>> Rally 0 length: 24291'\n",
      "'>>> Rally 1 length: 22976'\n",
      "'>>> Rally 2 length: 12491'\n",
      "'>>> Rally 3 length: 8802'\n",
      "'>>> Rally 4 length: 10662'\n",
      "'>>> Rally 5 length: 15759'\n",
      "'>>> Rally 6 length: 13867'\n",
      "'>>> Rally 7 length: 15730'\n",
      "'>>> Rally 8 length: 22452'\n",
      "'>>> Rally 9 length: 12007'\n",
      "'>>> Rally 10 length: 13599'\n",
      "'>>> Rally 11 length: 14241'\n",
      "'>>> Rally 12 length: 12027'\n",
      "'>>> Rally 13 length: 13050'\n",
      "'>>> Rally 14 length: 18351'\n",
      "'>>> Rally 15 length: 16629'\n",
      "'>>> Rally 16 length: 11906'\n",
      "'>>> Rally 17 length: 12482'\n",
      "'>>> Rally 18 length: 19059'\n",
      "'>>> Rally 19 length: 15646'\n",
      "'>>> Rally 20 length: 19325'\n",
      "'>>> Rally 21 length: 13165'\n",
      "'>>> Rally 22 length: 11902'\n",
      "'>>> Rally 23 length: 8570'\n",
      "'>>> Rally 24 length: 15375'\n",
      "'>>> Rally 25 length: 14479'\n",
      "'>>> Rally 26 length: 12752'\n",
      "'>>> Rally 27 length: 16218'\n",
      "'>>> Rally 28 length: 3016'\n",
      "'>>> Rally 29 length: 14459'\n",
      "'>>> Rally 30 length: 15064'\n",
      "'>>> Rally 31 length: 12457'\n",
      "'>>> Rally 32 length: 8942'\n",
      "'>>> Rally 33 length: 14664'\n",
      "'>>> Rally 34 length: 8255'\n"
     ]
    }
   ],
   "source": [
    "for idx, sample in enumerate(tokenized_dataset[\"input_ids\"]):\n",
    "    print(f\"'>>> Rally {idx} length: {len(sample)}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b071b23",
   "metadata": {},
   "source": [
    "Let's take a look at the full length of the entire dataset at once:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5455fc7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'>>> Concatenated reviews length: 494670'\n"
     ]
    }
   ],
   "source": [
    "tokenized_dataset_dict = tokenized_dataset.to_dict()\n",
    "# tokenized_dataset_dict = tokenized_dataset[:2]\n",
    "concatenated_dataset = {\n",
    "    k: sum(tokenized_dataset_dict[k], []) for k in tokenized_dataset_dict.keys()\n",
    "}\n",
    "total_length = len(concatenated_dataset[\"input_ids\"])\n",
    "print(f\"'>>> Concatenated reviews length: {total_length}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a5e46510",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 78'\n"
     ]
    }
   ],
   "source": [
    "chunks = {\n",
    "    k: [t[i : i + chunk_size] for i in range(0, total_length, chunk_size)]\n",
    "    for k, t in concatenated_dataset.items()\n",
    "}\n",
    "\n",
    "for chunk in chunks[\"input_ids\"]:\n",
    "    print(f\"'>>> Chunk length: {len(chunk)}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b06091d1",
   "metadata": {},
   "source": [
    "And now to put it all in a function to map to our dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "684ed633",
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_texts(examples):\n",
    "    # Concatenate all texts\n",
    "    concatenated_dataset = {k: sum(examples[k], []) for k in examples.keys()}\n",
    "    # Compute length of concatenated texts\n",
    "    total_length = len(concatenated_dataset[list(examples.keys())[0]])\n",
    "    # We drop the last chunk if it's smaller than chunk_size\n",
    "    total_length = (total_length // chunk_size) * chunk_size\n",
    "    # Split by chunks of max_len\n",
    "    result = {\n",
    "        k: [t[i : i + chunk_size] for i in range(0, total_length, chunk_size)]\n",
    "        for k, t in concatenated_dataset.items()\n",
    "    }\n",
    "    # Create a new labels column\n",
    "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41f93fcc",
   "metadata": {},
   "source": [
    "At the end of the group_texts function we create a labels column which is a copy of the input_ids. That is needed in masked language modeling in order to provide the ground truth for our language model to learn from.\n",
    "\n",
    "Now let's map the function to the tokenized dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9db8cfec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/35 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input_ids', 'attention_mask', 'word_ids', 'labels'],\n",
       "    num_rows: 1932\n",
       "})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm_datasets = tokenized_dataset.map(group_texts, batched=True)\n",
    "lm_datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f219fab",
   "metadata": {},
   "source": [
    "By Grouping and then splitting the text into chunks we now have ended up with quite a few more additional examples but those examples contain all of the data present in our texts, most of which would have been lost without taking this approach.\n",
    "\n",
    "Let's have a look at he first rally speech decoded from the tokens using the .decode() method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e11e97b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[CLS] thank you. thank you. thank you to vice president pence. he\\'s a good guy. we\\'ve done a great job together. and merry christmas, michigan. thank you, michigan. what a victory we had in michigan. what a victory was that. one of the greats. was that the greatest evening? but i\\'m thrilled to be here with thousands of hardworking patriots as we celebrate the miracle of christmas, the greatness of america and the glory of god. thank you very much. and did you notice that everybody is saying merry christmas again? did you notice? saying merry christmas. i remember when i first started this beautiful trip, this beautiful journey, i just said to the first lady, \" you are so lucky. i took you on this fantastic journey. it\\'s so much fun. they want to impeach you. they want to do worse than that. \" by the way, by the way, by the way, it doesn\\'t really feel like we\\'re being impeached. the country is doing better than ever before. we did nothing wrong. we did nothing wrong. and we have tremendous support in the republican party like we\\'ve never had before. nobody\\'s ever had this kind'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(lm_datasets[0][\"input_ids\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e309e164",
   "metadata": {},
   "source": [
    "And now the labels of that same speech:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d5de7bb3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[CLS] thank you. thank you. thank you to vice president pence. he\\'s a good guy. we\\'ve done a great job together. and merry christmas, michigan. thank you, michigan. what a victory we had in michigan. what a victory was that. one of the greats. was that the greatest evening? but i\\'m thrilled to be here with thousands of hardworking patriots as we celebrate the miracle of christmas, the greatness of america and the glory of god. thank you very much. and did you notice that everybody is saying merry christmas again? did you notice? saying merry christmas. i remember when i first started this beautiful trip, this beautiful journey, i just said to the first lady, \" you are so lucky. i took you on this fantastic journey. it\\'s so much fun. they want to impeach you. they want to do worse than that. \" by the way, by the way, by the way, it doesn\\'t really feel like we\\'re being impeached. the country is doing better than ever before. we did nothing wrong. we did nothing wrong. and we have tremendous support in the republican party like we\\'ve never had before. nobody\\'s ever had this kind'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(lm_datasets[0][\"labels\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f882771c",
   "metadata": {},
   "source": [
    "We have the exact same thing in both columns as is to be expected. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13ebef4f",
   "metadata": {},
   "source": [
    "## Fine-tuning DistilBERT with the Trainer API\n",
    "Next step is to insert the mask tokens into the ids which we do via the use of a data collator. All we need to pass it is the tokenizer that we are using and the <b>mlm_probability</b> argument that specifies what fraction of the tokens to mask.=:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "301800e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm_probability=0.15)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "817bd1a8",
   "metadata": {},
   "source": [
    "Let's take a look at what the masked texts which the collator produces. It expects a list of dicts, where each dict represents a single chunk of contiguous text so we need to first iterate over the dataset before feeding the batch to the collator. We remove the \"word_ids\" key for this data collator as it does not expect it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fbe54778",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a DistilBertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "'>>> [CLS] thank you. [MASK] [MASK]. thank you to [MASK] president pence. he's a good guy. we've done a great job together. maia merry christmas, michigan. thank you, michigan. what [MASK] victory we [MASK] in michigan. what a victory was that. one of the greats. was that the greatest evening? but i cavendish m thrilled to be [MASK] [MASK] thousands of hardworking patriots as we celebrate the miracle of christmas, the greatness of america and the [MASK] [MASK] god. thank [MASK] very much. and did you notice [MASK] everybody is saying merry christmas again? did you notice? saying merry christmas. i remember when i first started this beautiful trip, this beautiful [MASK], i just said to the [MASK] lady [MASK] \" you are so lucky [MASK] i took you on this fantastic [MASK]. it's [MASK] much fun. they want to impeach youdp they want to do worse [MASK] that. \" by the way, by [MASK] way, by the way [MASK] it doesn't really feel like [MASK]'re being impeached. the country is doing better than ever before. we [MASK] nothing [MASK]. we did [MASK] wrong. [MASK] we have [MASK] support workforce the republican party like we [MASK] ve never had [MASK]. nobody's ever had this kind'\n",
      "\n",
      "'>>> of support [MASK] but this sacred season, our country [MASK] thriving and it's thriving truly like it [MASK] never, it has never happened before to the extent what's happening now. and by the way, your [MASK], because [MASK] us, not because of [MASK] government [MASK] but because of [MASK], because of the harsh [MASK] we've done. because i understand [MASK]'s not fixing those potholes. that's what the word is. it was [MASK] [MASK] roads and they [MASK] to [MASK] those gasoline taxes on you. we don't want to do that. but she's not fixing [MASK] potholes. [MASK] michigan's had the best year it's ever had. best year it's ever [MASK]. and that's because [MASK] have auto companies expanding and thriving and [MASK]'[MASK] coming in from japan [MASK] they're [MASK] in from a lot of other places. look what'[MASK] [MASK]. and you [MASK], i don't signaled if you know this, [MASK] [MASK] [MASK] years ago i was honored. i was the man of the year by i think somebody, whoever. i was the man of [MASK] year in michigan [MASK] can you believe it? long time [MASK] [MASK] that was [MASK] before i ever decided to [MASK] this. i was happy'\n"
     ]
    }
   ],
   "source": [
    "#samples = lm_datasets.to_list()\n",
    "samples = [lm_datasets[i] for i in range(2)]\n",
    "for sample in samples:\n",
    "    _ = sample.pop(\"word_ids\")\n",
    "\n",
    "for chunk in data_collator(samples)[\"input_ids\"]:\n",
    "    print(f\"\\n'>>> {tokenizer.decode(chunk)}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "346aae71",
   "metadata": {},
   "source": [
    "We can see that the [MASK] token has been randomly inserted at various locations in our text. These will be the tokens which our model will have to predict during training and those masks will be randomised with each batch during training.\n",
    "\n",
    "When training models for masked language modeling, one technique that can be used is to mask whole words together, not just individual tokens. This approach is called whole word masking. If we want to use whole word masking, we will need to build a data collator ourselves. A data collator is just a function that takes a list of samples and converts them into a batch, so let’s do this now! We’ll use the word IDs computed earlier to make a map between word indices and the corresponding tokens, then randomly decide which words to mask and apply that mask on the inputs. Note that the labels are all -100 except for the ones corresponding to mask words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d9d21e7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "wwm_probability = 0.2\n",
    "\n",
    "\n",
    "def whole_word_masking_data_collator(features):\n",
    "    for feature in features:\n",
    "        word_ids = feature.pop(\"word_ids\")\n",
    "\n",
    "        # Create a map between words and corresponding token indices\n",
    "        mapping = collections.defaultdict(list)\n",
    "        current_word_index = -1\n",
    "        current_word = None\n",
    "        for idx, word_id in enumerate(word_ids):\n",
    "            if word_id is not None:\n",
    "                if word_id != current_word:\n",
    "                    current_word = word_id\n",
    "                    current_word_index += 1\n",
    "                mapping[current_word_index].append(idx)\n",
    "\n",
    "        # Randomly mask words\n",
    "        mask = np.random.binomial(1, wwm_probability, (len(mapping),))\n",
    "        input_ids = feature[\"input_ids\"]\n",
    "        labels = feature[\"labels\"]\n",
    "        new_labels = [-100] * len(labels)\n",
    "        for word_id in np.where(mask)[0]:\n",
    "            word_id = word_id.item()\n",
    "            for idx in mapping[word_id]:\n",
    "                new_labels[idx] = labels[idx]\n",
    "                input_ids[idx] = tokenizer.mask_token_id\n",
    "        feature[\"labels\"] = new_labels\n",
    "\n",
    "    return tf_default_data_collator(features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "056e36e5",
   "metadata": {},
   "source": [
    "Let's test it on the same sample as before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "46392e6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "'>>> [CLS] thank you. thank you. [MASK] you to vice president [MASK] [MASK]. [MASK]'s a good guy [MASK] we'[MASK] [MASK] a [MASK] job together. and merry christmas, michigan. thank you [MASK] michigan. [MASK] a victory we had in michigan. [MASK] a victory was that [MASK] one [MASK] the [MASK] [MASK] [MASK] was [MASK] [MASK] greatest evening [MASK] but [MASK]'m thrilled to be here with [MASK] of hardworking patriots [MASK] we [MASK] the miracle [MASK] christmas, the greatness of america and the glory of god. thank [MASK] very [MASK]. and did you notice that everybody [MASK] saying merry [MASK] [MASK]? did you notice? [MASK] merry christmas. i [MASK] when i first started [MASK] [MASK] [MASK], this beautiful journey, [MASK] just said to [MASK] first lady, \" you are so lucky. i [MASK] [MASK] on this fantastic journey. it's so much fun. they want to [MASK] [MASK] [MASK] you [MASK] they want to do [MASK] [MASK] [MASK] [MASK] \" by [MASK] way [MASK] by [MASK] way, [MASK] the [MASK], it doesn'[MASK] really feel like [MASK]'[MASK] being [MASK] [MASK] [MASK]. the country is doing [MASK] [MASK] ever before. we did nothing [MASK]. we did nothing wrong. and we [MASK] tremendous support in the republican party like we've never had before. nobody's [MASK] had this [MASK]'\n",
      "\n",
      "'>>> [MASK] support. but this sacred season, our country is thriving and it [MASK] s [MASK] truly like it has never [MASK] it [MASK] never happened before to the extent what [MASK] s happening now. and by the way, your state, [MASK] [MASK] [MASK], [MASK] because of local government, but [MASK] [MASK] us, because of the job that we've [MASK]. because [MASK] understand she [MASK] s [MASK] fixing those potholes. [MASK]'s what the [MASK] [MASK]. it [MASK] all about [MASK] and they want to raise those gasoline [MASK] on you. we don [MASK] t want to do that. [MASK] she's not fixing the potholes. but [MASK]'[MASK] had [MASK] best year it's ever [MASK] [MASK] best [MASK] it's ever had. and that's because we have auto companies expanding and thriving and [MASK]'re coming in from [MASK] and they're coming in from a lot of other places. look what's happening. and you know, i don't know if you know [MASK], but probably 10 [MASK] [MASK] i was [MASK]. i was the man of the year by i [MASK] somebody [MASK] whoever [MASK] i was [MASK] [MASK] [MASK] the [MASK] in [MASK] [MASK] [MASK] you believe it [MASK] [MASK] time. and that [MASK] long before i ever [MASK] to do this. i was happy'\n"
     ]
    }
   ],
   "source": [
    "# samples = lm_datasets.to_list()\n",
    "samples = [lm_datasets[i] for i in range(2)]\n",
    "batch = whole_word_masking_data_collator(samples)\n",
    "\n",
    "for chunk in batch[\"input_ids\"]:\n",
    "    print(f\"\\n'>>> {tokenizer.decode(chunk)}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41d7fb62",
   "metadata": {},
   "source": [
    "### Train/Test Split\n",
    "We now need to split the data into train and test datasets. We can make use of Dataset.train_test_split():"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "59b5e35e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'word_ids', 'labels'],\n",
       "        num_rows: 1739\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'word_ids', 'labels'],\n",
       "        num_rows: 193\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_size = round(0.9 * len(lm_datasets))\n",
    "test_size = (len(lm_datasets) - train_size)\n",
    "\n",
    "dataset_split = lm_datasets.train_test_split(\n",
    "    train_size=train_size, test_size=test_size, seed=42\n",
    ")\n",
    "dataset_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbcac7aa",
   "metadata": {},
   "source": [
    "### Login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9c913bf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token is valid.\n",
      "Your token has been saved in your configured git credential helpers (manager-core).\n",
      "Your token has been saved to C:\\Users\\ksbon\\.huggingface\\token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "\n",
    "notebook_login()\n",
    "\n",
    "# huggingface-cli login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f5ffb4cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_train_dataset = model.prepare_tf_dataset(\n",
    "    dataset_split[\"train\"],\n",
    "    collate_fn=data_collator,\n",
    "    shuffle=True,\n",
    "    batch_size=32,\n",
    ")\n",
    "\n",
    "tf_eval_dataset = model.prepare_tf_dataset(\n",
    "    dataset_split[\"test\"],\n",
    "    collate_fn=data_collator,\n",
    "    shuffle=False,\n",
    "    batch_size=32,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "2180d9dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFDistilBertForMaskedLM.\n",
      "\n",
      "All the layers of TFDistilBertForMaskedLM were initialized from the model checkpoint at distilbert-base-uncased-finetuned-dt-rally-speeches.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFDistilBertForMaskedLM for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "# Load model from local dir\n",
    "save_dir = \"C:/Users/ksbon/Desktop/Jupyter/repos/Hugging Face models/distilbert-base-uncased-finetuned-dt-rally-speeches\"\n",
    "# model = TFAutoModelForMaskedLM.from_pretrained(save_dir)\n",
    "\n",
    "# Load model from Huggingface hub\n",
    "model = TFAutoModelForMaskedLM.from_pretrained(\"distilbert-base-uncased-finetuned-dt-rally-speeches\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "89628b9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import AdamWeightDecay\n",
    "# optimizer = AdamWeightDecay(learning_rate=2e-5, weight_decay_rate=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "c7bc4022",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No loss specified in compile() - the model's internal loss computation will be used as the loss. Don't panic - this is a common way to train TensorFlow models in Transformers! To disable this behaviour please pass a loss argument, or explicitly pass `loss=None` if you do not want your model to compute a loss.\n",
      "C:\\Users\\ksbon\\Desktop\\Jupyter\\repos\\Hugging Face models\\distilbert-base-uncased-finetuned-dt-rally-speeches is already a clone of https://huggingface.co/Shmendel/distilbert-base-uncased-finetuned-dt-rally-speeches. Make sure you pull the latest changes with `repo.git_pull()`.\n"
     ]
    }
   ],
   "source": [
    "# num_train_steps = len(tf_train_dataset)\n",
    "num_train_steps = 250\n",
    "optimizer, schedule = create_optimizer(\n",
    "    init_lr=2e-5,\n",
    "    num_warmup_steps=1_000,\n",
    "    num_train_steps=num_train_steps,\n",
    "    weight_decay_rate=0.01\n",
    ")\n",
    "model.compile(optimizer=optimizer, \n",
    "#               metrics=['accuracy']\n",
    "             )\n",
    "\n",
    "# Train in mixed-precision float16\n",
    "# tf.keras.mixed_precision.set_global_policy(\"mixed_float16\")\n",
    "\n",
    "model_name = model_checkpoint.split(\"/\")[-1]\n",
    "callback = PushToHubCallback(\n",
    "#     output_dir=f\"{model_name}-finetuned-dt-rally-speeches\", \n",
    "    output_dir=save_dir, \n",
    "    tokenizer=tokenizer,\n",
    "#     hub_model_id=\"distilbert-base-uncased-finetuned-dt-rally-speeches\",\n",
    "#     hub_token=\"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\",\n",
    "    save_strategy=\"no\" # getting an error with anything other than no\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "aa27c96b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "54/54 [==============================] - 685s 13s/step - loss: 1.8323 - val_loss: 1.7319\n",
      "Epoch 2/10\n",
      "54/54 [==============================] - 639s 12s/step - loss: 1.8230 - val_loss: 1.7765\n",
      "Epoch 3/10\n",
      "54/54 [==============================] - 638s 12s/step - loss: 1.8213 - val_loss: 1.7305\n",
      "Epoch 4/10\n",
      "54/54 [==============================] - 636s 12s/step - loss: 1.8191 - val_loss: 1.6551\n",
      "Epoch 5/10\n",
      "54/54 [==============================] - 636s 12s/step - loss: 1.8031 - val_loss: 1.6772\n",
      "Epoch 6/10\n",
      "54/54 [==============================] - 637s 12s/step - loss: 1.7946 - val_loss: 1.6388\n",
      "Epoch 7/10\n",
      "54/54 [==============================] - 635s 12s/step - loss: 1.7777 - val_loss: 1.6475\n",
      "Epoch 8/10\n",
      "54/54 [==============================] - 636s 12s/step - loss: 1.7552 - val_loss: 1.6849\n",
      "Epoch 9/10\n",
      "54/54 [==============================] - 636s 12s/step - loss: 1.7392 - val_loss: 1.6441\n",
      "Epoch 10/10\n",
      "54/54 [==============================] - 636s 12s/step - loss: 1.7297 - val_loss: 1.5226\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de63292af0274814bdafff621131ed43",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Upload file tf_model.h5:   0%|          | 1.00/347M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "To https://huggingface.co/Shmendel/distilbert-base-uncased-finetuned-dt-rally-speeches\n",
      "   76ad3d9..9b05dbb  main -> main\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x26b003f6f70>"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x=tf_train_dataset, \n",
    "          validation_data=tf_eval_dataset,\n",
    "          epochs=10,\n",
    "          callbacks=[callback]\n",
    "         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "id": "04bb2332",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manual model push to hub\n",
    "# model.push_to_hub(\"distilbert-base-uncased-finetuned-dt-rally-speeches\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "1cf566c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7/7 [==============================] - 24s 3s/step - loss: 1.6360\n",
      "Perplexity: 5.13\n"
     ]
    }
   ],
   "source": [
    "eval_loss = model.evaluate(tf_eval_dataset)\n",
    "print(f\"Perplexity: {math.exp(eval_loss):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "16d95439",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at distilbert-base-uncased were not used when initializing TFDistilBertForMaskedLM: ['activation_13']\n",
      "- This IS expected if you are initializing TFDistilBertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFDistilBertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFDistilBertForMaskedLM were initialized from the model checkpoint at distilbert-base-uncased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFDistilBertForMaskedLM for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "base_model = TFAutoModelForMaskedLM.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "59f8e04f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base model in fill-mask configuration\n",
    "base_mask_filler = pipeline(\n",
    "    task=\"fill-mask\", \n",
    "    model=base_model, \n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "# Fine tuned model in fill-mask configuration\n",
    "mask_filler = pipeline(\n",
    "    task=\"fill-mask\", \n",
    "    model=model, \n",
    "#     model=\"huggingface-Shmendel/distilbert-base-uncased-finetuned-dt-rally-speeches\",\n",
    "    tokenizer=tokenizer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "id": "2fd059a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# text = \"It's [MASK] news\" # fake is first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "id": "3447e84a",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = 'One [MASK] nation' # great is first and other similar ones are high up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "id": "1cc5505a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# text = 'Let\\'s make America [MASK] again!'  # great is second\n",
    "# text = 'The [MASK] American nation'  # great is second"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "id": "deb216db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# text = 'Make [MASK] great again'\n",
    "# text = \"The [MASK] virus\"\n",
    "# text = \"kung [MASK]\"\n",
    "\n",
    "# text = 'We will make America [MASK] again!'\n",
    "# text = 'keep fighting keep [MASK]'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "id": "16d8890d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> one hundred nation -> 0.04\n",
      ">>> one - nation -> 0.04\n",
      ">>> one sovereign nation -> 0.02\n",
      ">>> one african nation -> 0.02\n",
      ">>> one nation nation -> 0.02\n"
     ]
    }
   ],
   "source": [
    "base_preds = base_mask_filler(text)\n",
    "\n",
    "for pred in base_preds:\n",
    "    print(f\">>> {pred['sequence']} -> {pred['score']:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "id": "4c39f8da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> one great nation -> 0.09\n",
      ">>> one proud nation -> 0.04\n",
      ">>> one strong nation -> 0.04\n",
      ">>> one - nation -> 0.04\n",
      ">>> one more nation -> 0.03\n"
     ]
    }
   ],
   "source": [
    "preds = mask_filler(text)\n",
    "\n",
    "for pred in preds:\n",
    "    print(f\">>> {pred['sequence']} -> {pred['score']:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "a0542f9b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'score': 0.42231783270835876,\n",
       "  'token': 7098,\n",
       "  'token_str': 'proud',\n",
       "  'sequence': \"let's make america proud again!\"},\n",
       " {'score': 0.07160758972167969,\n",
       "  'token': 2307,\n",
       "  'token_str': 'great',\n",
       "  'sequence': \"let's make america great again!\"},\n",
       " {'score': 0.06872954964637756,\n",
       "  'token': 2844,\n",
       "  'token_str': 'strong',\n",
       "  'sequence': \"let's make america strong again!\"},\n",
       " {'score': 0.027919413521885872,\n",
       "  'token': 3407,\n",
       "  'token_str': 'happy',\n",
       "  'sequence': \"let's make america happy again!\"},\n",
       " {'score': 0.01937711425125599,\n",
       "  'token': 6428,\n",
       "  'token_str': 'stronger',\n",
       "  'sequence': \"let's make america stronger again!\"}]"
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "13749f14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import jupyterthemes as jt\n",
    "# from jupyterthemes import get_themes\n",
    "# from jupyterthemes.stylefx import set_nb_theme\n",
    "# # jt -t monokai -f fira -fs 10 -nf ptsans -nfs 11 -N -kl -cursw 2 -cursc r -cellw 95% -T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "550f4331",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manual model save\n",
    "# save_dir = \"C:/Users/ksbon/Desktop/Jupyter/repos/Donald-Trump-Rally-Speeches-NLP/distilbert-base-uncased-finetuned-dt-rally-speeches\"\n",
    "# tokenizer.save_pretrained(save_dir)\n",
    "# model.save_pretrained(save_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77e508b0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "90fc05eb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99483006",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b0a6f2d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
