{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b1e61527",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d9797544",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import collections\n",
    "import math\n",
    "\n",
    "from datasets import Dataset\n",
    "from transformers import AutoTokenizer, TFAutoModelForMaskedLM, DataCollatorForLanguageModeling, create_optimizer, pipeline\n",
    "from transformers.data.data_collator import tf_default_data_collator\n",
    "from transformers.keras_callbacks import PushToHubCallback\n",
    "from huggingface_hub import notebook_login"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b172c36b",
   "metadata": {},
   "source": [
    "## Dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35131fcb",
   "metadata": {},
   "source": [
    "Importing dataframe from a previous notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3b6c7ffc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Location</th>\n",
       "      <th>Month</th>\n",
       "      <th>Year</th>\n",
       "      <th>filename</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Battle Creek</td>\n",
       "      <td>Dec</td>\n",
       "      <td>2019</td>\n",
       "      <td>BattleCreekDec19_2019.txt</td>\n",
       "      <td>Thank you. Thank you. Thank you to Vice Presid...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Bemidji</td>\n",
       "      <td>Sep</td>\n",
       "      <td>2020</td>\n",
       "      <td>BemidjiSep18_2020.txt</td>\n",
       "      <td>There's a lot of people. That's great. Thank y...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Charleston</td>\n",
       "      <td>Feb</td>\n",
       "      <td>2020</td>\n",
       "      <td>CharlestonFeb28_2020.txt</td>\n",
       "      <td>Thank you. Thank you. Thank you. All I can say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Charlotte</td>\n",
       "      <td>Mar</td>\n",
       "      <td>2020</td>\n",
       "      <td>CharlotteMar2_2020.txt</td>\n",
       "      <td>I want to thank you very much. North Carolina,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Cincinnati</td>\n",
       "      <td>Aug</td>\n",
       "      <td>2019</td>\n",
       "      <td>CincinnatiAug1_2019.txt</td>\n",
       "      <td>Thank you all. Thank you very much. Thank you ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Colorador Springs</td>\n",
       "      <td>Feb</td>\n",
       "      <td>2020</td>\n",
       "      <td>ColoradorSpringsFeb20_2020.txt</td>\n",
       "      <td>Hello Colorado. We love Colorado, most beautif...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Dallas</td>\n",
       "      <td>Oct</td>\n",
       "      <td>2019</td>\n",
       "      <td>DallasOct17_2019.txt</td>\n",
       "      <td>Thank you. Thank you very much. Hello Dallas. ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Des Moines</td>\n",
       "      <td>Jan</td>\n",
       "      <td>2020</td>\n",
       "      <td>DesMoinesJan30_2020.txt</td>\n",
       "      <td>I worked so hard for this state. I worked so h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Fayetteville</td>\n",
       "      <td>Sep</td>\n",
       "      <td>2020</td>\n",
       "      <td>FayettevilleSep19_2020.txt</td>\n",
       "      <td>What a crowd, what a crowd. Get those people o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Fayetteville</td>\n",
       "      <td>Sep</td>\n",
       "      <td>2019</td>\n",
       "      <td>FayettevilleSep9_2019.txt</td>\n",
       "      <td>Thank you everybody. Thank you and Vice Presi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Freeland</td>\n",
       "      <td>Sep</td>\n",
       "      <td>2020</td>\n",
       "      <td>FreelandSep10_2020.txt</td>\n",
       "      <td>We brought you a lot of car plants, Michigan. ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Greenville</td>\n",
       "      <td>Jul</td>\n",
       "      <td>2019</td>\n",
       "      <td>GreenvilleJul17_2019.txt</td>\n",
       "      <td>Thank you very much. Thank you. Thank you. Tha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Henderson</td>\n",
       "      <td>Sep</td>\n",
       "      <td>2020</td>\n",
       "      <td>HendersonSep13_2020.txt</td>\n",
       "      <td>Thank you, thank you. Wow. Wow, and I'm thrill...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Hershey</td>\n",
       "      <td>Dec</td>\n",
       "      <td>2019</td>\n",
       "      <td>HersheyDec10_2019.txt</td>\n",
       "      <td>Well, thank you to Vice President Pence. Than...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Las Vegas</td>\n",
       "      <td>Feb</td>\n",
       "      <td>2020</td>\n",
       "      <td>LasVegasFeb21_2020.txt</td>\n",
       "      <td>Well, thank you very much. And hello Las Vegas...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Latrobe</td>\n",
       "      <td>Sep</td>\n",
       "      <td>2020</td>\n",
       "      <td>LatrobeSep3_2020.txt</td>\n",
       "      <td>So thank you Pennsylvania, very much. I'm thri...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Lexington</td>\n",
       "      <td>Nov</td>\n",
       "      <td>2019</td>\n",
       "      <td>LexingtonNov4_2019.txt</td>\n",
       "      <td>Thank you very much and thank you to the origi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Milwaukee</td>\n",
       "      <td>Jan</td>\n",
       "      <td>2020</td>\n",
       "      <td>MilwaukeeJan14_2020.txt</td>\n",
       "      <td>Well thank you very much. And I'm thrilled to ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Minden</td>\n",
       "      <td>Sep</td>\n",
       "      <td>2020</td>\n",
       "      <td>MindenSep12_2020.txt</td>\n",
       "      <td>Well, I thank you very much. So I want to star...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Minneapolis</td>\n",
       "      <td>Oct</td>\n",
       "      <td>2019</td>\n",
       "      <td>MinneapolisOct10_2019.txt</td>\n",
       "      <td>Thank you very much. Thank you, Minnesota. Thi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Mosinee</td>\n",
       "      <td>Sep</td>\n",
       "      <td>2020</td>\n",
       "      <td>MosineeSep17_2020.txt</td>\n",
       "      <td>Thank you, thank you very much. Thank you very...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>New Hampshire</td>\n",
       "      <td>Aug</td>\n",
       "      <td>2019</td>\n",
       "      <td>NewHampshireAug15_2019.txt</td>\n",
       "      <td>Thank you very much everybody. Thank you. Wow...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>New Hampshire</td>\n",
       "      <td>Aug</td>\n",
       "      <td>2020</td>\n",
       "      <td>NewHampshireAug28_2020.txt</td>\n",
       "      <td>Hello, everybody. Hello, everybody. Wow. Hello...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>New Hampshire</td>\n",
       "      <td>Feb</td>\n",
       "      <td>2020</td>\n",
       "      <td>NewHampshireFeb10_2020.txt</td>\n",
       "      <td>Hello, Manchester, and I am thrilled to be in ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>New Mexico</td>\n",
       "      <td>Sep</td>\n",
       "      <td>2019</td>\n",
       "      <td>NewMexicoSep16_2019.txt</td>\n",
       "      <td>Wow, thank you. Thank you, New Mexico. Thank ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Ohio</td>\n",
       "      <td>Sep</td>\n",
       "      <td>2020</td>\n",
       "      <td>OhioSep21_2020.txt</td>\n",
       "      <td>Wow, that's a big crowd. This is a big crowd. ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>Phoenix</td>\n",
       "      <td>Feb</td>\n",
       "      <td>2020</td>\n",
       "      <td>PhoenixFeb19_2020.txt</td>\n",
       "      <td>Thank you very much, Phoenix. We love to be b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Pittsburgh</td>\n",
       "      <td>Sep</td>\n",
       "      <td>2020</td>\n",
       "      <td>PittsburghSep22_2020.txt</td>\n",
       "      <td>Doesn't have the power. Doesn't have the stayi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>Texas</td>\n",
       "      <td>Sep</td>\n",
       "      <td>2019</td>\n",
       "      <td>TexasSep23_2019.txt</td>\n",
       "      <td>Hello, Houston. I am so thrilled to be here in...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>Toledo</td>\n",
       "      <td>Jan</td>\n",
       "      <td>2020</td>\n",
       "      <td>ToledoJan9_2020.txt</td>\n",
       "      <td>Well, thank you very much. Vice President Mike...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>Tulsa</td>\n",
       "      <td>Jun</td>\n",
       "      <td>2020</td>\n",
       "      <td>TulsaJun20_2020.txt</td>\n",
       "      <td>Thank you, thank you. So we begin, Oklahoma, w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>Tupelo</td>\n",
       "      <td>Nov</td>\n",
       "      <td>2019</td>\n",
       "      <td>TupeloNov1_2019.txt</td>\n",
       "      <td>ell, thank you very much. And hello, Tupelo. T...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>Wildwood</td>\n",
       "      <td>Jan</td>\n",
       "      <td>2020</td>\n",
       "      <td>WildwoodJan28_2020.txt</td>\n",
       "      <td>Thank you. Thank you. I love New Jersey and I'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>Salem</td>\n",
       "      <td>Sep</td>\n",
       "      <td>2020</td>\n",
       "      <td>Winston-SalemSep8_2020.txt</td>\n",
       "      <td>Well, thank you very much. Thank you. Thank yo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>Yuma</td>\n",
       "      <td>Aug</td>\n",
       "      <td>2020</td>\n",
       "      <td>YumaAug18_2020.txt</td>\n",
       "      <td>Oh, thank you very much, everybody. Thank you....</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Location Month  Year                        filename  \\\n",
       "0         Battle Creek   Dec  2019       BattleCreekDec19_2019.txt   \n",
       "1              Bemidji   Sep  2020           BemidjiSep18_2020.txt   \n",
       "2           Charleston   Feb  2020        CharlestonFeb28_2020.txt   \n",
       "3            Charlotte   Mar  2020          CharlotteMar2_2020.txt   \n",
       "4           Cincinnati   Aug  2019         CincinnatiAug1_2019.txt   \n",
       "5    Colorador Springs   Feb  2020  ColoradorSpringsFeb20_2020.txt   \n",
       "6               Dallas   Oct  2019            DallasOct17_2019.txt   \n",
       "7           Des Moines   Jan  2020         DesMoinesJan30_2020.txt   \n",
       "8         Fayetteville   Sep  2020      FayettevilleSep19_2020.txt   \n",
       "9         Fayetteville   Sep  2019       FayettevilleSep9_2019.txt   \n",
       "10            Freeland   Sep  2020          FreelandSep10_2020.txt   \n",
       "11          Greenville   Jul  2019        GreenvilleJul17_2019.txt   \n",
       "12           Henderson   Sep  2020         HendersonSep13_2020.txt   \n",
       "13             Hershey   Dec  2019           HersheyDec10_2019.txt   \n",
       "14           Las Vegas   Feb  2020          LasVegasFeb21_2020.txt   \n",
       "15             Latrobe   Sep  2020            LatrobeSep3_2020.txt   \n",
       "16           Lexington   Nov  2019          LexingtonNov4_2019.txt   \n",
       "17           Milwaukee   Jan  2020         MilwaukeeJan14_2020.txt   \n",
       "18              Minden   Sep  2020            MindenSep12_2020.txt   \n",
       "19         Minneapolis   Oct  2019       MinneapolisOct10_2019.txt   \n",
       "20             Mosinee   Sep  2020           MosineeSep17_2020.txt   \n",
       "21       New Hampshire   Aug  2019      NewHampshireAug15_2019.txt   \n",
       "22       New Hampshire   Aug  2020      NewHampshireAug28_2020.txt   \n",
       "23       New Hampshire   Feb  2020      NewHampshireFeb10_2020.txt   \n",
       "24          New Mexico   Sep  2019         NewMexicoSep16_2019.txt   \n",
       "25                Ohio   Sep  2020              OhioSep21_2020.txt   \n",
       "26             Phoenix   Feb  2020           PhoenixFeb19_2020.txt   \n",
       "27          Pittsburgh   Sep  2020        PittsburghSep22_2020.txt   \n",
       "28               Texas   Sep  2019             TexasSep23_2019.txt   \n",
       "29              Toledo   Jan  2020             ToledoJan9_2020.txt   \n",
       "30               Tulsa   Jun  2020             TulsaJun20_2020.txt   \n",
       "31              Tupelo   Nov  2019             TupeloNov1_2019.txt   \n",
       "32            Wildwood   Jan  2020          WildwoodJan28_2020.txt   \n",
       "33               Salem   Sep  2020      Winston-SalemSep8_2020.txt   \n",
       "34                Yuma   Aug  2020              YumaAug18_2020.txt   \n",
       "\n",
       "                                              content  \n",
       "0   Thank you. Thank you. Thank you to Vice Presid...  \n",
       "1   There's a lot of people. That's great. Thank y...  \n",
       "2   Thank you. Thank you. Thank you. All I can say...  \n",
       "3   I want to thank you very much. North Carolina,...  \n",
       "4   Thank you all. Thank you very much. Thank you ...  \n",
       "5   Hello Colorado. We love Colorado, most beautif...  \n",
       "6   Thank you. Thank you very much. Hello Dallas. ...  \n",
       "7   I worked so hard for this state. I worked so h...  \n",
       "8   What a crowd, what a crowd. Get those people o...  \n",
       "9    Thank you everybody. Thank you and Vice Presi...  \n",
       "10  We brought you a lot of car plants, Michigan. ...  \n",
       "11  Thank you very much. Thank you. Thank you. Tha...  \n",
       "12  Thank you, thank you. Wow. Wow, and I'm thrill...  \n",
       "13   Well, thank you to Vice President Pence. Than...  \n",
       "14  Well, thank you very much. And hello Las Vegas...  \n",
       "15  So thank you Pennsylvania, very much. I'm thri...  \n",
       "16  Thank you very much and thank you to the origi...  \n",
       "17  Well thank you very much. And I'm thrilled to ...  \n",
       "18  Well, I thank you very much. So I want to star...  \n",
       "19  Thank you very much. Thank you, Minnesota. Thi...  \n",
       "20  Thank you, thank you very much. Thank you very...  \n",
       "21   Thank you very much everybody. Thank you. Wow...  \n",
       "22  Hello, everybody. Hello, everybody. Wow. Hello...  \n",
       "23  Hello, Manchester, and I am thrilled to be in ...  \n",
       "24   Wow, thank you. Thank you, New Mexico. Thank ...  \n",
       "25  Wow, that's a big crowd. This is a big crowd. ...  \n",
       "26   Thank you very much, Phoenix. We love to be b...  \n",
       "27  Doesn't have the power. Doesn't have the stayi...  \n",
       "28  Hello, Houston. I am so thrilled to be here in...  \n",
       "29  Well, thank you very much. Vice President Mike...  \n",
       "30  Thank you, thank you. So we begin, Oklahoma, w...  \n",
       "31  ell, thank you very much. And hello, Tupelo. T...  \n",
       "32  Thank you. Thank you. I love New Jersey and I'...  \n",
       "33  Well, thank you very much. Thank you. Thank yo...  \n",
       "34  Oh, thank you very much, everybody. Thank you....  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%store -r DT_rally_speaches_dataset\n",
    "df = DT_rally_speaches_dataset\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0b52ea6",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "91fabcbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at distilbert-base-uncased were not used when initializing TFDistilBertForMaskedLM: ['activation_13']\n",
      "- This IS expected if you are initializing TFDistilBertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFDistilBertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFDistilBertForMaskedLM were initialized from the model checkpoint at distilbert-base-uncased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFDistilBertForMaskedLM for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"tf_distil_bert_for_masked_lm\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " distilbert (TFDistilBertMai  multiple                 66362880  \n",
      " nLayer)                                                         \n",
      "                                                                 \n",
      " vocab_transform (Dense)     multiple                  590592    \n",
      "                                                                 \n",
      " vocab_layer_norm (LayerNorm  multiple                 1536      \n",
      " alization)                                                      \n",
      "                                                                 \n",
      " vocab_projector (TFDistilBe  multiple                 23866170  \n",
      " rtLMHead)                                                       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 66,985,530\n",
      "Trainable params: 66,985,530\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_checkpoint = \"distilbert-base-uncased\"\n",
    "model = TFAutoModelForMaskedLM.from_pretrained(model_checkpoint)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "445f7290",
   "metadata": {},
   "source": [
    "## Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4cafc44a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac13193c",
   "metadata": {},
   "source": [
    "Let's pick a text to test the base model on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a4d6d7f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# text = \"This is a great [MASK].\"\n",
    "text = \"Make [MASK] great\"\n",
    "# text = \"[MASK] virus\"\n",
    "# text = \"kung [MASK]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3130c8b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at distilbert-base-uncased were not used when initializing TFDistilBertForMaskedLM: ['activation_13']\n",
      "- This IS expected if you are initializing TFDistilBertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFDistilBertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFDistilBertForMaskedLM were initialized from the model checkpoint at distilbert-base-uncased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFDistilBertForMaskedLM for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "base_model = model = TFAutoModelForMaskedLM.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "db7e3136",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Make yourself great\n",
      ">>> Make it great\n",
      ">>> Make thee great\n",
      ">>> Make me great\n",
      ">>> Make yourselves great\n"
     ]
    }
   ],
   "source": [
    "inputs = tokenizer(text, return_tensors=\"np\")\n",
    "token_logits = base_model(**inputs).logits\n",
    "# Find the location of [MASK] and extract its logits\n",
    "mask_token_index = np.argwhere(inputs[\"input_ids\"] == tokenizer.mask_token_id)[0, 1]\n",
    "mask_token_logits = token_logits[0, mask_token_index, :]\n",
    "# Pick the [MASK] candidates with the highest logits\n",
    "# We negate the array before argsort to get the largest, not the smallest, logits\n",
    "top_5_tokens = np.argsort(-mask_token_logits)[:5].tolist()\n",
    "\n",
    "for token in top_5_tokens:\n",
    "    print(f\">>> {text.replace(tokenizer.mask_token, tokenizer.decode([token]))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66de50d5",
   "metadata": {},
   "source": [
    "The tokenizer works best using a dataset so let's convert the pandas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6bacd68a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['Location', 'Month', 'Year', 'filename', 'content'],\n",
       "    num_rows: 35\n",
       "})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = Dataset.from_pandas(df)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d87a7441",
   "metadata": {},
   "source": [
    "Let's set up a tokenize function that can then be mapped onto the dataset. If using a fast tokenizer we can also use the word ids for whole word masking later on. We can also drop the column that will not be required for this task. \n",
    "\n",
    "Since we are working with very long texts we cannot truncate the excess since that will lose us most of the dataset. Instead we can split the texts into batches small enough to fit the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8ad1462c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/35 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (24291 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input_ids', 'attention_mask', 'word_ids'],\n",
       "    num_rows: 35\n",
       "})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def tokenize_function(examples):\n",
    "    result = tokenizer(examples[\"content\"])\n",
    "    if tokenizer.is_fast:\n",
    "        result[\"word_ids\"] = [result.word_ids(i) for i in range(len(result[\"input_ids\"]))]\n",
    "    return result\n",
    "\n",
    "\n",
    "# Use batched=True to activate fast multithreading!\n",
    "tokenized_dataset = dataset.map(\n",
    "    tokenize_function, batched=True, remove_columns=['Location', 'Month', 'Year', 'filename', 'content']\n",
    ")\n",
    "tokenized_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bca01e5e",
   "metadata": {},
   "source": [
    "Let's check the model's max context length in order to determine the size of the chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "96913d1c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "512"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.model_max_length"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec48a604",
   "metadata": {},
   "source": [
    "The capabilities of your machine is also a factor when picking the chunk size. If the machine is lacking in memmory it might be better to pick a smaller number than what the model is capable of handling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0675e8a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_size = 256"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e2e3aa7",
   "metadata": {},
   "source": [
    "Let's check the number of tokens per speech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "473546d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'>>> Rally 0 length: 24291'\n",
      "'>>> Rally 1 length: 22976'\n",
      "'>>> Rally 2 length: 12491'\n",
      "'>>> Rally 3 length: 8802'\n",
      "'>>> Rally 4 length: 10662'\n",
      "'>>> Rally 5 length: 15759'\n",
      "'>>> Rally 6 length: 13867'\n",
      "'>>> Rally 7 length: 15730'\n",
      "'>>> Rally 8 length: 22452'\n",
      "'>>> Rally 9 length: 12007'\n",
      "'>>> Rally 10 length: 13599'\n",
      "'>>> Rally 11 length: 14241'\n",
      "'>>> Rally 12 length: 12027'\n",
      "'>>> Rally 13 length: 13050'\n",
      "'>>> Rally 14 length: 18351'\n",
      "'>>> Rally 15 length: 16629'\n",
      "'>>> Rally 16 length: 11906'\n",
      "'>>> Rally 17 length: 12482'\n",
      "'>>> Rally 18 length: 19059'\n",
      "'>>> Rally 19 length: 15646'\n",
      "'>>> Rally 20 length: 19325'\n",
      "'>>> Rally 21 length: 13165'\n",
      "'>>> Rally 22 length: 11902'\n",
      "'>>> Rally 23 length: 8570'\n",
      "'>>> Rally 24 length: 15375'\n",
      "'>>> Rally 25 length: 14479'\n",
      "'>>> Rally 26 length: 12752'\n",
      "'>>> Rally 27 length: 16218'\n",
      "'>>> Rally 28 length: 3016'\n",
      "'>>> Rally 29 length: 14459'\n",
      "'>>> Rally 30 length: 15064'\n",
      "'>>> Rally 31 length: 12457'\n",
      "'>>> Rally 32 length: 8942'\n",
      "'>>> Rally 33 length: 14664'\n",
      "'>>> Rally 34 length: 8255'\n"
     ]
    }
   ],
   "source": [
    "for idx, sample in enumerate(tokenized_dataset[\"input_ids\"]):\n",
    "    print(f\"'>>> Rally {idx} length: {len(sample)}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b071b23",
   "metadata": {},
   "source": [
    "Let's take a look at the full length of the entire dataset at once:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5455fc7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'>>> Concatenated reviews length: 494670'\n"
     ]
    }
   ],
   "source": [
    "tokenized_dataset_dict = tokenized_dataset.to_dict()\n",
    "# tokenized_dataset_dict = tokenized_dataset[:2]\n",
    "concatenated_dataset = {\n",
    "    k: sum(tokenized_dataset_dict[k], []) for k in tokenized_dataset_dict.keys()\n",
    "}\n",
    "total_length = len(concatenated_dataset[\"input_ids\"])\n",
    "print(f\"'>>> Concatenated reviews length: {total_length}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a5e46510",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 256'\n",
      "'>>> Chunk length: 78'\n"
     ]
    }
   ],
   "source": [
    "chunks = {\n",
    "    k: [t[i : i + chunk_size] for i in range(0, total_length, chunk_size)]\n",
    "    for k, t in concatenated_dataset.items()\n",
    "}\n",
    "\n",
    "for chunk in chunks[\"input_ids\"]:\n",
    "    print(f\"'>>> Chunk length: {len(chunk)}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b06091d1",
   "metadata": {},
   "source": [
    "And now to put it all in a function to map to our dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "684ed633",
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_texts(examples):\n",
    "    # Concatenate all texts\n",
    "    concatenated_dataset = {k: sum(examples[k], []) for k in examples.keys()}\n",
    "    # Compute length of concatenated texts\n",
    "    total_length = len(concatenated_dataset[list(examples.keys())[0]])\n",
    "    # We drop the last chunk if it's smaller than chunk_size\n",
    "    total_length = (total_length // chunk_size) * chunk_size\n",
    "    # Split by chunks of max_len\n",
    "    result = {\n",
    "        k: [t[i : i + chunk_size] for i in range(0, total_length, chunk_size)]\n",
    "        for k, t in concatenated_dataset.items()\n",
    "    }\n",
    "    # Create a new labels column\n",
    "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41f93fcc",
   "metadata": {},
   "source": [
    "At the end of the group_texts function we create a labels column which is a copy of the input_ids. That is needed in masked language modeling in order to provide the ground truth for our language model to learn from.\n",
    "\n",
    "Now let's map the function to the tokenized dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9db8cfec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/35 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input_ids', 'attention_mask', 'word_ids', 'labels'],\n",
       "    num_rows: 1932\n",
       "})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm_datasets = tokenized_dataset.map(group_texts, batched=True)\n",
    "lm_datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f219fab",
   "metadata": {},
   "source": [
    "By Grouping and then splitting the text into chunks we now have ended up with quite a few more additional examples but those examples contain all of the data present in our texts, most of which would have been lost without taking this approach.\n",
    "\n",
    "Let's have a look at he first rally speech decoded from the tokens using the .decode() method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e11e97b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[CLS] thank you. thank you. thank you to vice president pence. he\\'s a good guy. we\\'ve done a great job together. and merry christmas, michigan. thank you, michigan. what a victory we had in michigan. what a victory was that. one of the greats. was that the greatest evening? but i\\'m thrilled to be here with thousands of hardworking patriots as we celebrate the miracle of christmas, the greatness of america and the glory of god. thank you very much. and did you notice that everybody is saying merry christmas again? did you notice? saying merry christmas. i remember when i first started this beautiful trip, this beautiful journey, i just said to the first lady, \" you are so lucky. i took you on this fantastic journey. it\\'s so much fun. they want to impeach you. they want to do worse than that. \" by the way, by the way, by the way, it doesn\\'t really feel like we\\'re being impeached. the country is doing better than ever before. we did nothing wrong. we did nothing wrong. and we have tremendous support in the republican party like we\\'ve never had before. nobody\\'s ever had this kind'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(lm_datasets[0][\"input_ids\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e309e164",
   "metadata": {},
   "source": [
    "And now the labels of that same speech:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d5de7bb3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[CLS] thank you. thank you. thank you to vice president pence. he\\'s a good guy. we\\'ve done a great job together. and merry christmas, michigan. thank you, michigan. what a victory we had in michigan. what a victory was that. one of the greats. was that the greatest evening? but i\\'m thrilled to be here with thousands of hardworking patriots as we celebrate the miracle of christmas, the greatness of america and the glory of god. thank you very much. and did you notice that everybody is saying merry christmas again? did you notice? saying merry christmas. i remember when i first started this beautiful trip, this beautiful journey, i just said to the first lady, \" you are so lucky. i took you on this fantastic journey. it\\'s so much fun. they want to impeach you. they want to do worse than that. \" by the way, by the way, by the way, it doesn\\'t really feel like we\\'re being impeached. the country is doing better than ever before. we did nothing wrong. we did nothing wrong. and we have tremendous support in the republican party like we\\'ve never had before. nobody\\'s ever had this kind'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(lm_datasets[0][\"labels\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f882771c",
   "metadata": {},
   "source": [
    "We have the exact same thing in both columns as is to be expected. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13ebef4f",
   "metadata": {},
   "source": [
    "## Fine-tuning DistilBERT with the Trainer API\n",
    "Next step is to insert the mask tokens into the ids which we do via the use of a data collator. All we need to pass it is the tokenizer that we are using and the <b>mlm_probability</b> argument that specifies what fraction of the tokens to mask.=:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "301800e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm_probability=0.15)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "817bd1a8",
   "metadata": {},
   "source": [
    "Let's take a look at what the masked texts which the collator produces. It expects a list of dicts, where each dict represents a single chunk of contiguous text so we need to first iterate over the dataset before feeding the batch to the collator. We remove the \"word_ids\" key for this data collator as it does not expect it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fbe54778",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a DistilBertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "'>>> [CLS] thank you. thank you [MASK] thank [MASK] to vice [MASK] pen [MASK]. he'[MASK] a good guy. inactive've done a [MASK] job together [MASK] and [MASK] christmas, michigan. thank you, michigan. what a victory we [MASK] in michigan. what 294 victory was that. one of the greats. was that the [MASK] evening? but i'[MASK] thrilled to be [MASK] with [MASK] of hardworking patriots [MASK] we [MASK] the miracle of christmas, the great [MASK] of america and the glory of god. thank you very much. and did you notice that everybody is saying merry christmas again? did you notice? saying merry [MASK] [MASK] i remember when i first started this beautiful trip, this beautiful journey, i just said to the first lady, \" [MASK] are so lucky. i took you on this fantastic journey. [MASK]'s so much fun [MASK] they want to impeach you. they want [MASK] do worse than that [MASK] \"bolic the way, by [MASK] [MASK], [MASK] the way, [MASK] doesn [MASK] [MASK] really feel like we're being [MASK]eached. the country [MASK] [MASK] better than ever before. we [MASK] [MASK] wrong [MASK] we did nothing wrong [MASK] and we have tremendous support in [MASK] republican party like we [MASK] [MASK] [MASK] westfield before. nobody [MASK] s ever had this kind'\n",
      "\n",
      "'>>> of support. but this sacred season, our country is thriving [MASK] [MASK]'s thriving truly like it adventurous never [MASK] it has never happened before to the extent what [MASK] s happening now. and by the way, your state, because of us, not because of local government [MASK] but because of us, [MASK] of the job that we [MASK] ve done [MASK] because i understand she'[MASK] not [MASK] [MASK] potholes [MASK] that's what the word is. it was [MASK] about [MASK] and they [MASK] to raise those gasoline [MASK] on you. [MASK] [MASK]'t want to [MASK] that. but she's [MASK] fixing the potholes. but michigan's [MASK] the best year it's ever had. best [MASK] it's ever had. and [MASK]'s because we have auto companies expanding and [MASK] and they're coming in [MASK] japan and [MASK]'re coming in from a lot of other places. look what's happening. [MASK] [MASK] know, i don'[MASK] know if you know this [MASK] but [MASK] 10 years ago i [MASK] honored [MASK] i was the man of the year by i think somebody, whoever. i [MASK] the man of the year in michigan. can you [MASK] it [MASK] long time. and that was long before i ever [MASK] to do [MASK] hero i was happy'\n"
     ]
    }
   ],
   "source": [
    "#samples = lm_datasets.to_list()\n",
    "samples = [lm_datasets[i] for i in range(2)]\n",
    "for sample in samples:\n",
    "    _ = sample.pop(\"word_ids\")\n",
    "\n",
    "for chunk in data_collator(samples)[\"input_ids\"]:\n",
    "    print(f\"\\n'>>> {tokenizer.decode(chunk)}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "346aae71",
   "metadata": {},
   "source": [
    "We can see that the [MASK] token has been randomly inserted at various locations in our text. These will be the tokens which our model will have to predict during training and those masks will be randomised with each batch during training.\n",
    "\n",
    "When training models for masked language modeling, one technique that can be used is to mask whole words together, not just individual tokens. This approach is called whole word masking. If we want to use whole word masking, we will need to build a data collator ourselves. A data collator is just a function that takes a list of samples and converts them into a batch, so let’s do this now! We’ll use the word IDs computed earlier to make a map between word indices and the corresponding tokens, then randomly decide which words to mask and apply that mask on the inputs. Note that the labels are all -100 except for the ones corresponding to mask words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d9d21e7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "wwm_probability = 0.2\n",
    "\n",
    "\n",
    "def whole_word_masking_data_collator(features):\n",
    "    for feature in features:\n",
    "        word_ids = feature.pop(\"word_ids\")\n",
    "\n",
    "        # Create a map between words and corresponding token indices\n",
    "        mapping = collections.defaultdict(list)\n",
    "        current_word_index = -1\n",
    "        current_word = None\n",
    "        for idx, word_id in enumerate(word_ids):\n",
    "            if word_id is not None:\n",
    "                if word_id != current_word:\n",
    "                    current_word = word_id\n",
    "                    current_word_index += 1\n",
    "                mapping[current_word_index].append(idx)\n",
    "\n",
    "        # Randomly mask words\n",
    "        mask = np.random.binomial(1, wwm_probability, (len(mapping),))\n",
    "        input_ids = feature[\"input_ids\"]\n",
    "        labels = feature[\"labels\"]\n",
    "        new_labels = [-100] * len(labels)\n",
    "        for word_id in np.where(mask)[0]:\n",
    "            word_id = word_id.item()\n",
    "            for idx in mapping[word_id]:\n",
    "                new_labels[idx] = labels[idx]\n",
    "                input_ids[idx] = tokenizer.mask_token_id\n",
    "        feature[\"labels\"] = new_labels\n",
    "\n",
    "    return tf_default_data_collator(features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "056e36e5",
   "metadata": {},
   "source": [
    "Let's test it on the same sample as before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "46392e6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "'>>> [CLS] thank [MASK]. thank [MASK] [MASK] [MASK] [MASK] to vice president pence [MASK] he's a good guy. we [MASK] ve done [MASK] great [MASK] together. and merry christmas, michigan. [MASK] you [MASK] michigan. what a victory we [MASK] in michigan. [MASK] a victory was that. one of the greats. was [MASK] the greatest [MASK] [MASK] but i'm thrilled to [MASK] here with [MASK] [MASK] hardworking patriots [MASK] we celebrate the miracle of christmas, the greatness of america and the glory [MASK] god. thank you very much. and did you notice [MASK] everybody is saying merry christmas again? [MASK] you notice? saying merry christmas. i [MASK] when i first started [MASK] beautiful trip [MASK] this beautiful journey, i [MASK] said to [MASK] first lady, [MASK] you are so lucky. [MASK] [MASK] you [MASK] this [MASK] [MASK]. it [MASK] s [MASK] [MASK] [MASK]. they want to impeach you [MASK] [MASK] want to do worse than that. \" by the way, by the way [MASK] by the way, it doesn't really feel like we're being impeached [MASK] the country [MASK] doing better than ever before. we did nothing wrong. we did [MASK] wrong. [MASK] we have tremendous support in [MASK] republican party like we've never had before. nobody's ever [MASK] this kind'\n",
      "\n",
      "'>>> of support [MASK] but this sacred season, our country [MASK] [MASK] [MASK] it's thriving truly like it has never [MASK] it has never happened before to the extent what'[MASK] happening now. and by the way, your state [MASK] because of [MASK], not because of local government, but because [MASK] [MASK], because of the [MASK] that we've done. because i understand she's not fixing those potholes. that's what the word is. it [MASK] all about roads and they want to [MASK] [MASK] [MASK] taxes on you. we don [MASK] t want [MASK] [MASK] [MASK]. but she's not fixing the [MASK] [MASK]. but michigan's [MASK] the best year it's ever had. [MASK] [MASK] it's ever had. and [MASK]'[MASK] [MASK] we have auto companies expanding and thriving and they'[MASK] [MASK] in from japan and they're coming in from a lot of other places. look what's happening. and you know, i don [MASK] t know if you know this, but probably 10 years ago i was honored. i was the [MASK] [MASK] the year by i think somebody, whoever. [MASK] was the [MASK] of [MASK] year [MASK] michigan. can you believe it [MASK] long [MASK]. [MASK] that was long before i ever decided [MASK] do this. i was happy'\n"
     ]
    }
   ],
   "source": [
    "# samples = lm_datasets.to_list()\n",
    "samples = [lm_datasets[i] for i in range(2)]\n",
    "batch = whole_word_masking_data_collator(samples)\n",
    "\n",
    "for chunk in batch[\"input_ids\"]:\n",
    "    print(f\"\\n'>>> {tokenizer.decode(chunk)}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41d7fb62",
   "metadata": {},
   "source": [
    "### Train/Test Split\n",
    "We now need to split the data into train and test datasets. We can make use of the Dataset.train_test_split() method to do the split based on supplied ratios for train and test size:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "59b5e35e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'word_ids', 'labels'],\n",
       "        num_rows: 1739\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'word_ids', 'labels'],\n",
       "        num_rows: 193\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_size = round(0.9 * len(lm_datasets))\n",
    "test_size = (len(lm_datasets) - train_size)\n",
    "\n",
    "dataset_split = lm_datasets.train_test_split(\n",
    "    train_size=train_size, test_size=test_size, seed=42\n",
    ")\n",
    "dataset_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8f6b19c",
   "metadata": {},
   "source": [
    "The model will require <b>tf.data</b> datasets as its inputs and that can be achived using the <b>prepare_tf_dataset()</b> method, which uses the given model to automatically infer which columns should go into the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f5ffb4cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_train_dataset = model.prepare_tf_dataset(\n",
    "    dataset_split[\"train\"],\n",
    "    collate_fn=data_collator,\n",
    "    shuffle=True,\n",
    "    batch_size=32,\n",
    ")\n",
    "\n",
    "tf_eval_dataset = model.prepare_tf_dataset(\n",
    "    dataset_split[\"test\"],\n",
    "    collate_fn=data_collator,\n",
    "    shuffle=False,\n",
    "    batch_size=32,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3b9d964",
   "metadata": {},
   "source": [
    "### Hugging Face Hub Login\n",
    "I want to be able to save my latest model to the Hugging Face hub whcih is quite streightforward especially when using the PushToHub callback which will save my latest version of the model when it is done training. I can then load the model much like I would load any other of the models present in the hub to make sure I am always using the latest version:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "26ea3408",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d801fe30dae4b28a03b9c44d8693fa7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2180d9dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFDistilBertForMaskedLM.\n",
      "\n",
      "All the layers of TFDistilBertForMaskedLM were initialized from the model checkpoint at distilbert-base-uncased-finetuned-dt-rally-speeches.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFDistilBertForMaskedLM for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "# Load model from Huggingface hub\n",
    "model = TFAutoModelForMaskedLM.from_pretrained(\"distilbert-base-uncased-finetuned-dt-rally-speeches\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "dcd965be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model from local dir\n",
    "# save_dir = \"C:/Users/ksbon/Desktop/Jupyter/repos/Hugging Face models/distilbert-base-uncased-finetuned-dt-rally-speeches\"\n",
    "# model = TFAutoModelForMaskedLM.from_pretrained(save_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ebdf537",
   "metadata": {},
   "source": [
    "## Optimizer and hyper parameters\n",
    "\n",
    "I can use the create_optimizer() function from the Hugging Face Transformers library, which gives me an <b>AdamW</b> optimizer with linear learning rate decay. I will also use the model’s built-in loss, which is the default when no loss is specified as an argument to compile().\n",
    "\n",
    "I have experimented a bit with learning rate and have found that lr of 2e-5 with a decay rate of 0.01 perform very well. Initially I used a higher learning rate (2e-3 and then 2e-4) which was benefitial only at the start when the model still had lots to learn from this particular dataset.\n",
    "\n",
    "I also tried reducing the warmup steps but it seemed to make the model's loss more volatile and it tends to shift rapidly at the start."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "89628b9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import AdamWeightDecay\n",
    "# optimizer = AdamWeightDecay(learning_rate=2e-5, weight_decay_rate=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c7bc4022",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No loss specified in compile() - the model's internal loss computation will be used as the loss. Don't panic - this is a common way to train TensorFlow models in Transformers! To disable this behaviour please pass a loss argument, or explicitly pass `loss=None` if you do not want your model to compute a loss.\n"
     ]
    }
   ],
   "source": [
    "num_train_steps = len(tf_train_dataset)\n",
    "# num_train_steps = 250\n",
    "optimizer, schedule = create_optimizer(\n",
    "    init_lr=2e-5,\n",
    "    num_warmup_steps=1_000,\n",
    "    num_train_steps=num_train_steps,\n",
    "    weight_decay_rate=0.01\n",
    ")\n",
    "model.compile(optimizer=optimizer, \n",
    "#               metrics=['accuracy']\n",
    "             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3186f73f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train in mixed-precision float16\n",
    "# tf.keras.mixed_precision.set_global_policy(\"mixed_float16\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c37ab88",
   "metadata": {},
   "source": [
    "Let's also set up the PushToHubCallback from tokenizers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "fbbb8286",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ksbon\\Desktop\\Jupyter\\repos\\Hugging Face models\\distilbert-base-uncased-finetuned-dt-rally-speeches is already a clone of https://huggingface.co/Shmendel/distilbert-base-uncased-finetuned-dt-rally-speeches. Make sure you pull the latest changes with `repo.git_pull()`.\n"
     ]
    }
   ],
   "source": [
    "# Push to hub\n",
    "model_name = model_checkpoint.split(\"/\")[-1]\n",
    "save_dir = \"C:/Users/ksbon/Desktop/Jupyter/repos/Hugging Face models/distilbert-base-uncased-finetuned-dt-rally-speeches\"\n",
    "\n",
    "push_to_hub_callback = PushToHubCallback(\n",
    "#     output_dir=f\"{model_name}-finetuned-dt-rally-speeches\", \n",
    "    output_dir=save_dir, \n",
    "    tokenizer=tokenizer,\n",
    "#     hub_model_id=\"distilbert-base-uncased-finetuned-dt-rally-speeches\",\n",
    "#     hub_token=\"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\",\n",
    "    save_strategy=\"no\" # getting an error with anything other than no\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da2ae49b",
   "metadata": {},
   "source": [
    "## Training\n",
    "I am commeting out the trining part. At this stage I've ran it for a total of about 30 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "aa27c96b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.fit(x=tf_train_dataset, \n",
    "#           validation_data=tf_eval_dataset,\n",
    "#           epochs=10,\n",
    "#           callbacks=[push_to_hub_callback]\n",
    "#          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "04bb2332",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manual model push to hub\n",
    "# model.push_to_hub(\"distilbert-base-uncased-finetuned-dt-rally-speeches\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c844d594",
   "metadata": {},
   "source": [
    "## Results\n",
    "Let's have a look at the results. One way to determine the model's performance is of course the loss but it has it's limitations. Unlike other tasks like text classification or question answering where we’re given a labeled corpus to train on, with language modeling we don’t have any explicit labels. \n",
    "\n",
    "One way to measure the quality of this language model is to calculate the probabilities it assigns to the next word in all the sentences of the test set. High probabilities indicates that the model indicates that the model is not “surprised” or “perplexed” by the unseen examples, and suggests it has learned the basic patterns of grammar in the language. To calculate the perplexity of the model we need to use the exponential of the cross-entropy loss. Thus, we can calculate the perplexity of our pretrained model by using the model.evaluate() method to compute the cross-entropy loss on the test set and then taking the exponential of the result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "1cf566c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7/7 [==============================] - 33s 4s/step - loss: 1.7816\n",
      "Perplexity: 5.94\n"
     ]
    }
   ],
   "source": [
    "eval_loss = model.evaluate(tf_eval_dataset)\n",
    "print(f\"Perplexity: {math.exp(eval_loss):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e30bd9ee",
   "metadata": {},
   "source": [
    "The perplexity score is decent but let's take a look of the results in practice. To do so I will pass the model a few of Donald Trump's favourite phrases to see if I get the suggestions which I expect. I can do the same thing with the base model as well ad compare the results.\n",
    "\n",
    "Let's start by setting up a function which would take the model and the text I want to test it on a print out the results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "75213abb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_mask(model, text):\n",
    "    # Fine tuned model in fill-mask configuration\n",
    "    mask_filler = pipeline(\n",
    "    task=\"fill-mask\", \n",
    "    model=model, \n",
    "    tokenizer=tokenizer\n",
    "    )\n",
    "    \n",
    "    # Printing predictions\n",
    "    preds = mask_filler(text)\n",
    "    for pred in preds:\n",
    "        print(f\">>> {pred['sequence']} -> {pred['score']:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5829d02d",
   "metadata": {},
   "source": [
    "Let's also load the base version which I started with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e63d4d55",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at distilbert-base-uncased were not used when initializing TFDistilBertForMaskedLM: ['activation_13']\n",
      "- This IS expected if you are initializing TFDistilBertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFDistilBertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFDistilBertForMaskedLM were initialized from the model checkpoint at distilbert-base-uncased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFDistilBertForMaskedLM for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "base_model = TFAutoModelForMaskedLM.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af791ce9",
   "metadata": {},
   "source": [
    "## Comparison\n",
    "\n",
    "#### 1. Fake News\n",
    "\n",
    "First phrase I am targeting is <b>\"fake news\"</b> since it is one of his favourites:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "304bc285",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> it's good news -> 0.12\n",
      ">>> it's breaking news -> 0.04\n",
      ">>> it's bad news -> 0.04\n",
      ">>> it's fox news -> 0.03\n",
      ">>> it's cbs news -> 0.03\n"
     ]
    }
   ],
   "source": [
    "text = \"It's [MASK] news\"\n",
    "\n",
    "fill_mask(base_model, text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "ed61dcb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> it's fake news -> 0.20\n",
      ">>> it's good news -> 0.14\n",
      ">>> it's the news -> 0.09\n",
      ">>> it's bad news -> 0.04\n",
      ">>> it's fox news -> 0.04\n"
     ]
    }
   ],
   "source": [
    "fill_mask(model, text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "717a65fe",
   "metadata": {},
   "source": [
    "<b>Fake news</b> is not part of the original model's suggestions but the fine tuned model does indeed have it as the top option which is the behavior I was going for.\n",
    "\n",
    "#### 2. Make America great\n",
    "\n",
    "\"Make America great again\" is the phrase I came across the most. In my everyday life during his presidency. The phrase is of course very common in his rally speaches as well so let's see what we get:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "9cb68a4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> let's make america proud again! -> 0.14\n",
      ">>> let's make america laugh again! -> 0.07\n",
      ">>> let's make america dance again! -> 0.05\n",
      ">>> let's make america happy again! -> 0.04\n",
      ">>> let's make america sing again! -> 0.03\n"
     ]
    }
   ],
   "source": [
    "# text = 'We will make America [MASK] again!'\n",
    "text = 'Let\\'s make America [MASK] again!'  # great is second\n",
    "# text = 'The [MASK] American nation'  # great is second\n",
    "\n",
    "fill_mask(base_model, text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "10f4977a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> let's make america proud again! -> 0.42\n",
      ">>> let's make america great again! -> 0.07\n",
      ">>> let's make america strong again! -> 0.07\n",
      ">>> let's make america happy again! -> 0.03\n",
      ">>> let's make america stronger again! -> 0.02\n"
     ]
    }
   ],
   "source": [
    "fill_mask(model, text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a452b9a6",
   "metadata": {},
   "source": [
    "Not exactly the result I expected. <b>great</b> is in second place which is not terrible but the level of confidence is rather low. Some more training would likely improve it but I'll leave it as is for now. <b>proud</b> on the other hand is at the very top with a very high confidence level. I dove into the data a bit a it does seem that he is using proud more often than great. \"Proud\" is also present in more diverse phrases like \"the proud American nation\" and such which does explain it showing up at the top.\n",
    "\n",
    "#### 3. One great nation\n",
    "\n",
    "While going over the data I spotted this phrase which was being used quite often so let's see what we get:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "e0a22b16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> one hundred nation -> 0.04\n",
      ">>> one - nation -> 0.04\n",
      ">>> one sovereign nation -> 0.02\n",
      ">>> one african nation -> 0.02\n",
      ">>> one nation nation -> 0.02\n"
     ]
    }
   ],
   "source": [
    "text = 'One [MASK] nation'\n",
    "\n",
    "fill_mask(base_model, text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "4c1946d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> one great nation -> 0.09\n",
      ">>> one proud nation -> 0.04\n",
      ">>> one strong nation -> 0.04\n",
      ">>> one - nation -> 0.04\n",
      ">>> one more nation -> 0.03\n"
     ]
    }
   ],
   "source": [
    "fill_mask(model, text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "116f3035",
   "metadata": {},
   "source": [
    "Great is at the top followed by proud with which I am happy with."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "047ad47a",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f526503b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
