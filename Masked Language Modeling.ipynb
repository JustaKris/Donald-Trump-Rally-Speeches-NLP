{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b1e61527",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d9797544",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "\n",
    "from transformers import AutoTokenizer, TFAutoModelForMaskedLM\n",
    "from datasets import Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b172c36b",
   "metadata": {},
   "source": [
    "## Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3b6c7ffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "%store -r DT_rally_speaches_dataset\n",
    "df = DT_rally_speaches_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9fa64f88",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Location</th>\n",
       "      <th>Month</th>\n",
       "      <th>Year</th>\n",
       "      <th>filename</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Battle Creek</td>\n",
       "      <td>Dec</td>\n",
       "      <td>2019</td>\n",
       "      <td>BattleCreekDec19_2019.txt</td>\n",
       "      <td>Thank you. Thank you. Thank you to Vice Presid...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Bemidji</td>\n",
       "      <td>Sep</td>\n",
       "      <td>2020</td>\n",
       "      <td>BemidjiSep18_2020.txt</td>\n",
       "      <td>There's a lot of people. That's great. Thank y...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Charleston</td>\n",
       "      <td>Feb</td>\n",
       "      <td>2020</td>\n",
       "      <td>CharlestonFeb28_2020.txt</td>\n",
       "      <td>Thank you. Thank you. Thank you. All I can say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Charlotte</td>\n",
       "      <td>Mar</td>\n",
       "      <td>2020</td>\n",
       "      <td>CharlotteMar2_2020.txt</td>\n",
       "      <td>I want to thank you very much. North Carolina,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Cincinnati</td>\n",
       "      <td>Aug</td>\n",
       "      <td>2019</td>\n",
       "      <td>CincinnatiAug1_2019.txt</td>\n",
       "      <td>Thank you all. Thank you very much. Thank you ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Colorador Springs</td>\n",
       "      <td>Feb</td>\n",
       "      <td>2020</td>\n",
       "      <td>ColoradorSpringsFeb20_2020.txt</td>\n",
       "      <td>Hello Colorado. We love Colorado, most beautif...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Dallas</td>\n",
       "      <td>Oct</td>\n",
       "      <td>2019</td>\n",
       "      <td>DallasOct17_2019.txt</td>\n",
       "      <td>Thank you. Thank you very much. Hello Dallas. ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Des Moines</td>\n",
       "      <td>Jan</td>\n",
       "      <td>2020</td>\n",
       "      <td>DesMoinesJan30_2020.txt</td>\n",
       "      <td>I worked so hard for this state. I worked so h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Fayetteville</td>\n",
       "      <td>Sep</td>\n",
       "      <td>2020</td>\n",
       "      <td>FayettevilleSep19_2020.txt</td>\n",
       "      <td>What a crowd, what a crowd. Get those people o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Fayetteville</td>\n",
       "      <td>Sep</td>\n",
       "      <td>2019</td>\n",
       "      <td>FayettevilleSep9_2019.txt</td>\n",
       "      <td>Thank you everybody. Thank you and Vice Presi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Freeland</td>\n",
       "      <td>Sep</td>\n",
       "      <td>2020</td>\n",
       "      <td>FreelandSep10_2020.txt</td>\n",
       "      <td>We brought you a lot of car plants, Michigan. ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Greenville</td>\n",
       "      <td>Jul</td>\n",
       "      <td>2019</td>\n",
       "      <td>GreenvilleJul17_2019.txt</td>\n",
       "      <td>Thank you very much. Thank you. Thank you. Tha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Henderson</td>\n",
       "      <td>Sep</td>\n",
       "      <td>2020</td>\n",
       "      <td>HendersonSep13_2020.txt</td>\n",
       "      <td>Thank you, thank you. Wow. Wow, and I'm thrill...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Hershey</td>\n",
       "      <td>Dec</td>\n",
       "      <td>2019</td>\n",
       "      <td>HersheyDec10_2019.txt</td>\n",
       "      <td>Well, thank you to Vice President Pence. Than...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Las Vegas</td>\n",
       "      <td>Feb</td>\n",
       "      <td>2020</td>\n",
       "      <td>LasVegasFeb21_2020.txt</td>\n",
       "      <td>Well, thank you very much. And hello Las Vegas...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Latrobe</td>\n",
       "      <td>Sep</td>\n",
       "      <td>2020</td>\n",
       "      <td>LatrobeSep3_2020.txt</td>\n",
       "      <td>So thank you Pennsylvania, very much. I'm thri...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Lexington</td>\n",
       "      <td>Nov</td>\n",
       "      <td>2019</td>\n",
       "      <td>LexingtonNov4_2019.txt</td>\n",
       "      <td>Thank you very much and thank you to the origi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Milwaukee</td>\n",
       "      <td>Jan</td>\n",
       "      <td>2020</td>\n",
       "      <td>MilwaukeeJan14_2020.txt</td>\n",
       "      <td>Well thank you very much. And I'm thrilled to ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Minden</td>\n",
       "      <td>Sep</td>\n",
       "      <td>2020</td>\n",
       "      <td>MindenSep12_2020.txt</td>\n",
       "      <td>Well, I thank you very much. So I want to star...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Minneapolis</td>\n",
       "      <td>Oct</td>\n",
       "      <td>2019</td>\n",
       "      <td>MinneapolisOct10_2019.txt</td>\n",
       "      <td>Thank you very much. Thank you, Minnesota. Thi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Mosinee</td>\n",
       "      <td>Sep</td>\n",
       "      <td>2020</td>\n",
       "      <td>MosineeSep17_2020.txt</td>\n",
       "      <td>Thank you, thank you very much. Thank you very...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>New Hampshire</td>\n",
       "      <td>Aug</td>\n",
       "      <td>2019</td>\n",
       "      <td>NewHampshireAug15_2019.txt</td>\n",
       "      <td>Thank you very much everybody. Thank you. Wow...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>New Hampshire</td>\n",
       "      <td>Aug</td>\n",
       "      <td>2020</td>\n",
       "      <td>NewHampshireAug28_2020.txt</td>\n",
       "      <td>Hello, everybody. Hello, everybody. Wow. Hello...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>New Hampshire</td>\n",
       "      <td>Feb</td>\n",
       "      <td>2020</td>\n",
       "      <td>NewHampshireFeb10_2020.txt</td>\n",
       "      <td>Hello, Manchester, and I am thrilled to be in ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>New Mexico</td>\n",
       "      <td>Sep</td>\n",
       "      <td>2019</td>\n",
       "      <td>NewMexicoSep16_2019.txt</td>\n",
       "      <td>Wow, thank you. Thank you, New Mexico. Thank ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Ohio</td>\n",
       "      <td>Sep</td>\n",
       "      <td>2020</td>\n",
       "      <td>OhioSep21_2020.txt</td>\n",
       "      <td>Wow, that's a big crowd. This is a big crowd. ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>Phoenix</td>\n",
       "      <td>Feb</td>\n",
       "      <td>2020</td>\n",
       "      <td>PhoenixFeb19_2020.txt</td>\n",
       "      <td>Thank you very much, Phoenix. We love to be b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Pittsburgh</td>\n",
       "      <td>Sep</td>\n",
       "      <td>2020</td>\n",
       "      <td>PittsburghSep22_2020.txt</td>\n",
       "      <td>Doesn't have the power. Doesn't have the stayi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>Texas</td>\n",
       "      <td>Sep</td>\n",
       "      <td>2019</td>\n",
       "      <td>TexasSep23_2019.txt</td>\n",
       "      <td>Hello, Houston. I am so thrilled to be here in...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>Toledo</td>\n",
       "      <td>Jan</td>\n",
       "      <td>2020</td>\n",
       "      <td>ToledoJan9_2020.txt</td>\n",
       "      <td>Well, thank you very much. Vice President Mike...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>Tulsa</td>\n",
       "      <td>Jun</td>\n",
       "      <td>2020</td>\n",
       "      <td>TulsaJun20_2020.txt</td>\n",
       "      <td>Thank you, thank you. So we begin, Oklahoma, w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>Tupelo</td>\n",
       "      <td>Nov</td>\n",
       "      <td>2019</td>\n",
       "      <td>TupeloNov1_2019.txt</td>\n",
       "      <td>ell, thank you very much. And hello, Tupelo. T...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>Wildwood</td>\n",
       "      <td>Jan</td>\n",
       "      <td>2020</td>\n",
       "      <td>WildwoodJan28_2020.txt</td>\n",
       "      <td>Thank you. Thank you. I love New Jersey and I'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>Salem</td>\n",
       "      <td>Sep</td>\n",
       "      <td>2020</td>\n",
       "      <td>Winston-SalemSep8_2020.txt</td>\n",
       "      <td>Well, thank you very much. Thank you. Thank yo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>Yuma</td>\n",
       "      <td>Aug</td>\n",
       "      <td>2020</td>\n",
       "      <td>YumaAug18_2020.txt</td>\n",
       "      <td>Oh, thank you very much, everybody. Thank you....</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Location Month  Year                        filename  \\\n",
       "0         Battle Creek   Dec  2019       BattleCreekDec19_2019.txt   \n",
       "1              Bemidji   Sep  2020           BemidjiSep18_2020.txt   \n",
       "2           Charleston   Feb  2020        CharlestonFeb28_2020.txt   \n",
       "3            Charlotte   Mar  2020          CharlotteMar2_2020.txt   \n",
       "4           Cincinnati   Aug  2019         CincinnatiAug1_2019.txt   \n",
       "5    Colorador Springs   Feb  2020  ColoradorSpringsFeb20_2020.txt   \n",
       "6               Dallas   Oct  2019            DallasOct17_2019.txt   \n",
       "7           Des Moines   Jan  2020         DesMoinesJan30_2020.txt   \n",
       "8         Fayetteville   Sep  2020      FayettevilleSep19_2020.txt   \n",
       "9         Fayetteville   Sep  2019       FayettevilleSep9_2019.txt   \n",
       "10            Freeland   Sep  2020          FreelandSep10_2020.txt   \n",
       "11          Greenville   Jul  2019        GreenvilleJul17_2019.txt   \n",
       "12           Henderson   Sep  2020         HendersonSep13_2020.txt   \n",
       "13             Hershey   Dec  2019           HersheyDec10_2019.txt   \n",
       "14           Las Vegas   Feb  2020          LasVegasFeb21_2020.txt   \n",
       "15             Latrobe   Sep  2020            LatrobeSep3_2020.txt   \n",
       "16           Lexington   Nov  2019          LexingtonNov4_2019.txt   \n",
       "17           Milwaukee   Jan  2020         MilwaukeeJan14_2020.txt   \n",
       "18              Minden   Sep  2020            MindenSep12_2020.txt   \n",
       "19         Minneapolis   Oct  2019       MinneapolisOct10_2019.txt   \n",
       "20             Mosinee   Sep  2020           MosineeSep17_2020.txt   \n",
       "21       New Hampshire   Aug  2019      NewHampshireAug15_2019.txt   \n",
       "22       New Hampshire   Aug  2020      NewHampshireAug28_2020.txt   \n",
       "23       New Hampshire   Feb  2020      NewHampshireFeb10_2020.txt   \n",
       "24          New Mexico   Sep  2019         NewMexicoSep16_2019.txt   \n",
       "25                Ohio   Sep  2020              OhioSep21_2020.txt   \n",
       "26             Phoenix   Feb  2020           PhoenixFeb19_2020.txt   \n",
       "27          Pittsburgh   Sep  2020        PittsburghSep22_2020.txt   \n",
       "28               Texas   Sep  2019             TexasSep23_2019.txt   \n",
       "29              Toledo   Jan  2020             ToledoJan9_2020.txt   \n",
       "30               Tulsa   Jun  2020             TulsaJun20_2020.txt   \n",
       "31              Tupelo   Nov  2019             TupeloNov1_2019.txt   \n",
       "32            Wildwood   Jan  2020          WildwoodJan28_2020.txt   \n",
       "33               Salem   Sep  2020      Winston-SalemSep8_2020.txt   \n",
       "34                Yuma   Aug  2020              YumaAug18_2020.txt   \n",
       "\n",
       "                                              content  \n",
       "0   Thank you. Thank you. Thank you to Vice Presid...  \n",
       "1   There's a lot of people. That's great. Thank y...  \n",
       "2   Thank you. Thank you. Thank you. All I can say...  \n",
       "3   I want to thank you very much. North Carolina,...  \n",
       "4   Thank you all. Thank you very much. Thank you ...  \n",
       "5   Hello Colorado. We love Colorado, most beautif...  \n",
       "6   Thank you. Thank you very much. Hello Dallas. ...  \n",
       "7   I worked so hard for this state. I worked so h...  \n",
       "8   What a crowd, what a crowd. Get those people o...  \n",
       "9    Thank you everybody. Thank you and Vice Presi...  \n",
       "10  We brought you a lot of car plants, Michigan. ...  \n",
       "11  Thank you very much. Thank you. Thank you. Tha...  \n",
       "12  Thank you, thank you. Wow. Wow, and I'm thrill...  \n",
       "13   Well, thank you to Vice President Pence. Than...  \n",
       "14  Well, thank you very much. And hello Las Vegas...  \n",
       "15  So thank you Pennsylvania, very much. I'm thri...  \n",
       "16  Thank you very much and thank you to the origi...  \n",
       "17  Well thank you very much. And I'm thrilled to ...  \n",
       "18  Well, I thank you very much. So I want to star...  \n",
       "19  Thank you very much. Thank you, Minnesota. Thi...  \n",
       "20  Thank you, thank you very much. Thank you very...  \n",
       "21   Thank you very much everybody. Thank you. Wow...  \n",
       "22  Hello, everybody. Hello, everybody. Wow. Hello...  \n",
       "23  Hello, Manchester, and I am thrilled to be in ...  \n",
       "24   Wow, thank you. Thank you, New Mexico. Thank ...  \n",
       "25  Wow, that's a big crowd. This is a big crowd. ...  \n",
       "26   Thank you very much, Phoenix. We love to be b...  \n",
       "27  Doesn't have the power. Doesn't have the stayi...  \n",
       "28  Hello, Houston. I am so thrilled to be here in...  \n",
       "29  Well, thank you very much. Vice President Mike...  \n",
       "30  Thank you, thank you. So we begin, Oklahoma, w...  \n",
       "31  ell, thank you very much. And hello, Tupelo. T...  \n",
       "32  Thank you. Thank you. I love New Jersey and I'...  \n",
       "33  Well, thank you very much. Thank you. Thank yo...  \n",
       "34  Oh, thank you very much, everybody. Thank you....  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5d496a86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rallies = [text for text in df[\"content\"]]\n",
    "# rallies_dict = {\n",
    "#     'train': rallies[:30],\n",
    "#     'test': rallies[30:],\n",
    "#     'unsupervised': rallies\n",
    "# }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7e604597",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rallies_dict2 = {\n",
    "#     'rallies': rallies\n",
    "# }\n",
    "# dataset = Dataset.from_dict(rallies_dict2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0b52ea6",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "91fabcbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at distilbert-base-uncased were not used when initializing TFDistilBertForMaskedLM: ['activation_13']\n",
      "- This IS expected if you are initializing TFDistilBertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFDistilBertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFDistilBertForMaskedLM were initialized from the model checkpoint at distilbert-base-uncased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFDistilBertForMaskedLM for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "model_checkpoint = \"distilbert-base-uncased\"\n",
    "model = TFAutoModelForMaskedLM.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "91c8371a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"tf_distil_bert_for_masked_lm\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " distilbert (TFDistilBertMai  multiple                 66362880  \n",
      " nLayer)                                                         \n",
      "                                                                 \n",
      " vocab_transform (Dense)     multiple                  590592    \n",
      "                                                                 \n",
      " vocab_layer_norm (LayerNorm  multiple                 1536      \n",
      " alization)                                                      \n",
      "                                                                 \n",
      " vocab_projector (TFDistilBe  multiple                 23866170  \n",
      " rtLMHead)                                                       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 66,985,530\n",
      "Trainable params: 66,985,530\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "445f7290",
   "metadata": {},
   "source": [
    "## Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d15fcc26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# text = \"This is a great [MASK].\"\n",
    "# text = \"Make [MASK] great again!\"\n",
    "# text = \"[MASK] virus\"\n",
    "text = \"kung [MASK]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4cafc44a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "db7e3136",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> kung fu\n",
      ">>> kung chung\n",
      ">>> kung .\n",
      ">>> kung kung\n",
      ">>> kung !\n"
     ]
    }
   ],
   "source": [
    "inputs = tokenizer(text, return_tensors=\"np\")\n",
    "token_logits = model(**inputs).logits\n",
    "# Find the location of [MASK] and extract its logits\n",
    "mask_token_index = np.argwhere(inputs[\"input_ids\"] == tokenizer.mask_token_id)[0, 1]\n",
    "mask_token_logits = token_logits[0, mask_token_index, :]\n",
    "# Pick the [MASK] candidates with the highest logits\n",
    "# We negate the array before argsort to get the largest, not the smallest, logits\n",
    "top_5_tokens = np.argsort(-mask_token_logits)[:5].tolist()\n",
    "\n",
    "for token in top_5_tokens:\n",
    "    print(f\">>> {text.replace(tokenizer.mask_token, tokenizer.decode([token]))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6bacd68a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['Location', 'Month', 'Year', 'filename', 'content'],\n",
       "    num_rows: 35\n",
       "})"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = Dataset.from_pandas(df)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8ad1462c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/35 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (24291 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input_ids', 'attention_mask', 'word_ids'],\n",
       "    num_rows: 35\n",
       "})"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def tokenize_function(examples):\n",
    "    result = tokenizer(examples[\"content\"])\n",
    "    if tokenizer.is_fast:\n",
    "        result[\"word_ids\"] = [result.word_ids(i) for i in range(len(result[\"input_ids\"]))]\n",
    "    return result\n",
    "\n",
    "\n",
    "# Use batched=True to activate fast multithreading!\n",
    "tokenized_datasets = dataset.map(\n",
    "    tokenize_function, batched=True, remove_columns=['Location', 'Month', 'Year', 'filename', 'content']\n",
    ")\n",
    "tokenized_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "96913d1c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "512"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.model_max_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0675e8a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_size = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "42f7f5da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'>>> Rally 0 length: 24291'\n",
      "'>>> Rally 1 length: 22976'\n",
      "'>>> Rally 2 length: 12491'\n"
     ]
    }
   ],
   "source": [
    "tokenized_samples = tokenized_datasets[:3]\n",
    "\n",
    "for idx, sample in enumerate(tokenized_samples[\"input_ids\"]):\n",
    "    print(f\"'>>> Rally {idx} length: {len(sample)}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "114eab73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'>>> Concatenated reviews length: 59758'\n"
     ]
    }
   ],
   "source": [
    "concatenated_examples = {\n",
    "    k: sum(tokenized_samples[k], []) for k in tokenized_samples.keys()\n",
    "}\n",
    "total_length = len(concatenated_examples[\"input_ids\"])\n",
    "print(f\"'>>> Concatenated reviews length: {total_length}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b93fa722",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 110'\n"
     ]
    }
   ],
   "source": [
    "chunks = {\n",
    "    k: [t[i : i + chunk_size] for i in range(0, total_length, chunk_size)]\n",
    "    for k, t in concatenated_examples.items()\n",
    "}\n",
    "\n",
    "for chunk in chunks[\"input_ids\"]:\n",
    "    print(f\"'>>> Chunk length: {len(chunk)}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "69cb7181",
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_texts(examples):\n",
    "    # Concatenate all texts\n",
    "    concatenated_examples = {k: sum(examples[k], []) for k in examples.keys()}\n",
    "    # Compute length of concatenated texts\n",
    "    total_length = len(concatenated_examples[list(examples.keys())[0]])\n",
    "    # We drop the last chunk if it's smaller than chunk_size\n",
    "    total_length = (total_length // chunk_size) * chunk_size\n",
    "    # Split by chunks of max_len\n",
    "    result = {\n",
    "        k: [t[i : i + chunk_size] for i in range(0, total_length, chunk_size)]\n",
    "        for k, t in concatenated_examples.items()\n",
    "    }\n",
    "    # Create a new labels column\n",
    "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cf5383ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/35 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input_ids', 'attention_mask', 'word_ids', 'labels'],\n",
       "    num_rows: 3864\n",
       "})"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm_datasets = tokenized_datasets.map(group_texts, batched=True)\n",
    "lm_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "67112a76",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'when i first started this beautiful trip, this beautiful journey, i just said to the first lady, \" you are so lucky. i took you on this fantastic journey. it\\'s so much fun. they want to impeach you. they want to do worse than that. \" by the way, by the way, by the way, it doesn\\'t really feel like we\\'re being impeached. the country is doing better than ever before. we did nothing wrong. we did nothing wrong. and we have tremendous support in the republican party like we\\'ve never had before. nobody\\'s ever had this kind'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(lm_datasets[1][\"input_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "965ff1df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'when i first started this beautiful trip, this beautiful journey, i just said to the first lady, \" you are so lucky. i took you on this fantastic journey. it\\'s so much fun. they want to impeach you. they want to do worse than that. \" by the way, by the way, by the way, it doesn\\'t really feel like we\\'re being impeached. the country is doing better than ever before. we did nothing wrong. we did nothing wrong. and we have tremendous support in the republican party like we\\'ve never had before. nobody\\'s ever had this kind'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(lm_datasets[1][\"labels\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "301800e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForLanguageModeling\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm_probability=0.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c5f5ac29",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a DistilBertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "'>>> [CLS] thank [MASK]. thank you. thank you to vice president pence. he's a good guy [MASK] we've done a great job together. [MASK] merry christmas, michigan.ক you, michigan. what a [MASK] we had in michigan. what a victory was that. one of the greats. was that the greatest evening? but i'm thrilled to be here with thousands of hardworking patriots as we celebrate the miracle of christmas [MASK] the great [MASK] paisley america and [MASK] glory of god [MASK] [MASK] you very much. and did you notice that everybody is saying merry christmas again? [MASK] you notice? saying taiwan christmas [MASK] i remember'\n",
      "\n",
      "'>>> [MASK] [MASK] first [MASK] this beautiful trip, this beautiful journey, i just said to the first lady, \" you are so lucky. i scully you on [MASK] [MASK] journey. it'raging so much fun. they want to impeach you. they want to do worse than that. \" by the way, by the way, by the [MASK], it doesn [MASK] t really feel like we're being impeached. the country is doing better than ever before. we did [MASK] rhineland. we [MASK] nothing [MASK]. and we [MASK] tremendous support in the republican party exhaustion we've never had before. nobody's ever [MASK] this kind'\n"
     ]
    }
   ],
   "source": [
    "samples = [lm_datasets[i] for i in range(2)]\n",
    "for sample in samples:\n",
    "    _ = sample.pop(\"word_ids\")\n",
    "\n",
    "for chunk in data_collator(samples)[\"input_ids\"]:\n",
    "    print(f\"\\n'>>> {tokenizer.decode(chunk)}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d9d21e7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import numpy as np\n",
    "\n",
    "from transformers.data.data_collator import tf_default_data_collator\n",
    "\n",
    "wwm_probability = 0.2\n",
    "\n",
    "\n",
    "def whole_word_masking_data_collator(features):\n",
    "    for feature in features:\n",
    "        word_ids = feature.pop(\"word_ids\")\n",
    "\n",
    "        # Create a map between words and corresponding token indices\n",
    "        mapping = collections.defaultdict(list)\n",
    "        current_word_index = -1\n",
    "        current_word = None\n",
    "        for idx, word_id in enumerate(word_ids):\n",
    "            if word_id is not None:\n",
    "                if word_id != current_word:\n",
    "                    current_word = word_id\n",
    "                    current_word_index += 1\n",
    "                mapping[current_word_index].append(idx)\n",
    "\n",
    "        # Randomly mask words\n",
    "        mask = np.random.binomial(1, wwm_probability, (len(mapping),))\n",
    "        input_ids = feature[\"input_ids\"]\n",
    "        labels = feature[\"labels\"]\n",
    "        new_labels = [-100] * len(labels)\n",
    "        for word_id in np.where(mask)[0]:\n",
    "            word_id = word_id.item()\n",
    "            for idx in mapping[word_id]:\n",
    "                new_labels[idx] = labels[idx]\n",
    "                input_ids[idx] = tokenizer.mask_token_id\n",
    "        feature[\"labels\"] = new_labels\n",
    "\n",
    "    return tf_default_data_collator(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d1069499",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "'>>> [CLS] thank you. thank [MASK]. thank you [MASK] vice president pence. he's a good guy. we [MASK] ve done [MASK] great job together. and merry christmas, michigan. [MASK] you, michigan. [MASK] a [MASK] we had [MASK] michigan. what [MASK] victory was that. [MASK] of the [MASK] [MASK]. was that [MASK] greatest [MASK]? but i'm thrilled to be here with thousands of hardworking patriots as we celebrate the miracle of [MASK], the [MASK] [MASK] of america [MASK] the glory of god. [MASK] [MASK] very much. [MASK] [MASK] you notice that everybody is saying merry christmas again [MASK] [MASK] you [MASK]? saying merry christmas [MASK] i remember'\n",
      "\n",
      "'>>> when i first started this beautiful trip, this beautiful [MASK] [MASK] [MASK] [MASK] [MASK] to the first lady, [MASK] you are so lucky. i took you [MASK] this fantastic journey. it's so much [MASK] [MASK] they want to impeach [MASK]. they [MASK] to do worse than [MASK]. \" by the way, by [MASK] way [MASK] by the way, it [MASK]'t really feel like we'[MASK] being impeached. the country [MASK] [MASK] better than ever before. we did nothing wrong. [MASK] did nothing wrong [MASK] and we have tremendous support in the republican party like we've never had before [MASK] nobody's ever had this kind'\n"
     ]
    }
   ],
   "source": [
    "samples = [lm_datasets[i] for i in range(2)]\n",
    "batch = whole_word_masking_data_collator(samples)\n",
    "\n",
    "for chunk in batch[\"input_ids\"]:\n",
    "    print(f\"\\n'>>> {tokenizer.decode(chunk)}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "78d83934",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'word_ids', 'labels'],\n",
       "        num_rows: 3500\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'word_ids', 'labels'],\n",
       "        num_rows: 350\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_size = 3500\n",
    "test_size = int(0.1 * train_size)\n",
    "\n",
    "downsampled_dataset = lm_datasets.train_test_split(\n",
    "    train_size=train_size, test_size=test_size, seed=42\n",
    ")\n",
    "downsampled_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f5ffb4cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_train_dataset = model.prepare_tf_dataset(\n",
    "    downsampled_dataset[\"train\"],\n",
    "    collate_fn=data_collator,\n",
    "    shuffle=True,\n",
    "    batch_size=32,\n",
    ")\n",
    "\n",
    "tf_eval_dataset = model.prepare_tf_dataset(\n",
    "    downsampled_dataset[\"test\"],\n",
    "    collate_fn=data_collator,\n",
    "    shuffle=False,\n",
    "    batch_size=32,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c7bc4022",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No loss specified in compile() - the model's internal loss computation will be used as the loss. Don't panic - this is a common way to train TensorFlow models in Transformers! To disable this behaviour please pass a loss argument, or explicitly pass `loss=None` if you do not want your model to compute a loss.\n"
     ]
    }
   ],
   "source": [
    "from transformers import create_optimizer\n",
    "from transformers.keras_callbacks import PushToHubCallback\n",
    "import tensorflow as tf\n",
    "\n",
    "num_train_steps = len(tf_train_dataset)\n",
    "optimizer, schedule = create_optimizer(\n",
    "    init_lr=2e-5,\n",
    "    num_warmup_steps=1_000,\n",
    "    num_train_steps=num_train_steps,\n",
    "    weight_decay_rate=0.01,\n",
    ")\n",
    "model.compile(optimizer=optimizer)\n",
    "\n",
    "# Train in mixed-precision float16\n",
    "tf.keras.mixed_precision.set_global_policy(\"mixed_float16\")\n",
    "\n",
    "model_name = model_checkpoint.split(\"/\")[-1]\n",
    "# callback = PushToHubCallback(\n",
    "#     output_dir=f\"{model_name}-finetuned-imdb\", tokenizer=tokenizer\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "31eeb9e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/11 [==============================] - 26s 2s/step - loss: 2.6719\n",
      "Perplexity: 14.47\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "eval_loss = model.evaluate(tf_eval_dataset)\n",
    "print(f\"Perplexity: {math.exp(eval_loss):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "aa27c96b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "109/109 [==============================] - 653s 6s/step - loss: 2.5111 - val_loss: 2.3096\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x186e8d77ca0>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(tf_train_dataset, validation_data=tf_eval_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "1cf566c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/11 [==============================] - 22s 2s/step - loss: 2.2259\n",
      "Perplexity: 9.26\n"
     ]
    }
   ],
   "source": [
    "eval_loss = model.evaluate(tf_eval_dataset)\n",
    "print(f\"Perplexity: {math.exp(eval_loss):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "59f8e04f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "mask_filler = pipeline(\n",
    "    \"fill-mask\", model=model, tokenizer=tokenizer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "9168d401",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> this is a great deal.\n",
      ">>> this is a great success.\n",
      ">>> this is a great adventure.\n",
      ">>> this is a great idea.\n",
      ">>> this is a great shame.\n"
     ]
    }
   ],
   "source": [
    "preds = mask_filler(text)\n",
    "\n",
    "for pred in preds:\n",
    "    print(f\">>> {pred['sequence']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13749f14",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a61783e4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
