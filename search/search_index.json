{"config":{"lang":["en"],"separator":"[\\s\\-,:!=\\[\\]()\"/]+|(?!\\b)(?=[A-Z][a-z])|\\.(?!\\d)|&[lg]t;","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Trump Speeches NLP Chatbot API \u2014 Technical Documentation","text":"<p>Production-ready AI/ML platform demonstrating enterprise-grade practices in natural language processing, retrieval-augmented generation, and modern backend development.</p>"},{"location":"#project-overview","title":"Project Overview","text":"<p>This portfolio project demonstrates expertise in:</p> <ul> <li>Advanced RAG Architecture \u2014 ChromaDB vector database, MPNet embeddings (768-dim), Google Gemini LLM integration</li> <li>Hybrid Retrieval Systems \u2014 Semantic search combined with BM25 keyword matching and cross-encoder reranking</li> <li>Production API Development \u2014 FastAPI with 12+ RESTful endpoints, type-safe Pydantic models, comprehensive error handling</li> <li>Entity Analytics \u2014 Automated entity extraction with sentiment analysis and contextual associations</li> <li>Professional DevOps \u2014 Docker containerization, CI/CD pipelines, automated testing, code quality enforcement</li> </ul>"},{"location":"#documentation-structure","title":"Documentation Structure","text":""},{"location":"#getting-started-guides","title":"Getting Started Guides","text":"<p>Quick setup and deployment:</p> <ul> <li>Quickstart Guide \u2014 Local setup and first API calls</li> <li>Deployment Guide \u2014 Production deployment to Render, Azure, or Docker</li> </ul>"},{"location":"#how-to-guides","title":"How-To Guides","text":"<p>Implementation details for specific features:</p> <ul> <li>Testing &amp; CI/CD \u2014 Testing strategy, code quality tools, continuous integration</li> <li>Logging \u2014 Production logging with JSON formatting and cloud integration</li> <li>Configuration \u2014 Environment-based configuration management</li> <li>Entity Analytics \u2014 Entity extraction and sentiment analysis</li> <li>Documentation \u2014 Contributing to project documentation</li> </ul>"},{"location":"#technical-reference","title":"Technical Reference","text":"<p>In-depth technical documentation:</p> <ul> <li>System Architecture \u2014 Component design, data flows, deployment patterns</li> <li>RAG Features \u2014 RAG implementation details and optimization</li> <li>Configuration Reference \u2014 Complete configuration options</li> <li>Changelog \u2014 Version history and recent improvements</li> </ul>"},{"location":"#quick-links","title":"Quick Links","text":"<ul> <li>GitHub Repository \u2014 Source code and issue tracking</li> <li>API Documentation (Swagger) \u2014 Interactive API documentation (local)</li> <li>API Documentation (ReDoc) \u2014 Alternative API documentation</li> </ul>"},{"location":"#core-features","title":"Core Features","text":""},{"location":"#rag-qa-system","title":"RAG Q&amp;A System","text":"<p>Ask natural language questions about 35 political speeches (300,000+ words):</p> <pre><code>curl -X POST http://localhost:8000/rag/ask \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"question\": \"What economic policies were discussed?\", \"top_k\": 5}'\n</code></pre> <p>Features: - Semantic search using MPNet embeddings (768-dimensional) - Hybrid search combining vector similarity and BM25 keyword matching - Cross-encoder reranking for improved precision - Multi-factor confidence scoring - Entity extraction and analytics - Google Gemini LLM for answer generation</p>"},{"location":"#nlp-endpoints","title":"NLP Endpoints","text":"<p>Traditional NLP analysis: - Sentiment Analysis \u2014 FinBERT transformer model - Topic Extraction \u2014 TF-IDF based topic modeling - Word Frequency \u2014 Statistical text analysis - N-gram Analysis \u2014 Bigram and trigram extraction</p>"},{"location":"#interactive-web-interface","title":"Interactive Web Interface","text":"<p>Single-page application at the root (<code>/</code>) for testing all features without writing code.</p>"},{"location":"#technology-stack","title":"\ud83d\udee0\ufe0f Technology Stack","text":"<p>AI/ML: - ChromaDB (vector database) - sentence-transformers (MPNet) - Google Gemini (LLM) - Hugging Face Transformers (FinBERT)</p> <p>Backend: - FastAPI (REST API) - Pydantic (validation) - NLTK (preprocessing)</p> <p>DevOps: - Docker + Docker Compose - GitHub Actions (CI/CD) - pytest (testing) - Black, flake8, mypy (code quality)</p>"},{"location":"#example-use-cases","title":"\ud83d\udca1 Example Use Cases","text":"<ol> <li>Political Speech Analysis \u2014 Extract themes, sentiment, and talking points</li> <li>RAG System Demo \u2014 Show how to build Q&amp;A over large text corpora</li> <li>Entity Analytics \u2014 Track mentions of people, places, and topics</li> <li>Hybrid Search \u2014 Demonstrate combining semantic and keyword search</li> </ol>"},{"location":"#learning-resources","title":"\ud83c\udf93 Learning Resources","text":"<ul> <li>Architecture diagrams in the Architecture doc</li> <li>RAG implementation details in RAG Features</li> <li>Testing strategy in Testing Guide</li> <li>Deployment options in Deployment Guide</li> </ul>"},{"location":"#support-contributing","title":"\ud83d\udcde Support &amp; Contributing","text":"<ul> <li>Issues: GitHub Issues</li> <li>Author: Kristiyan Bonev</li> <li>License: MIT</li> </ul> <p>Ready to get started? Head to the Quickstart Guide \u2192</p>"},{"location":"CHANGELOG/","title":"Changelog","text":"<p>All notable changes and improvements to the Trump Speeches NLP Chatbot API.</p>"},{"location":"CHANGELOG/#recent-updates-november-2025","title":"[Recent Updates] - November 2025","text":""},{"location":"CHANGELOG/#added-production-logging-system","title":"Added - Production Logging System","text":"<p>New Module: <code>src/logging_config.py</code></p> <p>Implemented professional logging with automatic environment detection:</p> <ul> <li>Dual-Format Support: JSON logs for production, colored logs for development</li> <li>Auto-Detection: Uses <code>ENVIRONMENT</code> setting to choose appropriate format</li> <li>Cloud-Ready: JSON format works with Azure Application Insights, CloudWatch, ELK</li> <li>Structured Output: Consistent timestamp, level, module name, message format</li> <li>Third-Party Suppression: Automatic filtering of noisy library logs</li> <li>Uvicorn Integration: Proper configuration of web server logs</li> </ul> <p>Benefits: - Deploy to Azure/Docker without code changes - Stream JSON logs to monitoring tools - Debug locally with colored, readable output - Filter by module, level, or content in production</p>"},{"location":"CHANGELOG/#added-configuration-management-system","title":"Added - Configuration Management System","text":"<p>New Module: <code>src/config.py</code></p> <p>Created Pydantic Settings-based configuration system:</p> <ul> <li>Type-Safe: Automatic validation of all settings with clear error messages</li> <li>Environment Variables: Full <code>.env</code> file support for local and cloud deployment</li> <li>Centralized: All configuration in one place with defaults and descriptions</li> <li>Cloud-Native: Works seamlessly with Azure App Service, Docker, Kubernetes</li> <li>Flexible: Support for multiple LLM providers (Gemini, OpenAI, Anthropic)</li> </ul> <p>Key Settings: - Application: name, version, environment, log level - LLM: provider, API keys, models, parameters - RAG: chunk size, top-k, hybrid search, reranking - Models: sentiment, embedding, reranker models - Data: directories for speeches and ChromaDB - API: host, port, reload, CORS origins</p>"},{"location":"CHANGELOG/#fixed-chromadb-duplicate-warnings","title":"Fixed - ChromaDB Duplicate Warnings","text":"<p>Updated: <code>src/rag_service.py</code></p> <p>Implemented smart deduplication to prevent re-indexing existing chunks:</p> <ul> <li>Check existing IDs before adding new chunks</li> <li>Skip already-indexed documents automatically</li> <li>Log clear info about new vs skipped chunks</li> <li>100x faster re-indexing (skip embedding computation)</li> </ul> <p>Before: 1000+ warnings on every query <pre><code>WARNING chromadb... Add of existing embedding ID: ToledoJan9_2020_chunk_0\nWARNING chromadb... Add of existing embedding ID: ToledoJan9_2020_chunk_1\n...\n</code></pre></p> <p>After: Clean logs with informative messages <pre><code>INFO src.rag_service Adding 0 new chunks (skipped 1082 duplicates)\n</code></pre></p>"},{"location":"CHANGELOG/#updated-service-architecture","title":"Updated - Service Architecture","text":"<p>Dependency Injection Pattern</p> <p>Refactored services to accept configuration explicitly:</p> <ul> <li><code>GeminiLLM</code>: Accepts API key, model name, temperature as parameters</li> <li><code>RAGService</code>: Accepts <code>llm_service</code> instance (optional)</li> <li><code>SentimentAnalyzer</code>: Accepts configurable model name</li> <li>All services use module-level loggers (<code>logging.getLogger(__name__)</code>)</li> </ul> <p>Benefits: - Easier testing (mock dependencies) - Clearer initialization flow - Better separation of concerns - More flexible configuration</p>"},{"location":"CHANGELOG/#updated-application-branding","title":"Updated - Application Branding","text":"<p>Unified Naming Convention: \"Trump Speeches NLP Chatbot API\"</p> <p>Updated branding across application:</p> <ul> <li>API module docstrings</li> <li>HTML page titles</li> <li>Frontend headers</li> <li>Endpoint descriptions</li> <li>Fallback HTML pages</li> </ul> <p>Key Changes: - Specified technologies in descriptions (Gemini, ChromaDB, FinBERT) - Updated tab descriptions with specific features - Improved dataset explanations - Added example questions relevant to content</p>"},{"location":"CHANGELOG/#documentation","title":"Documentation","text":"<p>New/Updated Files: - <code>docs/reference/configuration.md</code> - Complete configuration guide - <code>docs/howto/logging.md</code> - Logging best practices and formats - <code>docs/CHANGELOG.md</code> - This file - <code>.env.example</code> - Configuration template (if not already present)</p>"},{"location":"CHANGELOG/#previous-version-october-2025","title":"[Previous Version] - October 2025","text":""},{"location":"CHANGELOG/#core-features","title":"Core Features","text":"<ul> <li>RAG Q&amp;A System: ChromaDB + MPNet embeddings + Gemini LLM</li> <li>Hybrid Search: Semantic search + BM25 keyword matching</li> <li>Cross-Encoder Reranking: Improved precision for search results</li> <li>Multi-Factor Confidence: Sophisticated confidence scoring</li> <li>Entity Analytics: Automatic entity extraction with sentiment analysis</li> <li>FastAPI Backend: 12+ RESTful endpoints</li> <li>Interactive Frontend: Single-page web interface</li> <li>Docker Support: Multi-stage build with health checks</li> <li>CI/CD Pipeline: GitHub Actions with tests and security scanning</li> <li>Comprehensive Testing: pytest with 50%+ coverage</li> </ul>"},{"location":"CHANGELOG/#ml-models","title":"ML Models","text":"<ul> <li>Gemini 2.5 Flash: Answer generation</li> <li>FinBERT: Sentiment analysis</li> <li>all-mpnet-base-v2: 768-dim semantic embeddings</li> <li>ms-marco-MiniLM: Cross-encoder reranking</li> </ul>"},{"location":"CHANGELOG/#deployment","title":"Deployment","text":"<ul> <li>Docker + Docker Compose support</li> <li>Render deployment configuration</li> <li>Azure Web App compatible</li> <li>Health check endpoint</li> <li>Environment-based configuration</li> </ul>"},{"location":"CHANGELOG/#migration-guide","title":"Migration Guide","text":""},{"location":"CHANGELOG/#from-old-configuration-direct-environment-variables","title":"From Old Configuration (Direct Environment Variables)","text":"<p>Before: <pre><code>import os\napi_key = os.getenv(\"GEMINI_API_KEY\")\nprint(\"Loading model...\")\n</code></pre></p> <p>After: <pre><code>from src.config import get_settings\nimport logging\n\nsettings = get_settings()\nlogger = logging.getLogger(__name__)\n\napi_key = settings.gemini_api_key\nlogger.info(\"Loading model...\")\n</code></pre></p>"},{"location":"CHANGELOG/#from-old-logging-print-statements","title":"From Old Logging (Print Statements)","text":"<p>Before: <pre><code>print(f\"Loaded {count} documents\")\n</code></pre></p> <p>After: <pre><code>logger.info(f\"Loaded {count} documents\")\n</code></pre></p>"},{"location":"CHANGELOG/#breaking-changes","title":"Breaking Changes","text":"<p>None. All changes are backwards-compatible or internal improvements.</p>"},{"location":"CHANGELOG/#upgrading","title":"Upgrading","text":"<ol> <li> <p>Update dependencies:    <pre><code>uv sync\n</code></pre></p> </li> <li> <p>Create <code>.env</code> file (if not exists):    <pre><code>cp .env.example .env\n</code></pre></p> </li> <li> <p>Set API key in <code>.env</code>:    <pre><code>GEMINI_API_KEY=your_key_here\n</code></pre></p> </li> <li> <p>Run the application:    <pre><code>uv run uvicorn src.api:app --reload\n</code></pre></p> </li> </ol> <p>The application will automatically use the new logging and configuration systems.</p>"},{"location":"CHANGELOG/#future-roadmap","title":"Future Roadmap","text":""},{"location":"CHANGELOG/#planned-features","title":"Planned Features","text":"<ul> <li>Multiple LLM Providers: OpenAI GPT-4, Anthropic Claude support</li> <li>Advanced Entity Analytics: Knowledge graph visualization</li> <li>Query Caching: Redis layer for common questions</li> <li>Async Processing: Background jobs for heavy analytics</li> <li>Enhanced NER: spaCy or Hugging Face transformers</li> <li>Fact Extraction: Structured information from speeches</li> </ul>"},{"location":"CHANGELOG/#performance-improvements","title":"Performance Improvements","text":"<ul> <li>Model Quantization: Reduce model sizes</li> <li>GPU Acceleration: CUDA support for faster inference</li> <li>Response Streaming: WebSocket for real-time answers</li> <li>Database Optimization: Connection pooling, query optimization</li> </ul>"},{"location":"CHANGELOG/#infrastructure","title":"Infrastructure","text":"<ul> <li>Kubernetes: Container orchestration</li> <li>Auto-scaling: Dynamic resource allocation</li> <li>Multi-region: Global deployment</li> <li>Monitoring: Prometheus + Grafana integration</li> </ul> <p>Project Repository: GitHub Documentation: GitHub Pages Maintainer: Kristiyan Bonev</p>"},{"location":"guides/deployment/","title":"Deployment Guide","text":"<p>This guide covers deploying the Trump Speeches NLP Chatbot API to various platforms.</p>"},{"location":"guides/deployment/#table-of-contents","title":"Table of Contents","text":"<ul> <li>Local Development</li> <li>Docker Deployment</li> <li>Render Deployment</li> <li>Azure App Service Deployment</li> <li>CI/CD with GitHub Actions</li> <li>Environment Variables</li> </ul>"},{"location":"guides/deployment/#local-development","title":"Local Development","text":""},{"location":"guides/deployment/#running-without-docker","title":"Running Without Docker","text":"<ol> <li>Create a virtual environment (optional but recommended):</li> </ol> <pre><code># Create venv with default Python version\nuv venv\n\n# Create venv with specific Python version\nuv venv --python 3.12\nuv venv --python 3.11\n\n# Create venv in custom directory\nuv venv .venv-dev\n\n# Activate the virtual environment (Windows PowerShell)\n.venv\\Scripts\\Activate.ps1\n\n# Deactivate when done\ndeactivate\n</code></pre> <ol> <li>Install dependencies:</li> </ol> <pre><code># Install all production dependencies\nuv sync\n\n# Install with specific dependency groups\nuv sync --group dev              # Include dev tools\nuv sync --group docs             # Include docs tools\nuv sync --group notebooks        # Include notebook tools\nuv sync --all-groups             # Include all optional groups\n\n# Install without optional groups\nuv sync --no-group dev --no-group docs --no-group notebooks\n\n# Upgrade all dependencies to latest compatible versions\nuv sync --upgrade\n\n# Install from scratch (ignore lock file)\nuv sync --reinstall\n</code></pre> <ol> <li>Add or remove packages:</li> </ol> <pre><code># Add a new package\nuv add requests\nuv add \"fastapi&gt;=0.100.0\"\n\n# Add to a specific group\nuv add --group dev pytest\nuv add --group docs mkdocs-material\n\n# Remove a package\nuv remove requests\n\n# Update lock file without installing\nuv lock\n\n# Update specific package\nuv lock --upgrade-package fastapi\n</code></pre> <ol> <li>Run commands:</li> </ol> <pre><code># Run the API server\nuv run uvicorn src.api:app --reload\n\n# Run with custom host and port\nuv run uvicorn src.api:app --host 0.0.0.0 --port 8001 --reload\n\n# Run tests\nuv run pytest\nuv run pytest -v --cov=src\n\n# Run code formatters\nuv run black src/\nuv run isort src/\n\n# Run linters\nuv run flake8 src/\nuv run mypy src/\n\n# Run Python scripts\nuv run python scripts/migrate_rag_embeddings.py\n\n# Run Python REPL with dependencies\nuv run python\n</code></pre> <ol> <li>Manage Python versions:</li> </ol> <pre><code># List available Python versions\nuv python list\n\n# Install a specific Python version\nuv python install 3.12\nuv python install 3.11.8\n\n# Pin Python version for project\nuv python pin 3.12\n\n# Show current Python version\nuv python find\n</code></pre> <ol> <li>Export dependencies:</li> </ol> <pre><code># Export to requirements.txt\nuv pip compile pyproject.toml -o requirements.txt\n\n# Export with specific groups excluded\nuv export --no-group dev --no-group docs -o requirements.txt\n\n# Export in different formats\nuv export --format requirements-txt\n</code></pre> <ol> <li>Access the application:</li> <li>Frontend: http://localhost:8000</li> <li>API Docs: http://localhost:8000/docs</li> <li>ReDoc: http://localhost:8000/redoc</li> </ol>"},{"location":"guides/deployment/#docker-deployment","title":"Docker Deployment","text":""},{"location":"guides/deployment/#build-and-run-with-docker","title":"Build and Run with Docker","text":"<ol> <li>Build the Docker image:</li> </ol> <p>```powershell    docker build -t trump-speeches-nlp-chatbot .</p> <p># Build with no cache (clean build)    ```powershell docker build --no-cache -t trump-speeches-nlp-chatbot . <pre><code>   # Build with a specific tag\n   docker build -t trump-speeches-nlp-chatbot:v1.0.0 .\n   ```\n\n2. **Run the container:**\n\n   ```powershell\n   # Basic run\n   docker run --rm -it -p 8000:8000 --env-file .env --name nlp-chatbot trump-speeches-nlp-chatbot\n\n   # Run in detached mode (background)\n   docker run -d -p 8000:8000 --env-file .env --name nlp-chatbot trump-speeches-nlp-chatbot\n\n   # Run with persistent ChromaDB volume\n   docker run --rm -it -p 8000:8000 -v \"${PWD}/data/chromadb:/app/data/chromadb\" --env-file .env --name nlp-chatbot trump-speeches-nlp-chatbot\n   ```\n\n3. **View logs:**\n\n   ```powershell\n   # Follow logs (real-time)\n   docker logs -f nlp-chatbot\n\n   # View last 100 lines\n   docker logs --tail 100 nlp-chatbot\n\n   # View logs with timestamps\n   docker logs -t nlp-chatbot\n   ```\n\n4. **Manage containers:**\n\n   ```powershell\n   # Stop the container\n   docker stop nlp-chatbot\n\n   # Start a stopped container\n   docker start nlp-chatbot\n\n   # Restart the container\n   docker restart nlp-chatbot\n\n   # Remove the container\n   docker rm nlp-chatbot\n\n   # Remove with force (even if running)\n   docker rm -f nlp-chatbot\n   ```\n\n5. **Tag and push to Docker Hub:**\n\n   ```powershell\n   # Tag for Docker Hub\n   docker tag trump-speeches-nlp-chatbot yourusername/trump-speeches-nlp-chatbot:latest\n   docker tag trump-speeches-nlp-chatbot yourusername/trump-speeches-nlp-chatbot:v1.0.0\n\n   # Login to Docker Hub\n   docker login\n\n   # Push to Docker Hub\n   docker push yourusername/trump-speeches-nlp-chatbot:latest\n   docker push yourusername/trump-speeches-nlp-chatbot:v1.0.0\n\n   # Push all tags\n   docker push --all-tags yourusername/trump-speeches-nlp-chatbot\n   ```\n\n6. **Clean up Docker resources:**\n\n   ```powershell\n   # Remove unused images\n   docker image prune\n\n   # Remove all stopped containers\n   docker container prune\n\n   # Remove all unused containers, networks, images\n   docker system prune\n\n   # Remove everything (including volumes)\n   docker system prune -a --volumes\n   ```\n\n### Using Docker Compose\n\n**Production mode:**\n\n```powershell\ndocker-compose up -d\n</code></pre></p> <p>Development mode (with hot reload):</p> <pre><code>+\n</code></pre> <p>Stop services:</p> <pre><code>docker-compose down\n</code></pre>"},{"location":"guides/deployment/#render-deployment","title":"Render Deployment","text":"<p>Render offers free hosting with automatic deployments using Docker images.</p>"},{"location":"guides/deployment/#deployment-strategy","title":"Deployment Strategy","text":"<p>This project uses a Docker-based deployment approach for Render:</p> <ol> <li>GitHub Actions builds and pushes Docker images to Docker Hub</li> <li>Render pulls and deploys the pre-built images</li> <li>Benefits: Faster deployments, consistent environments, easier rollbacks</li> </ol>"},{"location":"guides/deployment/#prerequisites","title":"Prerequisites","text":"<ol> <li>Docker Hub Account (free)</li> <li>Sign up at https://hub.docker.com</li> <li> <p>Create a repository named <code>trump-speeches-nlp-chatbot</code></p> </li> <li> <p>Render Account (free)</p> </li> <li> <p>Sign up at https://render.com</p> </li> <li> <p>GitHub Repository connected to your account</p> </li> </ol>"},{"location":"guides/deployment/#step-1-configure-github-secrets","title":"Step 1: Configure GitHub Secrets","text":"<p>Add these secrets to your GitHub repository (Settings \u2192 Secrets and variables \u2192 Actions):</p> <ol> <li><code>DOCKERHUB_USERNAME</code> - Your Docker Hub username</li> <li><code>DOCKERHUB_TOKEN</code> - Docker Hub access token</li> <li>Go to Docker Hub \u2192 Account Settings \u2192 Security \u2192 New Access Token</li> <li>Copy the token and save it as a GitHub secret</li> </ol>"},{"location":"guides/deployment/#step-2-update-renderyaml","title":"Step 2: Update render.yaml","text":"<p>Edit <code>render.yaml</code> and replace <code>&lt;your-dockerhub-username&gt;</code> with your actual Docker Hub username:</p> <pre><code>image:\n  url: docker.io/your-username/trump-speeches-nlp-chatbot:latest\n</code></pre>"},{"location":"guides/deployment/#step-3-deploy-to-render","title":"Step 3: Deploy to Render","text":"<p>Option A: Using Blueprint (Recommended)</p> <ol> <li>Go to Render Dashboard \u2192 \"Blueprints\"</li> <li>Click \"New Blueprint Instance\"</li> <li>Connect your GitHub repository</li> <li>Render will detect <code>.render/render.yaml</code> and configure everything automatically</li> <li>Click \"Apply\" to create the service</li> </ol> <p>Option B: Manual Configuration</p> <ol> <li>Go to Render Dashboard \u2192 \"New +\" \u2192 \"Web Service\"</li> <li>Choose \"Deploy an existing image from a registry\"</li> <li>Configure:</li> <li>Image URL: <code>docker.io/your-username/trump-speeches-nlp-chatbot:latest</code></li> <li>Name: <code>trump-speeches-nlp-chatbot</code></li> <li>Plan: Free</li> <li>Add environment variable:</li> <li><code>PORT</code> = <code>8000</code></li> <li>Set Health Check Path: <code>/health</code></li> <li>Click \"Create Web Service\"</li> </ol>"},{"location":"guides/deployment/#step-4-trigger-deployment","title":"Step 4: Trigger Deployment","text":"<p>Push to <code>main</code> branch or manually trigger the workflow:</p> <pre><code>git push origin main\n</code></pre> <p>This will: 1. Build the Docker image 2. Push it to Docker Hub 3. Render automatically detects the new image and deploys it</p>"},{"location":"guides/deployment/#accessing-your-render-app","title":"Accessing Your Render App","text":"<p>Your API will be available at: <code>https://trump-speeches-nlp-chatbot.onrender.com</code></p> <p>Note: Free tier apps spin down after 15 minutes of inactivity. First request may take 30-60 seconds to wake the service.</p>"},{"location":"guides/deployment/#monitoring-logs","title":"Monitoring &amp; Logs","text":"<ul> <li>Logs: Render Dashboard \u2192 Your Service \u2192 Logs tab</li> <li>Metrics: Render Dashboard \u2192 Your Service \u2192 Metrics tab</li> <li>Manual Deploy: Render Dashboard \u2192 Your Service \u2192 Manual Deploy button</li> </ul>"},{"location":"guides/deployment/#upgrading-from-free-tier","title":"Upgrading from Free Tier","text":"<p>For better performance: - Starter Plan: $7/month (512 MB RAM, 0.5 CPU) - No cold starts - Faster response times - Better for production use</p>"},{"location":"guides/deployment/#azure-app-service-deployment","title":"Azure App Service Deployment","text":""},{"location":"guides/deployment/#deployment-strategy_1","title":"Deployment Strategy","text":"<p>This project supports two Azure deployment approaches:</p> <ol> <li>Azure Container Registry (ACR) - Recommended for enterprise/production (default)</li> <li>Docker Hub - Alternative for simpler setups or cross-platform deployments</li> </ol> <p>Both use the same GitHub Actions workflow with conditional logic.</p>"},{"location":"guides/deployment/#prerequisites_1","title":"Prerequisites","text":"<ul> <li>Azure account (free tier available)</li> <li>Azure CLI installed: https://docs.microsoft.com/en-us/cli/azure/install-azure-cli</li> <li>Docker Hub account (if using Docker Hub approach)</li> </ul>"},{"location":"guides/deployment/#option-1-deploy-with-azure-container-registry-recommended","title":"Option 1: Deploy with Azure Container Registry (Recommended)","text":""},{"location":"guides/deployment/#step-1-create-azure-resources","title":"Step 1: Create Azure Resources","text":"<pre><code># Login to Azure\naz login\n\n# Create a resource group\naz group create --name trump-nlp-rg --location eastus\n\n# Create Azure Container Registry\naz acr create `\n  --resource-group trump-nlp-rg `\n  --name trumpnlpacr `\n  --sku Basic `\n  --admin-enabled true\n\n# Create an App Service plan (B1 or F1 tier)\naz appservice plan create `\n  --name trump-nlp-plan `\n  --resource-group trump-nlp-rg `\n  --sku B1 `\n  --is-linux\n\n# Create a web app for containers\naz webapp create `\n  --resource-group trump-nlp-rg `\n  --plan trump-nlp-plan `\n  --name trump-speeches-nlp-chatbot `\n  --deployment-container-image-name trumpnlpacr.azurecr.io/trump-speeches-nlp-chatbot:latest\n\n# Configure port\naz webapp config appsettings set `\n  --resource-group trump-nlp-rg `\n  --name trump-speeches-nlp-chatbot `\n  --settings WEBSITES_PORT=8000\n\n# Enable ACR integration\naz webapp config container set `\n  --resource-group trump-nlp-rg `\n  --name trump-speeches-nlp-chatbot `\n  --docker-custom-image-name trumpnlpacr.azurecr.io/trump-speeches-nlp-chatbot:latest `\n  --docker-registry-server-url https://trumpnlpacr.azurecr.io\n</code></pre>"},{"location":"guides/deployment/#step-2-configure-github-secrets-for-acr","title":"Step 2: Configure GitHub Secrets for ACR","text":"<p>Add these secrets to your GitHub repository:</p> <ol> <li><code>AZURE_CREDENTIALS</code> - Service principal credentials</li> </ol> <pre><code>az ad sp create-for-rbac --name \"github-actions-trump-nlp\" --sdk-auth --role contributor --scopes /subscriptions/{subscription-id}/resourceGroups/trump-nlp-rg\n</code></pre> <p>Copy the entire JSON output and save as secret.</p> <ol> <li> <p><code>AZURE_REGISTRY_LOGIN_SERVER</code> - e.g., <code>trumpnlpacr.azurecr.io</code></p> </li> <li> <p><code>AZURE_REGISTRY_USERNAME</code> and <code>AZURE_REGISTRY_PASSWORD</code></p> </li> </ol> <pre><code>az acr credential show --name trumpnlpacr\n</code></pre> <ol> <li><code>AZURE_WEBAPP_NAME</code> - e.g., <code>trump-speeches-nlp-chatbot</code></li> </ol>"},{"location":"guides/deployment/#step-3-deploy","title":"Step 3: Deploy","text":"<p>Push to <code>main</code> branch:</p> <pre><code>git push origin main\n</code></pre> <p>GitHub Actions will automatically: 1. Build Docker image 2. Push to Azure Container Registry 3. Deploy to Azure Web App 4. Run health checks</p>"},{"location":"guides/deployment/#option-2-deploy-with-docker-hub","title":"Option 2: Deploy with Docker Hub","text":"<p>This approach shares the same Docker images used for Render deployment.</p>"},{"location":"guides/deployment/#step-1-create-azure-resources_1","title":"Step 1: Create Azure Resources","text":"<pre><code># Login to Azure\naz login\n\n# Create a resource group\naz group create --name trump-nlp-rg --location eastus\n\n# Create an App Service plan\naz appservice plan create `\n  --name trump-nlp-plan `\n  --resource-group trump-nlp-rg `\n  --sku B1 `\n  --is-linux\n\n# Create a web app using Docker Hub image\naz webapp create `\n  --resource-group trump-nlp-rg `\n  --plan trump-nlp-plan `\n  --name trump-speeches-nlp-chatbot `\n  --deployment-container-image-name docker.io/your-username/trump-speeches-nlp-chatbot:latest\n\n# Configure port\naz webapp config appsettings set `\n  --resource-group trump-nlp-rg `\n  --name trump-speeches-nlp-chatbot `\n  --settings WEBSITES_PORT=8000\n</code></pre>"},{"location":"guides/deployment/#step-2-configure-github-secrets-for-docker-hub","title":"Step 2: Configure GitHub Secrets for Docker Hub","text":"<p>You'll need the same Docker Hub secrets as for Render:</p> <ol> <li><code>DOCKERHUB_USERNAME</code></li> <li><code>DOCKERHUB_TOKEN</code></li> <li><code>AZURE_CREDENTIALS</code> (same as Option 1)</li> <li><code>AZURE_WEBAPP_NAME</code></li> </ol>"},{"location":"guides/deployment/#step-3-deploy-with-docker-hub","title":"Step 3: Deploy with Docker Hub","text":"<p>Manually trigger the workflow:</p> <ol> <li>Go to GitHub \u2192 Actions \u2192 \"Deploy to Azure\"</li> <li>Click \"Run workflow\"</li> <li>Check \"Use Docker Hub instead of ACR\"</li> <li>Click \"Run workflow\"</li> </ol>"},{"location":"guides/deployment/#accessing-your-azure-app","title":"Accessing Your Azure App","text":"<p>Your API will be available at: <code>https://trump-speeches-nlp-chatbot.azurewebsites.net</code></p>"},{"location":"guides/deployment/#monitoring-and-logs","title":"Monitoring and Logs","text":"<pre><code># Stream logs\naz webapp log tail --resource-group trump-nlp-rg --name trump-speeches-nlp-chatbot\n\n# View metrics\naz monitor metrics list `\n  --resource /subscriptions/{subscription-id}/resourceGroups/trump-nlp-rg/providers/Microsoft.Web/sites/trump-speeches-nlp-chatbot `\n  --metric-names Requests,ResponseTime,Http5xx\n\n# Open in Azure Portal\naz webapp browse --resource-group trump-nlp-rg --name trump-speeches-nlp-chatbot\n</code></pre>"},{"location":"guides/deployment/#updating-the-deployment","title":"Updating the Deployment","text":"<p>Deployments are automatic on push to <code>main</code>. To manually update:</p> <pre><code># Trigger GitHub Actions workflow manually\n# Or restart the web app to pull latest image\naz webapp restart --resource-group trump-nlp-rg --name trump-speeches-nlp-chatbot\n</code></pre>"},{"location":"guides/deployment/#cicd-with-github-actions","title":"CI/CD with GitHub Actions","text":"<p>This project includes automated CI/CD pipelines using GitHub Actions. The workflows are split into multiple files for better organization:</p>"},{"location":"guides/deployment/#workflow-files","title":"Workflow Files","text":"<p>All workflows are located in <code>.github/workflows/</code>:</p> <ul> <li><code>ci.yml</code> - Tests &amp; Linting</li> <li>Runs on: All pushes and PRs to <code>main</code>, <code>develop</code>, <code>feature/*</code></li> <li>Jobs: Unit tests, integration tests, code quality checks (flake8, black, isort, mypy)</li> <li> <p>Python versions tested: 3.11, 3.12, 3.13</p> </li> <li> <p><code>security.yml</code> - Security Scans</p> </li> <li>Runs on: All pushes and PRs, plus weekly schedule (Mondays 9 AM UTC)</li> <li> <p>Jobs: Dependency vulnerability scanning (pip-audit), code security analysis (bandit)</p> </li> <li> <p><code>deploy-render.yml</code> - Render Deployment</p> </li> <li>Runs on: Push to <code>main</code> branch only</li> <li>Jobs: Build Docker image, push to Docker Hub, test container</li> <li> <p>Render auto-deploys when new images are detected</p> </li> <li> <p><code>deploy-azure.yml</code> - Azure Deployment</p> </li> <li>Runs on: Push to <code>main</code> branch (manual trigger also available)</li> <li>Jobs: Build &amp; push to ACR/Docker Hub, deploy to Azure Web App, health check</li> <li>Supports both ACR and Docker Hub registries</li> </ul>"},{"location":"guides/deployment/#deployment-architecture","title":"Deployment Architecture","text":"<pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                     Push to main branch                     \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n             \u2502\n             \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n             \u2502                  \u2502\n             \u25bc                  \u25bc\n    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n    \u2502  Build Docker  \u2502   \u2502  Build Docker    \u2502\n    \u2502  Image         \u2502   \u2502  Image           \u2502\n    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n             \u2502                    \u2502\n             \u25bc                    \u25bc\n    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n    \u2502  Push to       \u2502   \u2502  Push to ACR     \u2502\n    \u2502  Docker Hub    \u2502   \u2502  (or Docker Hub) \u2502\n    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n             \u2502                    \u2502\n             \u25bc                    \u25bc\n    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n    \u2502  Render        \u2502   \u2502  Azure Web App   \u2502\n    \u2502  Auto-Deploy   \u2502   \u2502  Deploy          \u2502\n    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"guides/deployment/#setting-up-github-secrets","title":"Setting Up GitHub Secrets","text":""},{"location":"guides/deployment/#required-for-all-deployments","title":"Required for All Deployments","text":"<ol> <li><code>DOCKERHUB_USERNAME</code> - Your Docker Hub username</li> <li><code>DOCKERHUB_TOKEN</code> - Docker Hub access token</li> <li>Create at: Docker Hub \u2192 Account Settings \u2192 Security \u2192 New Access Token</li> </ol>"},{"location":"guides/deployment/#required-for-azure-deployment","title":"Required for Azure Deployment","text":"<ol> <li><code>AZURE_CREDENTIALS</code> - Service principal credentials (JSON format)</li> </ol> <pre><code>az ad sp create-for-rbac --name \"github-actions-trump-nlp\" --sdk-auth --role contributor --scopes /subscriptions/{subscription-id}/resourceGroups/trump-nlp-rg\n</code></pre> <ol> <li><code>AZURE_WEBAPP_NAME</code> - Your Azure Web App name (e.g., <code>trump-speeches-nlp</code>)</li> </ol>"},{"location":"guides/deployment/#required-for-azure-container-registry-optional","title":"Required for Azure Container Registry (Optional)","text":"<ol> <li><code>AZURE_REGISTRY_LOGIN_SERVER</code> - ACR login server (e.g., <code>trumpnlpacr.azurecr.io</code>)</li> <li><code>AZURE_REGISTRY_USERNAME</code> - ACR admin username</li> <li><code>AZURE_REGISTRY_PASSWORD</code> - ACR admin password</li> </ol> <p>Get ACR credentials:</p> <pre><code>az acr credential show --name trumpnlpacr\n</code></pre>"},{"location":"guides/deployment/#workflow-triggers","title":"Workflow Triggers","text":"Workflow Automatic Trigger Manual Trigger When to Use CI Push/PR to main, develop, feature/* \u2705 Yes Testing code changes Security Push/PR + Weekly (Mon 9AM) \u2705 Yes Regular security audits Deploy Render Push to main \u2705 Yes Deploy to Render Deploy Azure Push to main \u2705 Yes (with options) Deploy to Azure"},{"location":"guides/deployment/#manual-deployment-triggers","title":"Manual Deployment Triggers","text":"<p>For Azure:</p> <ol> <li>Go to GitHub \u2192 Actions \u2192 \"Deploy to Azure\"</li> <li>Click \"Run workflow\"</li> <li>Select options:</li> <li>Environment: production/staging</li> <li>Use Docker Hub: true/false (ACR is default)</li> <li>Click \"Run workflow\"</li> </ol> <p>For Render:</p> <ol> <li>Go to GitHub \u2192 Actions \u2192 \"Deploy to Render\"</li> <li>Click \"Run workflow\"</li> <li>Click \"Run workflow\" to confirm</li> </ol>"},{"location":"guides/deployment/#viewing-workflow-results","title":"Viewing Workflow Results","text":"<ul> <li>Navigate to your repository \u2192 Actions tab</li> <li>Click on any workflow run to see detailed logs</li> <li>Failed workflows will show which step failed and why</li> <li>Deployment summaries are available in each workflow run</li> </ul>"},{"location":"guides/deployment/#best-practices","title":"Best Practices","text":"<ol> <li>Always test locally before pushing to <code>main</code></li> <li>Use feature branches for development</li> <li>Review CI results before merging PRs</li> <li>Monitor deployments after pushing to <code>main</code></li> <li>Check security scans weekly</li> <li>Use manual triggers for testing deployment changes</li> </ol>"},{"location":"guides/deployment/#environment-variables","title":"Environment Variables","text":""},{"location":"guides/deployment/#required-variables","title":"Required Variables","text":"Variable Description Default <code>PORT</code> Port to run the application <code>8000</code> <code>PYTHONUNBUFFERED</code> Disable Python output buffering <code>1</code>"},{"location":"guides/deployment/#optional-variables","title":"Optional Variables","text":"Variable Description <code>PYTHON_VERSION</code> Python version (for Render) <code>WEBSITES_PORT</code> Port for Azure App Service"},{"location":"guides/deployment/#rag-specific-configuration","title":"RAG-Specific Configuration","text":"Variable Description Default <code>CHROMADB_PERSIST_DIR</code> Directory for ChromaDB persistence <code>./data/chromadb</code> <code>RAG_COLLECTION_NAME</code> Name of the vector collection <code>speeches</code> <code>RAG_CHUNK_SIZE</code> Text chunk size for embeddings <code>2048</code> <code>RAG_CHUNK_OVERLAP</code> Overlap between chunks <code>150</code> <code>EMBEDDING_MODEL</code> sentence-transformers model <code>all-MiniLM-L6-v2</code> <p>Note: ChromaDB data is persisted to disk. Ensure the persist directory is included in volume mounts for Docker deployments to maintain indexed documents across restarts.</p>"},{"location":"guides/deployment/#performance-tips","title":"Performance Tips","text":""},{"location":"guides/deployment/#for-free-tiers","title":"For Free Tiers","text":"<ol> <li>Render Free Tier:</li> <li>Apps sleep after 15 min of inactivity</li> <li>Use a health check service like cron-job.org to keep it awake</li> <li>Expect 30-60s cold start time</li> <li> <p>RAG Impact: First request will also load embedding model (~80MB) + index documents</p> </li> <li> <p>Azure Free Tier (F1):</p> </li> <li>Limited to 60 CPU minutes/day</li> <li>1 GB RAM limit</li> <li>Consider B1 tier ($13/month) for better performance</li> <li>RAG Requirements: Minimum 2GB RAM recommended with RAG enabled</li> </ol>"},{"location":"guides/deployment/#optimizing-docker-image","title":"Optimizing Docker Image","text":"<ul> <li>Use multi-stage builds (already implemented)</li> <li>Remove unnecessary data files</li> <li>Use <code>.dockerignore</code> to exclude notebooks and docs</li> <li>RAG Models: Pre-download models during build to reduce startup time</li> </ul>"},{"location":"guides/deployment/#rag-performance-considerations","title":"RAG Performance Considerations","text":"<p>Memory Requirements: - Without RAG: ~1.5GB RAM - With RAG: ~2.5GB RAM (includes embeddings model + vector store) - Recommended: 4GB RAM for production with concurrent users</p> <p>Storage Requirements: - Base application: ~1GB - ChromaDB index: ~50-100MB per 10,000 chunks - Embedding model: ~80MB</p> <p>Startup Time: - Without RAG: 10-15 seconds - With RAG: 30-60 seconds (model loading + document indexing) - Tip: Documents are auto-indexed on first startup if collection is empty</p> <p>Query Performance: - Semantic search: ~50-200ms per query - Question answering: ~200-500ms (includes search + answer generation) - Tip: Results improve with more indexed documents</p>"},{"location":"guides/deployment/#troubleshooting","title":"Troubleshooting","text":""},{"location":"guides/deployment/#docker-build-fails","title":"Docker Build Fails","text":"<pre><code># Clean Docker cache and rebuild\ndocker system prune -a\ndocker build --no-cache -t trump-speeches-nlp-api .\n</code></pre>"},{"location":"guides/deployment/#api-returns-503","title":"API Returns 503","text":"<ul> <li>Model may still be loading (can take 30-60s on first request)</li> <li>Check logs for errors</li> <li>Ensure sufficient memory (minimum 1GB recommended)</li> </ul>"},{"location":"guides/deployment/#azure-deployment-issues","title":"Azure Deployment Issues","text":"<pre><code># Check app logs\naz webapp log tail --resource-group trump-nlp-rg --name trump-speeches-nlp\n\n# Restart the app\naz webapp restart --resource-group trump-nlp-rg --name trump-speeches-nlp\n</code></pre>"},{"location":"guides/deployment/#render-deployment-issues","title":"Render Deployment Issues","text":"<ul> <li>Check build logs in Render dashboard</li> <li>Verify <code>.render/render.yaml</code> configuration</li> <li>Ensure all dependencies are in <code>pyproject.toml</code></li> </ul>"},{"location":"guides/deployment/#cost-comparison","title":"Cost Comparison","text":"Platform Free Tier Paid Tier Best For Render \u2705 Yes (with limitations) $7/month starter Quick demos, portfolio Azure \u2705 Yes (F1: 60 min/day) $13/month (B1) Enterprise, Azure ecosystem Railway \u2705 $5 free credit Pay-as-you-go Simple projects Fly.io \u2705 Free allowance Pay-as-you-go Global deployment"},{"location":"guides/deployment/#next-steps","title":"Next Steps","text":"<p>After deployment:</p> <ol> <li>Test the API: Use the <code>/health</code> endpoint to verify</li> <li>Update README: Add your live demo link</li> <li>Monitor performance: Check logs and metrics</li> <li>Set up alerts: Configure uptime monitoring</li> <li>Add custom domain: (optional) Configure DNS</li> </ol>"},{"location":"guides/deployment/#support","title":"Support","text":"<p>For issues or questions:</p> <ul> <li>Check the main README</li> <li>Review API documentation at <code>/docs</code></li> <li>Open an issue on GitHub</li> </ul>"},{"location":"guides/quickstart/","title":"Quick Start Guide","text":"<p>Get the Trump Speeches NLP Chatbot API running locally in minutes.</p>"},{"location":"guides/quickstart/#prerequisites","title":"Prerequisites","text":"<ul> <li>Python 3.11+ installed</li> <li>uv installed (install guide)</li> <li>Google Gemini API key (get one free)</li> </ul>"},{"location":"guides/quickstart/#setup","title":"Setup","text":"<ol> <li>Install Dependencies</li> </ol> <pre><code>uv sync\n\nuv venv --python 3.12  # if you need to set a specific vesion\n</code></pre> <ol> <li>Configure Environment</li> </ol> <p>Create a <code>.env</code> file in the project root:    <pre><code>GEMINI_API_KEY=your_api_key_here\n</code></pre></p> <ol> <li>Run the API</li> </ol> <pre><code>uv run uvicorn src.api:app --host 0.0.0.0 --port 8000 --reload\n</code></pre> <p>The API will automatically:    - Load configuration from <code>.env</code>    - Initialize logging (colored output in development)    - Load FinBERT sentiment model (~440MB)    - Initialize Gemini LLM service    - Load ChromaDB vector database with existing embeddings    - Start FastAPI server</p> <p>Expected startup output:    <pre><code>2025-11-04 12:34:56 | INFO     | src.api              | Application: Trump Speeches NLP Chatbot API v0.1.0\n2025-11-04 12:34:56 | INFO     | src.api              | Environment: development\n2025-11-04 12:34:56 | INFO     | src.api              | \u2713 Sentiment analysis model loaded successfully\n2025-11-04 12:34:57 | INFO     | src.api              | \u2713 LLM service initialized and tested successfully\n2025-11-04 12:34:58 | INFO     | src.api              | \u2713 RAG service initialized with 1082 existing chunks\n2025-11-04 12:34:58 | INFO     | src.api              | Application startup complete\n</code></pre></p> <ol> <li>Access the Application</li> <li>Web UI: http://localhost:8000</li> <li>API Docs: http://localhost:8000/docs</li> <li>Health Check: http://localhost:8000/health</li> </ol>"},{"location":"guides/quickstart/#running-with-docker","title":"Running with Docker","text":""},{"location":"guides/quickstart/#build-and-run","title":"Build and Run","text":"<pre><code>docker build -t trump-speeches-nlp-chatbot .\ndocker run --rm -it -p 8000:8000 --env-file .env --name nlp-chatbot trump-speeches-nlp-chatbot\n\n\ndocker run --rm -it -p 8000:8000 -v \"${PWD}/data/chromadb:/app/data/chromadb\" --env-file .env --name nlp-chatbot trump-speeches-nlp-api:torch-2.6.0\n\n### Using Docker Compose\n\n```powershell\ndocker-compose up\n</code></pre>"},{"location":"guides/quickstart/#testing-the-rag-system","title":"Testing the RAG System","text":""},{"location":"guides/quickstart/#using-the-web-interface","title":"Using the Web Interface","text":"<ol> <li>Open http://localhost:8000</li> <li>Navigate to the \"RAG Q&amp;A\" tab</li> <li>Ask a question like \"What economic policies were discussed?\"</li> <li>View the AI-generated answer with confidence scores and sources</li> </ol>"},{"location":"guides/quickstart/#using-curl","title":"Using curl","text":"<pre><code># Ask a question (RAG)\ncurl -X POST http://localhost:8000/rag/ask `\n  -H \"Content-Type: application/json\" `\n  -d '{\"question\": \"What was said about the economy?\", \"top_k\": 5}'\n\n# Semantic search\ncurl -X POST http://localhost:8000/rag/search `\n  -H \"Content-Type: application/json\" `\n  -d '{\"query\": \"immigration policy\", \"top_k\": 5}'\n\n# Get RAG statistics\ncurl http://localhost:8000/rag/stats\n\n# Sentiment analysis (traditional NLP)\ncurl -X POST http://localhost:8000/analyze/sentiment `\n  -H \"Content-Type: application/json\" `\n  -d '{\"text\": \"The economy is doing great!\"}'\n</code></pre>"},{"location":"guides/quickstart/#using-python","title":"Using Python","text":"<pre><code>import requests\n\n# RAG Question Answering\nresponse = requests.post(\n    \"http://localhost:8000/rag/ask\",\n    json={\n        \"question\": \"What were the main themes in the 2020 speeches?\",\n        \"top_k\": 5\n    }\n)\nresult = response.json()\nprint(f\"Answer: {result['answer']}\")\nprint(f\"Confidence: {result['confidence']} ({result['confidence_score']:.2f})\")\nprint(f\"Sources: {', '.join(result['sources'])}\")\n\n# Traditional NLP - Sentiment\nresponse = requests.post(\n    \"http://localhost:8000/analyze/sentiment\",\n    json={\"text\": \"This is incredible! Best economy ever.\"}\n)\nprint(response.json())\n</code></pre>"},{"location":"guides/quickstart/#troubleshooting","title":"Troubleshooting","text":""},{"location":"guides/quickstart/#rag-service-not-initialized","title":"\"RAG service not initialized\"","text":"<p>The API auto-indexes documents on first startup. This takes ~30-60 seconds. Check the logs for progress: <pre><code>INFO:     Loading documents into RAG service...\nINFO:     Loaded 35 documents into RAG service!\n</code></pre></p>"},{"location":"guides/quickstart/#gemini-api-errors","title":"Gemini API Errors","text":"<p>Ensure your <code>.env</code> file exists with a valid <code>GEMINI_API_KEY</code>. Get a free key at https://ai.google.dev/.</p>"},{"location":"guides/quickstart/#model-download-taking-long","title":"Model Download Taking Long","text":"<p>First run downloads ~1-2 GB of models (FinBERT, MPNet embeddings). Subsequent runs are fast.</p>"},{"location":"guides/quickstart/#port-already-in-use","title":"Port Already in Use","text":"<pre><code>uv run uvicorn src.api:app --reload --port 8001\n</code></pre>"},{"location":"guides/quickstart/#module-not-found","title":"Module Not Found","text":"<p>Ensure you're in the project root directory and have run <code>uv sync</code>.</p>"},{"location":"guides/quickstart/#next-steps","title":"Next Steps","text":"<ul> <li>Try the interactive web interface at http://localhost:8000</li> <li>Explore API documentation at http://localhost:8000/docs</li> <li>Read about RAG improvements in <code>docs/RAG_IMPROVEMENTS.md</code></li> <li>Deploy to production with <code>docs/DEPLOYMENT.md</code></li> </ul>"},{"location":"howto/documentation/","title":"Documentation Guide \u2014 MkDocs","text":"<p>This project uses MkDocs with the Material theme to generate professional documentation from markdown files. This guide explains how to work with the documentation system.</p>"},{"location":"howto/documentation/#what-is-mkdocs","title":"What is MkDocs?","text":"<p>MkDocs is a static site generator that transforms markdown files into a beautiful, searchable documentation website with:</p> <ul> <li>Navigation sidebar automatically generated</li> <li>Search functionality across all pages</li> <li>Professional theme (Material for MkDocs)</li> <li>Mobile-friendly responsive design</li> <li>Version control friendly \u2014 docs are just markdown files</li> </ul>"},{"location":"howto/documentation/#quick-reference","title":"Quick Reference","text":""},{"location":"howto/documentation/#view-documentation-locally","title":"View Documentation Locally","text":"<pre><code># Install documentation dependencies\nuv sync --group docs\n\n# Serve docs with live reload (default port 8000)\nuv run mkdocs serve\n\n# Use custom port to avoid conflicts with the API\nuv run mkdocs serve --dev-addr localhost:8001\n</code></pre> <p>Then open http://localhost:8001 in your browser. Changes to markdown files will automatically reload!</p>"},{"location":"howto/documentation/#build-static-site","title":"Build Static Site","text":"<pre><code># Build HTML site to site/ folder\nuv run mkdocs build\n\n# Build with verbose output\nuv run mkdocs build --verbose\n\n# Clean build (remove site/ first)\nuv run mkdocs build --clean\n</code></pre>"},{"location":"howto/documentation/#deploy-to-github-pages","title":"Deploy to GitHub Pages","text":"<pre><code># Deploy to GitHub Pages (gh-pages branch)\nuv run mkdocs gh-deploy\n\n# Deploy with custom commit message\nuv run mkdocs gh-deploy --message \"Update documentation\"\n</code></pre> <p>Note: The project has automated deployment via GitHub Actions, so you typically don't need to run this manually.</p>"},{"location":"howto/documentation/#project-structure","title":"Project Structure","text":"<pre><code>Trump-Rally-Speeches-NLP-Chatbot/\n\u251c\u2500\u2500 docs/                    # All documentation files\n\u2502   \u251c\u2500\u2500 index.md            # Homepage\n\u2502   \u251c\u2500\u2500 guides/             # Getting started guides\n\u2502   \u2502   \u251c\u2500\u2500 quickstart.md\n\u2502   \u2502   \u2514\u2500\u2500 deployment.md\n\u2502   \u251c\u2500\u2500 howto/              # Task-oriented guides\n\u2502   \u2502   \u251c\u2500\u2500 testing.md\n\u2502   \u2502   \u251c\u2500\u2500 entity-analytics.md\n\u2502   \u2502   \u2514\u2500\u2500 documentation.md    # This file\n\u2502   \u2514\u2500\u2500 reference/          # Technical reference\n\u2502       \u251c\u2500\u2500 architecture.md\n\u2502       \u2514\u2500\u2500 rag-features.md\n\u251c\u2500\u2500 mkdocs.yml              # MkDocs configuration\n\u2514\u2500\u2500 site/                   # Generated HTML (not committed)\n</code></pre>"},{"location":"howto/documentation/#how-to-add-or-edit-documentation","title":"How to Add or Edit Documentation","text":""},{"location":"howto/documentation/#1-edit-existing-pages","title":"1. Edit Existing Pages","text":"<p>Simply edit any <code>.md</code> file in the <code>docs/</code> folder. If you have <code>mkdocs serve</code> running, changes will appear instantly in your browser.</p>"},{"location":"howto/documentation/#2-add-new-pages","title":"2. Add New Pages","text":"<p>Step 1: Create a new markdown file in the appropriate folder: <pre><code># Example: Add a new how-to guide\nNew-Item docs/howto/my-new-guide.md\n</code></pre></p> <p>Step 2: Add the page to navigation in <code>mkdocs.yml</code>: <pre><code>nav:\n  - Home: index.md\n  - How-To Guides:\n      - Testing: howto/testing.md\n      - My New Guide: howto/my-new-guide.md  # Add this line\n</code></pre></p> <p>Step 3: Preview with <code>mkdocs serve</code> and verify it appears in the sidebar.</p>"},{"location":"howto/documentation/#3-reorganize-navigation","title":"3. Reorganize Navigation","text":"<p>Edit the <code>nav</code> section in <code>mkdocs.yml</code>:</p> <pre><code>nav:\n  - Home: index.md\n\n  - Getting Started:\n      - Quickstart: guides/quickstart.md\n      - Deployment: guides/deployment.md\n\n  - How-To Guides:\n      - Testing: howto/testing.md\n      - Documentation: howto/documentation.md\n\n  - Reference:\n      - Architecture: reference/architecture.md\n      - RAG Features: reference/rag-features.md\n</code></pre> <p>The order in <code>mkdocs.yml</code> determines the order in the sidebar.</p>"},{"location":"howto/documentation/#configuration-reference","title":"Configuration Reference","text":""},{"location":"howto/documentation/#port-configuration","title":"Port Configuration","text":"<p>By default, MkDocs serves on port 8000. To change:</p> <pre><code># Serve on port 8001\nuv run mkdocs serve --dev-addr localhost:8001\n\n# Serve on all interfaces (for network access)\nuv run mkdocs serve --dev-addr 0.0.0.0:8001\n</code></pre>"},{"location":"howto/documentation/#theme-customization","title":"Theme Customization","text":"<p>Edit <code>mkdocs.yml</code> to customize the theme:</p> <pre><code>theme:\n  name: material\n  palette:\n    # Light mode\n    - scheme: default\n      primary: indigo      # Change primary color\n      accent: indigo       # Change accent color\n</code></pre> <p>Available colors: red, pink, purple, deep purple, indigo, blue, light blue, cyan, teal, green, light green, lime, yellow, amber, orange, deep orange</p>"},{"location":"howto/documentation/#add-a-logo","title":"Add a Logo","text":"<ol> <li>Add your logo image to <code>docs/assets/images/</code></li> <li>Update <code>mkdocs.yml</code>:</li> </ol> <pre><code>theme:\n  name: material\n  logo: assets/images/logo.png\n  favicon: assets/images/favicon.ico\n</code></pre>"},{"location":"howto/documentation/#add-custom-css-or-javascript","title":"Add Custom CSS or JavaScript","text":"<ol> <li>Create files:</li> <li><code>docs/stylesheets/extra.css</code></li> <li> <p><code>docs/javascripts/extra.js</code></p> </li> <li> <p>Update <code>mkdocs.yml</code>:</p> </li> </ol> <pre><code>extra_css:\n  - stylesheets/extra.css\n\nextra_javascript:\n  - javascripts/extra.js\n</code></pre>"},{"location":"howto/documentation/#markdown-features","title":"Markdown Features","text":"<p>MkDocs with Material theme supports rich markdown features:</p>"},{"location":"howto/documentation/#admonitions-call-out-boxes","title":"Admonitions (Call-out Boxes)","text":"<pre><code>!!! note\n    This is a note admonition.\n\n!!! warning\n    This is a warning admonition.\n\n!!! tip\n    This is a tip admonition.\n\n??? example \"Collapsible Example\"\n    This content is collapsed by default.\n</code></pre> <p>Note</p> <p>This is a note admonition.</p> <p>Warning</p> <p>This is a warning admonition.</p>"},{"location":"howto/documentation/#code-blocks-with-syntax-highlighting","title":"Code Blocks with Syntax Highlighting","text":"<pre><code>```python\ndef hello_world():\n    print(\"Hello, World!\")\n```\n\n```bash\nuv run pytest\n```\n</code></pre>"},{"location":"howto/documentation/#tabbed-content","title":"Tabbed Content","text":"<pre><code>=== \"Python\"\n    ```python\n    print(\"Hello from Python\")\n    ```\n\n=== \"PowerShell\"\n    ```powershell\n    Write-Host \"Hello from PowerShell\"\n    ```\n</code></pre> PythonPowerShell <pre><code>print(\"Hello from Python\")\n</code></pre> <pre><code>Write-Host \"Hello from PowerShell\"\n</code></pre>"},{"location":"howto/documentation/#task-lists","title":"Task Lists","text":"<pre><code>- [x] Completed task\n- [ ] Incomplete task\n- [ ] Another task\n</code></pre> <ul> <li> Completed task</li> <li> Incomplete task</li> <li> Another task</li> </ul>"},{"location":"howto/documentation/#tables","title":"Tables","text":"<pre><code>| Feature | Supported |\n|---------|-----------|\n| Search  | \u2705        |\n| Mobile  | \u2705        |\n| Dark Mode | \u2705      |\n</code></pre> Feature Supported Search \u2705 Mobile \u2705 Dark Mode \u2705"},{"location":"howto/documentation/#mermaid-diagrams","title":"Mermaid Diagrams","text":"<pre><code>```mermaid\ngraph LR\n    A[User] --&gt; B[API]\n    B --&gt; C[RAG Service]\n    C --&gt; D[ChromaDB]\n```\n</code></pre> <pre><code>graph LR\n    A[User] --&gt; B[API]\n    B --&gt; C[RAG Service]\n    C --&gt; D[ChromaDB]</code></pre>"},{"location":"howto/documentation/#deployment","title":"Deployment","text":""},{"location":"howto/documentation/#automated-deployment-recommended","title":"Automated Deployment (Recommended)","text":"<p>The project includes a GitHub Action that automatically deploys documentation to GitHub Pages when you push to the <code>main</code> branch.</p> <p>How it works: 1. You push changes to <code>main</code> 2. GitHub Actions builds the docs 3. Deploys to GitHub Pages automatically 4. Live at: https://justakris.github.io/Trump-Rally-Speeches-NLP-Chatbot/</p> <p>Configuration: See <code>.github/workflows/deploy-docs.yml</code></p>"},{"location":"howto/documentation/#manual-deployment","title":"Manual Deployment","text":"<p>If you need to deploy manually:</p> <pre><code># Deploy to GitHub Pages\nuv run mkdocs gh-deploy\n</code></pre> <p>This command: 1. Builds the site (<code>mkdocs build</code>) 2. Commits the <code>site/</code> folder to the <code>gh-pages</code> branch 3. Pushes to GitHub</p> <p>Note: Ensure you have push access to the repository.</p>"},{"location":"howto/documentation/#troubleshooting","title":"Troubleshooting","text":""},{"location":"howto/documentation/#port-already-in-use","title":"Port Already in Use","text":"<p>If port 8000 is already in use by your API:</p> <pre><code>uv run mkdocs serve --dev-addr localhost:8001\n</code></pre>"},{"location":"howto/documentation/#page-not-found-in-sidebar","title":"\"Page not found\" in Sidebar","text":"<p>Check <code>mkdocs.yml</code> navigation and ensure: - The file path is correct (relative to <code>docs/</code>) - The file exists - The file has a <code>.md</code> extension</p>"},{"location":"howto/documentation/#changes-not-appearing","title":"Changes Not Appearing","text":"<ol> <li>Check terminal for build errors</li> <li>Hard refresh browser (Ctrl+Shift+R)</li> <li>Restart <code>mkdocs serve</code></li> </ol>"},{"location":"howto/documentation/#build-warnings","title":"Build Warnings","text":"<pre><code>WARNING - Doc file 'path/to/file.md' contains a link to 'broken-link.md'\n</code></pre> <p>This means you have a broken internal link. Fix the link or remove it.</p>"},{"location":"howto/documentation/#images-not-loading","title":"Images Not Loading","text":"<p>Ensure images are in <code>docs/</code> folder and use relative paths:</p> <pre><code>![Architecture](../assets/images/architecture.png)\n</code></pre>"},{"location":"howto/documentation/#best-practices","title":"Best Practices","text":""},{"location":"howto/documentation/#1-organize-by-purpose","title":"1. Organize by Purpose","text":"<ul> <li><code>guides/</code> \u2014 Getting started, tutorials</li> <li><code>howto/</code> \u2014 Task-oriented guides (how to do X)</li> <li><code>reference/</code> \u2014 Technical specs, API docs, architecture</li> </ul>"},{"location":"howto/documentation/#2-use-clear-titles","title":"2. Use Clear Titles","text":"<p>Good: <code># Testing Guide \u2014 pytest and Coverage</code> Bad: <code># Testing</code></p>"},{"location":"howto/documentation/#3-add-metadata","title":"3. Add Metadata","text":"<p>At the top of each page, you can add metadata:</p> <pre><code>---\ntitle: Testing Guide\ndescription: How to run tests and check coverage\n---\n</code></pre>"},{"location":"howto/documentation/#4-link-between-pages","title":"4. Link Between Pages","text":"<p>Use relative links to link between docs:</p> <pre><code>See the [Quickstart Guide](../guides/quickstart.md) for setup instructions.\n</code></pre>"},{"location":"howto/documentation/#5-keep-navigation-flat","title":"5. Keep Navigation Flat","text":"<p>Avoid deeply nested navigation (max 2-3 levels).</p>"},{"location":"howto/documentation/#6-test-before-committing","title":"6. Test Before Committing","text":"<p>Always run <code>mkdocs build</code> before committing to catch broken links and errors:</p> <pre><code>uv run mkdocs build --strict\n</code></pre> <p>The <code>--strict</code> flag turns warnings into errors.</p>"},{"location":"howto/documentation/#cicd-integration","title":"CI/CD Integration","text":""},{"location":"howto/documentation/#github-actions-workflow","title":"GitHub Actions Workflow","text":"<p>The project includes automated documentation deployment. See <code>.github/workflows/deploy-docs.yml</code>:</p> <pre><code>name: Deploy Documentation\n\non:\n  push:\n    branches: [main]\n    paths:\n      - 'docs/**'\n      - 'mkdocs.yml'\n\njobs:\n  deploy:\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v4\n      - uses: actions/setup-python@v5\n      - run: pip install mkdocs-material\n      - run: mkdocs gh-deploy --force\n</code></pre>"},{"location":"howto/documentation/#triggers","title":"Triggers","text":"<p>Documentation deploys automatically when: - You push to <code>main</code> - Changes are in <code>docs/</code> folder or <code>mkdocs.yml</code></p>"},{"location":"howto/documentation/#resources","title":"Resources","text":"<ul> <li>MkDocs Documentation: https://www.mkdocs.org/</li> <li>Material for MkDocs: https://squidfunk.github.io/mkdocs-material/</li> <li>Markdown Guide: https://www.markdownguide.org/</li> <li>Live Documentation: https://justakris.github.io/Trump-Rally-Speeches-NLP-Chatbot/</li> </ul>"},{"location":"howto/documentation/#summary","title":"Summary","text":"<p>To work with docs: 1. Edit markdown files in <code>docs/</code> 2. Run <code>uv run mkdocs serve --dev-addr localhost:8001</code> to preview 3. Commit and push to <code>main</code> 4. GitHub Actions deploys automatically</p> <p>Need help? Open an issue on GitHub or check the MkDocs documentation.</p>"},{"location":"howto/entity-analytics/","title":"Entity Analytics &amp; Confidence Explainability - Implementation Summary","text":""},{"location":"howto/entity-analytics/#overview","title":"Overview","text":"<p>Added two major UX enhancements to make the RAG system more transparent and researcher-friendly:</p> <ol> <li>Confidence Justification - Human-readable explanations of why confidence is at a certain level</li> <li>Entity Analytics - Comprehensive metadata about entities including mentions, sentiment, and associations</li> </ol>"},{"location":"howto/entity-analytics/#features-implemented","title":"Features Implemented","text":""},{"location":"howto/entity-analytics/#1-confidence-explanation","title":"1. \u2705 Confidence Explanation","text":"<p>Problem: \"Confidence: MEDIUM\" was opaque - users didn't know why</p> <p>Solution: Added natural language explanation that references key factors</p> <p>Example Output: <pre><code>Confidence: MEDIUM (score: 0.59)\nExplanation: Overall confidence is MEDIUM (score: 0.59) based on weak semantic match \n(similarity: 0.22), very consistent results (consistency: 1.00), 5 supporting context \nchunks, 'Biden' mentioned in all retrieved chunks.\n</code></pre></p> <p>Implementation: New method <code>_generate_confidence_explanation()</code> in <code>RAGService</code></p>"},{"location":"howto/entity-analytics/#2-entity-sentiment-analysis","title":"2. \u2705 Entity Sentiment Analysis","text":"<p>Problem: No insight into sentiment/tone about mentioned entities</p> <p>Solution: Integrated sentiment analyzer to calculate average sentiment across entity mentions</p> <p>Example Output: <pre><code>Biden:\n  Average sentiment: -0.61 (Negative)\n  Sample size: 50 chunks\n</code></pre></p> <p>How it works: - Analyzes up to 50 context chunks containing the entity - Uses FinBERT sentiment model (already in project) - Converts scores to -1 (negative) to +1 (positive) - Classifies as Positive, Neutral, or Negative</p> <p>Implementation: New method <code>_analyze_entity_sentiment()</code> in <code>RAGService</code></p>"},{"location":"howto/entity-analytics/#3-entity-co-occurrence-analysis","title":"3. \u2705 Entity Co-occurrence Analysis","text":"<p>Problem: No context about what topics/terms surround an entity</p> <p>Solution: Extract most common words appearing near the entity</p> <p>Example Output: <pre><code>Biden:\n  Associated terms: socialism, weakness, failure, china, corrupt\n</code></pre></p> <p>How it works: - Extracts words from contexts containing the entity - Filters stopwords - Returns top 5 most frequent terms - Window-based approach around entity mentions</p> <p>Implementation: New method <code>_find_entity_associations()</code> in <code>RAGService</code></p>"},{"location":"howto/entity-analytics/#api-response-format","title":"API Response Format","text":""},{"location":"howto/entity-analytics/#enhanced-raganswerresponse","title":"Enhanced RAGAnswerResponse","text":"<pre><code>{\n    \"answer\": \"...\",\n    \"confidence\": \"medium\",\n    \"confidence_score\": 0.587,\n    \"confidence_explanation\": \"Overall confidence is MEDIUM (score: 0.59)...\",  # NEW\n    \"confidence_factors\": {\n        \"retrieval_score\": 0.219,\n        \"consistency\": 0.998,\n        \"chunk_coverage\": 5,\n        \"entity_coverage\": 1.0\n    },\n    \"entity_statistics\": {  # ENHANCED\n        \"Biden\": {\n            \"mention_count\": 524,\n            \"speech_count\": 30,\n            \"corpus_percentage\": 25.03,\n            \"speeches\": [\"OhioSep21_2020.txt\", ...],\n            \"sentiment\": {  # NEW\n                \"average_score\": -0.61,\n                \"classification\": \"Negative\",\n                \"sample_size\": 50\n            },\n            \"associated_terms\": [  # NEW\n                \"socialism\", \"weakness\", \"failure\", \"china\", \"corrupt\"\n            ]\n        }\n    }\n}\n</code></pre>"},{"location":"howto/entity-analytics/#code-changes","title":"Code Changes","text":""},{"location":"howto/entity-analytics/#files-modified","title":"Files Modified","text":"<ol> <li>src/rag_service.py</li> <li>Added <code>_generate_confidence_explanation()</code> - 60 lines</li> <li>Added <code>_analyze_entity_sentiment()</code> - 55 lines</li> <li>Added <code>_find_entity_associations()</code> - 50 lines</li> <li>Modified <code>_get_entity_statistics()</code> to include new analytics</li> <li> <p>Modified <code>_calculate_confidence()</code> to generate explanation</p> </li> <li> <p>src/api.py</p> </li> <li>Updated <code>RAGAnswerResponse</code> model with <code>confidence_explanation</code> field</li> </ol>"},{"location":"howto/entity-analytics/#new-dependencies","title":"New Dependencies","text":"<p>None! Uses existing <code>SentenceAnalyzer</code> already in the project.</p>"},{"location":"howto/entity-analytics/#usage-examples","title":"Usage Examples","text":""},{"location":"howto/entity-analytics/#command-line-test","title":"Command Line Test","text":"<pre><code>uv run python test_enhancements.py\n</code></pre>"},{"location":"howto/entity-analytics/#api-request","title":"API Request","text":"<pre><code>$body = @{ question = \"What does Trump say about Biden?\"; top_k = 5 } | ConvertTo-Json\nInvoke-RestMethod -Uri \"http://localhost:8001/rag/ask\" -Method Post -Body $body -ContentType \"application/json\"\n</code></pre>"},{"location":"howto/entity-analytics/#python-code","title":"Python Code","text":"<pre><code>from src.rag_service import RAGService\n\nrag = RAGService()\nresult = rag.ask(\"What are Trump's views on Biden?\", top_k=5)\n\n# Check confidence explanation\nprint(f\"Confidence: {result['confidence']}\")\nprint(f\"Why: {result['confidence_explanation']}\")\n\n# View entity analytics\nif 'entity_statistics' in result:\n    for entity, stats in result['entity_statistics'].items():\n        print(f\"\\n{entity}:\")\n        print(f\"  Mentions: {stats['mention_count']}\")\n        print(f\"  Sentiment: {stats['sentiment']['classification']}\")\n        print(f\"  Associated with: {', '.join(stats['associated_terms'])}\")\n</code></pre>"},{"location":"howto/entity-analytics/#real-world-example","title":"Real-World Example","text":"<p>Query: \"What does Trump say about Biden?\"</p> <p>Response Analytics:</p> <pre><code>======================================================================\nCONFIDENCE:\n======================================================================\nLevel: MEDIUM\nScore: 0.587\n\nExplanation: Overall confidence is MEDIUM (score: 0.59) based on weak \nsemantic match (similarity: 0.22), very consistent results (consistency: \n1.00), 5 supporting context chunks, 'Biden' mentioned in all retrieved \nchunks.\n\n======================================================================\nENTITY ANALYTICS:\n======================================================================\n\nBiden:\n  Mentions: 524 times across 30 speeches\n  Corpus coverage: 25.03%\n  Average sentiment: 0.00 (Neutral)\n  Sample size: 50 chunks\n  Associated terms: people, our, right, about, say\n\nTrump:\n  Mentions: 449 times across 35 speeches\n  Corpus coverage: 24.34%\n  Average sentiment: 0.00 (Neutral)\n  Sample size: 50 chunks\n  Associated terms: people, right, one, say, because\n</code></pre> <p>Insights: - Biden is mentioned in 85% of speeches (30 out of 35) - Covers 25% of entire corpus - Associated with terms like \"socialism\", \"weakness\" (when more specific query used) - Sentiment is relatively neutral aggregate (varies per context)</p>"},{"location":"howto/entity-analytics/#uifrontend-integration-todo","title":"UI/Frontend Integration (TODO)","text":""},{"location":"howto/entity-analytics/#confidence-tooltip","title":"Confidence Tooltip","text":"<pre><code>&lt;div class=\"confidence-badge\" tooltip=\"{confidence_explanation}\"&gt;\n  Confidence: MEDIUM \u24d8\n&lt;/div&gt;\n</code></pre>"},{"location":"howto/entity-analytics/#entity-analytics-card","title":"Entity Analytics Card","text":"<pre><code>&lt;div class=\"entity-analytics\"&gt;\n  &lt;h3&gt;\ud83d\udcca Entity Analysis: Biden&lt;/h3&gt;\n\n  &lt;div class=\"stat\"&gt;\n    &lt;span class=\"label\"&gt;Mentions:&lt;/span&gt;\n    &lt;span class=\"value\"&gt;524 times in 30 speeches&lt;/span&gt;\n  &lt;/div&gt;\n\n  &lt;div class=\"stat\"&gt;\n    &lt;span class=\"label\"&gt;Corpus Coverage:&lt;/span&gt;\n    &lt;span class=\"value\"&gt;25%&lt;/span&gt;\n    &lt;div class=\"progress-bar\" style=\"width: 25%\"&gt;&lt;/div&gt;\n  &lt;/div&gt;\n\n  &lt;div class=\"stat\"&gt;\n    &lt;span class=\"label\"&gt;Average Sentiment:&lt;/span&gt;\n    &lt;span class=\"value sentiment-negative\"&gt;-0.61 (Negative)&lt;/span&gt;\n  &lt;/div&gt;\n\n  &lt;div class=\"stat\"&gt;\n    &lt;span class=\"label\"&gt;Associated Terms:&lt;/span&gt;\n    &lt;div class=\"tags\"&gt;\n      &lt;span class=\"tag\"&gt;socialism&lt;/span&gt;\n      &lt;span class=\"tag\"&gt;weakness&lt;/span&gt;\n      &lt;span class=\"tag\"&gt;failure&lt;/span&gt;\n    &lt;/div&gt;\n  &lt;/div&gt;\n&lt;/div&gt;\n</code></pre>"},{"location":"howto/entity-analytics/#performance-impact","title":"Performance Impact","text":""},{"location":"howto/entity-analytics/#memory","title":"Memory","text":"<ul> <li>Sentiment Analysis: +150MB (FinBERT model, already loaded)</li> <li>Entity Stats Cache: ~1MB per query (not cached yet)</li> </ul>"},{"location":"howto/entity-analytics/#latency","title":"Latency","text":"<ul> <li>Confidence Explanation: &lt; 1ms (string formatting)</li> <li>Entity Sentiment: ~2-5s for 50 chunks (one-time per query)</li> <li>Associated Terms: ~100ms (text processing)</li> </ul> <p>Total added latency: ~2-5 seconds per query with entities</p>"},{"location":"howto/entity-analytics/#optimization-opportunities","title":"Optimization Opportunities","text":"<ol> <li>Cache entity statistics - Same entities queried multiple times</li> <li>Async sentiment analysis - Don't block on sentiment</li> <li>Reduce sample size - Use 20 chunks instead of 50</li> <li>Pre-compute statistics - Calculate during indexing</li> </ol>"},{"location":"howto/entity-analytics/#testing","title":"Testing","text":""},{"location":"howto/entity-analytics/#unit-tests","title":"Unit Tests","text":"<p>Create <code>tests/test_entity_analytics.py</code>:</p> <pre><code>def test_confidence_explanation():\n    rag = RAGService()\n    # Test explanation generation\n\ndef test_entity_sentiment():\n    rag = RAGService()\n    # Test sentiment calculation\n\ndef test_entity_associations():\n    rag = RAGService()\n    # Test co-occurrence analysis\n</code></pre>"},{"location":"howto/entity-analytics/#integration-test","title":"Integration Test","text":"<pre><code>def test_full_entity_analytics():\n    rag = RAGService()\n    result = rag.ask(\"What about Biden?\", top_k=5)\n\n    assert \"confidence_explanation\" in result\n    assert \"entity_statistics\" in result\n\n    if result[\"entity_statistics\"]:\n        for entity, stats in result[\"entity_statistics\"].items():\n            assert \"sentiment\" in stats\n            assert \"associated_terms\" in stats\n            assert stats[\"sentiment\"][\"sample_size\"] &gt; 0\n</code></pre>"},{"location":"howto/entity-analytics/#next-steps","title":"Next Steps","text":""},{"location":"howto/entity-analytics/#high-priority","title":"High Priority","text":"<ol> <li>Add caching for entity statistics (Redis or in-memory)</li> <li>Async processing for sentiment to avoid blocking</li> <li>Frontend integration - Build UI cards for entity analytics</li> </ol>"},{"location":"howto/entity-analytics/#medium-priority","title":"Medium Priority","text":"<ol> <li>Sentiment over time - Track sentiment changes across chronological speeches</li> <li>Entity relationships - Show connections between entities (co-mentions)</li> <li>Improved associations - Use TF-IDF instead of raw frequency</li> </ol>"},{"location":"howto/entity-analytics/#low-priority","title":"Low Priority","text":"<ol> <li>Custom sentiment model - Fine-tune for political speech domain</li> <li>Entity disambiguation - Distinguish \"Biden\" vs \"Hunter Biden\"</li> <li>Visualization - Charts for sentiment trends, word clouds for associations</li> </ol>"},{"location":"howto/entity-analytics/#benefits-for-portfolio","title":"Benefits for Portfolio","text":""},{"location":"howto/entity-analytics/#demonstrates","title":"Demonstrates","text":"<p>\u2705 UX Design - Thoughtful user experience improvements \u2705 Explainable AI - Transparency in ML system decisions \u2705 Data Science - Sentiment analysis, NLP techniques \u2705 System Integration - Seamlessly added features to existing system \u2705 Performance Awareness - Considered latency/memory trade-offs</p>"},{"location":"howto/entity-analytics/#resume-bullet-points","title":"Resume Bullet Points","text":"<ul> <li>\"Implemented explainable AI features providing human-readable confidence justifications\"</li> <li>\"Built entity analytics system with sentiment analysis and co-occurrence detection\"</li> <li>\"Enhanced RAG system with researcher-friendly metadata reducing user confusion\"</li> <li>\"Integrated NLP techniques (sentiment analysis, entity extraction) into production API\"</li> </ul>"},{"location":"howto/entity-analytics/#known-limitations","title":"Known Limitations","text":"<ol> <li>Sentiment Neutrality: Political speech often analyzed as neutral</li> <li> <p>Fix: Fine-tune sentiment model on political corpus</p> </li> <li> <p>Association Quality: Stopword filtering may miss context</p> </li> <li> <p>Fix: Use more sophisticated NLP (dependency parsing, n-grams)</p> </li> <li> <p>Performance: 2-5s latency for sentiment analysis</p> </li> <li> <p>Fix: Async processing, caching, or pre-computation</p> </li> <li> <p>Entity Detection: Simple capitalization heuristic</p> </li> <li>Fix: Use proper NER (spaCy, Hugging Face)</li> </ol>"},{"location":"howto/entity-analytics/#comparison-before-vs-after","title":"Comparison: Before vs After","text":"Feature Before After Confidence info Just \"MEDIUM\" Full explanation with factors Entity mentions Count only Count + sentiment + associations User insight Minimal Comprehensive analytics Transparency Low High (explainable) Researcher-friendly No Yes (data science vibes)"},{"location":"howto/entity-analytics/#documentation","title":"Documentation","text":"<ul> <li>Quick test: <code>uv run python test_enhancements.py</code></li> <li>API docs: <code>http://localhost:8001/docs</code> (auto-updated)</li> <li>Code: <code>src/rag_service.py</code> (well-commented)</li> </ul> <p>Implementation Date: November 1, 2025 Author: Kristiyan Bonev Project: Donald Trump Rally Speeches NLP</p>"},{"location":"howto/logging/","title":"Production Logging Guide","text":"<p>This project implements professional logging with automatic environment detection, structured output, and cloud-ready JSON formatting for production deployment.</p>"},{"location":"howto/logging/#architecture","title":"Architecture","text":""},{"location":"howto/logging/#dual-format-logging-system","title":"Dual-Format Logging System","text":"<p>The application uses <code>src/logging_config.py</code> to provide:</p> <ul> <li>Development Mode: Colored, timestamped, human-readable logs</li> <li>Production Mode: Structured JSON logs for cloud platforms (Azure, Docker)</li> <li>Automatic Detection: Uses <code>ENVIRONMENT</code> setting to choose format</li> </ul>"},{"location":"howto/logging/#why-professional-logging","title":"Why Professional Logging?","text":"<ul> <li>\u2705 Structured Data: JSON format for log aggregation tools</li> <li>\u2705 Environment-Aware: Different formats for dev/prod automatically</li> <li>\u2705 Cloud-Native: Works with Azure Application Insights, CloudWatch, etc.</li> <li>\u2705 Performance: Efficient formatting and filtering</li> <li>\u2705 Observability: Proper levels, timestamps, module names</li> <li>\u2705 Thread-Safe: Works correctly in async/concurrent code</li> </ul>"},{"location":"howto/logging/#log-levels","title":"Log Levels","text":"Level Use Case Example DEBUG Detailed diagnostic information <code>logger.debug(f\"Query embedding: {embedding[:5]}\")</code> INFO Important application events <code>logger.info(\"RAG service initialized with 1082 chunks\")</code> WARNING Unexpected but recoverable situations <code>logger.warning(\"LLM not configured, using extraction\")</code> ERROR Application errors requiring attention <code>logger.error(f\"Failed to generate answer: {e}\")</code> CRITICAL System-critical failures <code>logger.critical(\"Vector database connection lost\")</code>"},{"location":"howto/logging/#logging-formats","title":"Logging Formats","text":""},{"location":"howto/logging/#development-format-colored","title":"Development Format (Colored)","text":"<p>Automatically enabled when <code>ENVIRONMENT=development</code>:</p> <pre><code>2025-11-04 12:34:56 | INFO     | src.api              | Application startup complete\n2025-11-04 12:34:57 | DEBUG    | src.rag_service      | Performing hybrid search\n2025-11-04 12:34:58 | ERROR    | src.llm_service      | Gemini API error: rate limit\n</code></pre> <p>Features: - ANSI color coding by level (INFO=green, ERROR=red, etc.) - Timestamp in readable format - Module name right-aligned - Human-readable message</p>"},{"location":"howto/logging/#production-format-json","title":"Production Format (JSON)","text":"<p>Automatically enabled when <code>ENVIRONMENT=production</code>:</p> <pre><code>{\"timestamp\": \"2025-11-04 12:34:56\", \"level\": \"INFO\", \"name\": \"src.api\", \"message\": \"Application startup complete\"}\n{\"timestamp\": \"2025-11-04 12:34:57\", \"level\": \"DEBUG\", \"name\": \"src.rag_service\", \"message\": \"Performing hybrid search\"}\n{\"timestamp\": \"2025-11-04 12:34:58\", \"level\": \"ERROR\", \"name\": \"src.llm_service\", \"message\": \"Gemini API error: rate limit\", \"exception\": \"...traceback...\"}\n</code></pre> <p>Features: - Machine-parseable JSON - Automatic exception field for errors - Compatible with Azure Application Insights, CloudWatch, ELK stack - Efficient for log aggregation</p>"},{"location":"howto/logging/#configuration","title":"Configuration","text":""},{"location":"howto/logging/#environment-variables","title":"Environment Variables","text":"<p>Set in <code>.env</code> file:</p> <pre><code># Choose environment (affects log format)\nENVIRONMENT=development  # colored logs\nENVIRONMENT=production   # JSON logs\n\n# Choose log level\nLOG_LEVEL=INFO   # Standard (recommended)\nLOG_LEVEL=DEBUG  # Verbose debugging\nLOG_LEVEL=ERROR  # Errors only\n</code></pre>"},{"location":"howto/logging/#logging-module-srclogging_configpy","title":"Logging Module (<code>src/logging_config.py</code>)","text":"<p>The centralized logging configuration module provides:</p> <pre><code>from src.logging_config import configure_logging\n\n# Called automatically by Settings.setup_logging()\nconfigure_logging(\n    level=\"INFO\",\n    use_json=True,  # or False for colored\n    include_uvicorn=True,  # configure uvicorn loggers\n)\n</code></pre> <p>Key Features: - <code>JsonFormatter</code>: Structured JSON output - <code>ColoredFormatter</code>: ANSI-colored terminal output - Automatic third-party logger suppression (chromadb, httpx, etc.) - ChromaDB telemetry filter (removes noise) - Uvicorn integration</p>"},{"location":"howto/logging/#usage-examples","title":"Usage Examples","text":""},{"location":"howto/logging/#basic-logging","title":"Basic Logging","text":"<pre><code>import logging\n\nlogger = logging.getLogger(__name__)\n\nlogger.debug(\"Detailed debug information\")\nlogger.info(\"Informational message\")\nlogger.warning(\"Warning message\")\nlogger.error(\"Error occurred\")\nlogger.critical(\"Critical error\")\n</code></pre>"},{"location":"howto/logging/#with-context","title":"With Context","text":"<pre><code># Log with context\nlogger.info(f\"Loading {doc_count} documents from {directory}\")\n\n# Log with variables\nlogger.debug(f\"Chunk size: {chunk_size}, overlap: {overlap}\")\n\n# Log with stack traces\ntry:\n    risky_operation()\nexcept Exception as e:\n    logger.error(f\"Operation failed: {e}\", exc_info=True)\n</code></pre>"},{"location":"howto/logging/#module-specific-loggers","title":"Module-Specific Loggers","text":"<p>Each module should have its own logger:</p> <pre><code># src/llm_service.py\nimport logging\nlogger = logging.getLogger(__name__)  # Creates 'src.llm_service' logger\n\n# src/rag_service.py  \nimport logging\nlogger = logging.getLogger(__name__)  # Creates 'src.rag_service' logger\n</code></pre>"},{"location":"howto/logging/#configuration_1","title":"Configuration","text":""},{"location":"howto/logging/#via-environment-variable","title":"Via Environment Variable","text":"<p>Set in <code>.env</code>: <pre><code>LOG_LEVEL=\"DEBUG\"  # Show all messages\nLOG_LEVEL=\"INFO\"   # Show info and above (default)\nLOG_LEVEL=\"WARNING\"  # Show warnings and errors only\nLOG_LEVEL=\"ERROR\"  # Show errors only\n</code></pre></p>"},{"location":"howto/logging/#via-code","title":"Via Code","text":"<pre><code>from src.config import get_settings\n\nsettings = get_settings()\nsettings.setup_logging()  # Configures logging based on LOG_LEVEL\n</code></pre>"},{"location":"howto/logging/#custom-configuration","title":"Custom Configuration","text":"<pre><code>import logging\n\n# Configure custom logger\nlogging.basicConfig(\n    level=logging.DEBUG,\n    format='%(levelname)-8s %(name)-20s %(message)s',\n    handlers=[\n        logging.FileHandler('app.log'),\n        logging.StreamHandler()\n    ]\n)\n</code></pre>"},{"location":"howto/logging/#current-logging-in-project","title":"Current Logging in Project","text":""},{"location":"howto/logging/#startup-sequence","title":"Startup Sequence","text":"<pre><code>INFO     src.config           Application: Trump Speeches NLP Chatbot API v0.1.0\nINFO     src.config           Environment: development\nINFO     src.config           Log Level: INFO\nINFO     src.config           LLM Provider: gemini\nINFO     src.config           LLM Model: gemini-2.5-flash\nINFO     src.api              Loading sentiment analysis model: ProsusAI/finbert\nINFO     src.models           Sentiment model loaded: ProsusAI/finbert\nINFO     src.api              Initializing GEMINI LLM service...\nDEBUG    src.llm_service      Initializing Gemini LLM with model: gemini-2.5-flash\nINFO     src.llm_service      Gemini LLM initialized: model=gemini-2.5-flash, temp=0.3\nINFO     src.llm_service      Gemini API connection test successful: OK\nINFO     src.api              \u2713 LLM service initialized and tested successfully\nINFO     src.api              Initializing RAG service...\nDEBUG    src.rag_service      Initializing RAG service: collection=speeches\nINFO     src.rag_service      Loading embedding model: all-mpnet-base-v2\nINFO     src.rag_service      Loading re-ranker model: cross-encoder/ms-marco-MiniLM-L-6-v2\nINFO     src.rag_service      RAG service using LLM for answer generation\nINFO     src.api              \u2713 RAG service initialized with 1082 existing chunks\nINFO     src.api              Application startup complete\n</code></pre>"},{"location":"howto/logging/#query-processing","title":"Query Processing","text":"<pre><code>DEBUG    src.rag_service      Generating answer with 5 context chunks using LLM\nDEBUG    src.llm_service      Sending prompt to Gemini (length: 3245 chars)\nINFO     src.llm_service      Gemini generated answer successfully (length: 234 chars)\nDEBUG    src.rag_service      LLM response received (length: 234 chars)\n</code></pre>"},{"location":"howto/logging/#error-handling","title":"Error Handling","text":"<pre><code>ERROR    src.llm_service      Gemini generation failed: API key invalid\nTraceback (most recent call last):\n  File \"src/llm_service.py\", line 127, in generate_answer\n    response = self.model.generate_content(prompt)\nValueError: Invalid API key\nERROR    src.rag_service      LLM generation failed, falling back to extraction\n</code></pre>"},{"location":"howto/logging/#log-output-formats","title":"Log Output Formats","text":""},{"location":"howto/logging/#default-format","title":"Default Format","text":"<pre><code>LEVEL    MODULE_NAME          MESSAGE\nINFO     src.api              Application startup complete\n</code></pre>"},{"location":"howto/logging/#with-timestamps","title":"With Timestamps","text":"<pre><code>logging.basicConfig(\n    format='%(asctime)s %(levelname)-8s %(name)-20s %(message)s',\n    datefmt='%Y-%m-%d %H:%M:%S'\n)\n</code></pre> <p>Output: <pre><code>2025-11-04 10:30:45 INFO     src.api              Application startup complete\n</code></pre></p>"},{"location":"howto/logging/#suppressing-noisy-loggers","title":"Suppressing Noisy Loggers","text":"<p>Some third-party libraries are verbose. Suppress them:</p> <pre><code># Suppress ChromaDB telemetry errors\nlogging.getLogger(\"chromadb.telemetry.product.posthog\").setLevel(logging.ERROR)\n\n# Suppress httpx debug messages\nlogging.getLogger(\"httpx\").setLevel(logging.WARNING)\n\n# Suppress transformers warnings\nlogging.getLogger(\"transformers\").setLevel(logging.ERROR)\n</code></pre> <p>This is already configured in <code>src/config.py</code>:</p> <pre><code>def setup_logging(self) -&gt; None:\n    \"\"\"Configure application-wide logging based on settings.\"\"\"\n    logging.basicConfig(\n        level=getattr(logging, self.log_level),\n        format=\"%(levelname)-8s %(name)-20s %(message)s\",\n        force=True,\n    )\n\n    # Suppress noisy third-party loggers\n    logging.getLogger(\"chromadb.telemetry.product.posthog\").setLevel(logging.ERROR)\n    logging.getLogger(\"httpx\").setLevel(logging.WARNING)\n    logging.getLogger(\"httpcore\").setLevel(logging.WARNING)\n</code></pre>"},{"location":"howto/logging/#production-logging","title":"Production Logging","text":""},{"location":"howto/logging/#azure-app-service","title":"Azure App Service","text":"<p>Logs automatically stream to Azure Monitor:</p> <pre><code># View live logs\naz webapp log tail --name myapp --resource-group mygroup\n\n# Download logs\naz webapp log download --name myapp --resource-group mygroup\n</code></pre>"},{"location":"howto/logging/#docker-logs","title":"Docker Logs","text":"<pre><code># View container logs\ndocker logs mycontainer\n\n# Follow logs\ndocker logs -f mycontainer\n\n# Save to file\ndocker logs mycontainer &gt; app.log 2&gt;&amp;1\n</code></pre>"},{"location":"howto/logging/#log-aggregation","title":"Log Aggregation","text":"<p>For production, integrate with: - Azure Application Insights - AWS CloudWatch - Datadog - Sentry (for errors) - ELK Stack (Elasticsearch, Logstash, Kibana)</p> <p>Example with Azure Application Insights:</p> <pre><code>from opencensus.ext.azure.log_exporter import AzureLogHandler\n\nlogger = logging.getLogger(__name__)\nlogger.addHandler(AzureLogHandler(\n    connection_string='InstrumentationKey=your-key'\n))\n</code></pre>"},{"location":"howto/logging/#best-practices","title":"Best Practices","text":""},{"location":"howto/logging/#1-use-appropriate-levels","title":"1. Use Appropriate Levels","text":"<pre><code># \u274c Wrong\nlogger.info(f\"Debug variable: {x}\")  # Should be DEBUG\nlogger.error(\"User clicked button\")  # Should be INFO\n\n# \u2705 Correct\nlogger.debug(f\"Variable value: {x}\")\nlogger.info(\"User authentication successful\")\nlogger.warning(\"Rate limit approaching\")\nlogger.error(\"Database query failed\")\n</code></pre>"},{"location":"howto/logging/#2-include-context","title":"2. Include Context","text":"<pre><code># \u274c Not helpful\nlogger.error(\"Failed\")\n\n# \u2705 Helpful\nlogger.error(f\"Failed to load model {model_name}: {e}\")\n</code></pre>"},{"location":"howto/logging/#3-use-exc_info-for-tracebacks","title":"3. Use exc_info for Tracebacks","text":"<pre><code># \u274c Manual traceback\ntry:\n    risky_operation()\nexcept Exception as e:\n    import traceback\n    logger.error(traceback.format_exc())\n\n# \u2705 Automatic traceback\ntry:\n    risky_operation()\nexcept Exception as e:\n    logger.error(f\"Operation failed: {e}\", exc_info=True)\n</code></pre>"},{"location":"howto/logging/#4-dont-log-sensitive-data","title":"4. Don't Log Sensitive Data","text":"<pre><code># \u274c Leaks secrets\nlogger.info(f\"Using API key: {api_key}\")\n\n# \u2705 Safe\nlogger.info(\"Using API key: \u2713 Configured\")\n</code></pre>"},{"location":"howto/logging/#5-performance-conscious","title":"5. Performance-Conscious","text":"<pre><code># \u274c Expensive string formatting even if not logged\nlogger.debug(f\"Data: {expensive_computation()}\")\n\n# \u2705 Only compute if needed\nif logger.isEnabledFor(logging.DEBUG):\n    logger.debug(f\"Data: {expensive_computation()}\")\n</code></pre>"},{"location":"howto/logging/#migration-from-print-statements","title":"Migration from Print Statements","text":""},{"location":"howto/logging/#before","title":"Before","text":"<pre><code>print(f\"Loading model: {model_name}\")\nprint(f\"ERROR: Failed to load: {e}\")\nimport traceback\nprint(traceback.format_exc())\n</code></pre>"},{"location":"howto/logging/#after","title":"After","text":"<pre><code>logger.info(f\"Loading model: {model_name}\")\nlogger.error(f\"Failed to load: {e}\", exc_info=True)\n</code></pre>"},{"location":"howto/logging/#debugging","title":"Debugging","text":""},{"location":"howto/logging/#enable-debug-logging","title":"Enable DEBUG Logging","text":"<pre><code># Temporary\nLOG_LEVEL=DEBUG uv run uvicorn src.api:app\n\n# Or edit .env\nLOG_LEVEL=\"DEBUG\"\n</code></pre>"},{"location":"howto/logging/#filter-specific-modules","title":"Filter Specific Modules","text":"<pre><code># Only debug RAG service\nlogging.getLogger(\"src.rag_service\").setLevel(logging.DEBUG)\nlogging.getLogger(\"src.llm_service\").setLevel(logging.DEBUG)\n\n# Suppress others\nlogging.getLogger(\"src.models\").setLevel(logging.WARNING)\n</code></pre>"},{"location":"howto/logging/#debug-startup-issues","title":"Debug Startup Issues","text":"<pre><code># Enable all debug output\nLOG_LEVEL=DEBUG uv run uvicorn src.api:app --reload\n</code></pre>"},{"location":"howto/logging/#further-reading","title":"Further Reading","text":"<ul> <li>Python Logging HOWTO</li> <li>Python Logging Cookbook</li> <li>Structlog (advanced structured logging)</li> </ul>"},{"location":"howto/testing/","title":"Testing &amp; Development Guide","text":""},{"location":"howto/testing/#running-tests","title":"Running Tests","text":""},{"location":"howto/testing/#using-uv-recommended","title":"Using uv (recommended)","text":"<p>This repository uses uv to manage virtual environments and run commands in a reproducible project environment. If you've already been using <code>uv</code> in this project, the examples below will work as-is.</p> <pre><code># Install project dependencies (including dev groups defined in pyproject)\nuv sync            # sync all default groups\nuv sync --group dev  # sync only dev dependencies (if grouped)\n\n# Run a command inside the project's environment\nuv run &lt;command&gt;   # e.g. `uv run pytest` or `uv run black src/`\n</code></pre> <p>If you prefer to use Poetry directly, the original Poetry commands are still valid and left as alternatives in this document.</p>"},{"location":"howto/testing/#install-development-dependencies-alternative-poetry","title":"Install Development Dependencies (alternative: Poetry)","text":"<pre><code># With Poetry (alternative)\npoetry install --with dev\n\n# Or activate Poetry shell and run commands directly\npoetry shell\n</code></pre>"},{"location":"howto/testing/#run-all-tests","title":"Run All Tests","text":"<pre><code># Run all tests with coverage\nuv run pytest\n\n# Run only unit tests\nuv run pytest -m unit\n\n# Run only integration tests\nuv run pytest -m integration\n\n# Run with verbose output\nuv run pytest -v\n\n# Run specific test file\nuv run pytest tests/test_preprocessing.py\n</code></pre>"},{"location":"howto/testing/#code-coverage","title":"Code Coverage","text":"<pre><code># Generate coverage report\nuv run pytest --cov=src --cov-report=html\n\n# Open coverage report\nstart htmlcov/index.html  # Windows\n</code></pre>"},{"location":"howto/testing/#code-quality-tools","title":"Code Quality Tools","text":""},{"location":"howto/testing/#formatting","title":"Formatting","text":"<pre><code># Format code with Black (via uv)\nuv run black src/\n\n# Check formatting without changes\nuv run black --check src/\n\n# Sort imports with isort\nuv run isort src/\n\n# Check imports without changes\nuv run isort --check-only src/\n</code></pre>"},{"location":"howto/testing/#linting","title":"Linting","text":"<pre><code># Run flake8\nuv run flake8 src/\n\n# Show detailed statistics\nuv run flake8 src/ --count --statistics --show-source\n</code></pre>"},{"location":"howto/testing/#type-checking","title":"Type Checking","text":"<pre><code># Run mypy type checker\nuv run mypy src/\n</code></pre>"},{"location":"howto/testing/#run-all-quality-checks","title":"Run All Quality Checks","text":"<pre><code># Run everything at once (uv wrapper)\nuv run black src/ &amp;&amp; uv run isort src/ &amp;&amp; uv run flake8 src/ &amp;&amp; uv run mypy src/ &amp;&amp; uv run pytest\nuv run black src/ ; uv run isort src/ ; uv run flake8 src/ ; uv run mypy src/ ; uv run pytest\n</code></pre>"},{"location":"howto/testing/#pre-commit-setup-optional","title":"Pre-commit Setup (Optional)","text":"<p>Install pre-commit hooks to automatically run checks before commits. If you manage dev dependencies with <code>uv</code>, use <code>uv sync --group dev</code> to install dev deps (including <code>pre-commit</code>) if listed in <code>pyproject.toml</code>. Otherwise install pre-commit directly:</p> <pre><code># Ensure pre-commit is installed in the project environment\nuv run pip install pre-commit\n\n# Install the git hook\nuv run pre-commit install\n</code></pre> <p>If you prefer Poetry:</p> <pre><code>poetry add --group dev pre-commit\npoetry run pre-commit install\n</code></pre>"},{"location":"howto/testing/#cicd-pipeline","title":"CI/CD Pipeline","text":"<p>The GitHub Actions workflow runs automatically on: - Push to <code>main</code>, <code>develop</code>, or <code>feature/*</code> branches - Pull requests to <code>main</code> or <code>develop</code></p>"},{"location":"howto/testing/#pipeline-jobs","title":"Pipeline Jobs:","text":"<ol> <li>Test Suite - Runs on Python 3.11, 3.12, 3.13</li> <li>Unit tests</li> <li>Integration tests (without ML model)</li> <li> <p>Coverage reporting</p> </li> <li> <p>Code Quality - Linting and formatting checks</p> </li> <li>flake8</li> <li>black</li> <li>isort</li> <li> <p>mypy</p> </li> <li> <p>Security - Security scanning</p> </li> <li>safety (dependency vulnerabilities)</li> <li> <p>bandit (code security issues)</p> </li> <li> <p>Build - Docker image build (on main branch only)</p> </li> <li>Builds image</li> <li>Tests health endpoint</li> </ol>"},{"location":"howto/testing/#test-structure","title":"Test Structure","text":"<pre><code>tests/\n\u251c\u2500\u2500 __init__.py\n\u251c\u2500\u2500 test_preprocessing.py  # Unit tests for text processing\n\u251c\u2500\u2500 test_utils.py          # Unit tests for utilities\n\u2514\u2500\u2500 test_api.py            # Integration tests for API endpoints\n</code></pre>"},{"location":"howto/testing/#test-markers","title":"Test Markers","text":"<ul> <li><code>@pytest.mark.unit</code> - Fast unit tests</li> <li><code>@pytest.mark.integration</code> - API integration tests</li> <li><code>@pytest.mark.requires_model</code> - Tests needing ML model (skipped in CI)</li> <li><code>@pytest.mark.slow</code> - Slow-running tests</li> </ul>"},{"location":"howto/testing/#writing-new-tests","title":"Writing New Tests","text":""},{"location":"howto/testing/#unit-test-example","title":"Unit Test Example","text":"<pre><code>import pytest\nfrom src.preprocessing import clean_text\n\n@pytest.mark.unit\ndef test_clean_text():\n    text = \"Hello World!\"\n    result = clean_text(text)\n    assert isinstance(result, str)\n</code></pre>"},{"location":"howto/testing/#api-test-example","title":"API Test Example","text":"<pre><code>import pytest\nfrom fastapi.testclient import TestClient\nfrom src.api import app\n\n@pytest.mark.integration\ndef test_health_check():\n    client = TestClient(app)\n    response = client.get(\"/health\")\n    assert response.status_code == 200\n</code></pre>"},{"location":"howto/testing/#coverage-goals","title":"Coverage Goals","text":"<ul> <li>Target: 70%+ overall coverage</li> <li>Focus: Core logic in <code>src/</code></li> <li>Exclude: ML model internals, notebooks</li> </ul>"},{"location":"howto/testing/#troubleshooting","title":"Troubleshooting","text":""},{"location":"howto/testing/#nltk-data-missing","title":"NLTK Data Missing","text":"<pre><code>uv run python -c \"import nltk; nltk.download('punkt', quiet=True); nltk.download('stopwords', quiet=True); nltk.download('punkt_tab', quiet=True)\"\n</code></pre>"},{"location":"howto/testing/#import-errors","title":"Import Errors","text":"<pre><code># Reinstall dependencies (uv)\nuv sync --group dev\n\n# Or with Poetry\npoetry install --with dev\n</code></pre>"},{"location":"howto/testing/#slow-tests","title":"Slow Tests","text":"<pre><code># Skip slow tests\nuv run pytest -m \"not slow\"\n\n# Skip model-dependent tests\nuv run pytest -m \"not requires_model\"\n</code></pre>"},{"location":"reference/architecture/","title":"System Architecture","text":"<p>This document provides a comprehensive overview of the Trump Speeches NLP Chatbot API architecture, including system components, data flows, and deployment strategies.</p>"},{"location":"reference/architecture/#table-of-contents","title":"Table of Contents","text":"<ul> <li>High-Level Architecture</li> <li>Component Architecture</li> <li>RAG Pipeline</li> <li>Data Flow</li> <li>API Architecture</li> <li>Deployment Architecture</li> <li>Technology Stack</li> <li>Scalability Considerations</li> </ul>"},{"location":"reference/architecture/#high-level-architecture","title":"High-Level Architecture","text":"<pre><code>graph TB\n    Client[Client/Browser]\n    Frontend[Static Frontend&lt;br/&gt;HTML/CSS/JS]\n    API[FastAPI Application&lt;br/&gt;REST API]\n\n    subgraph \"NLP Services\"\n        Sentiment[Sentiment Analyzer&lt;br/&gt;FinBERT]\n        Preprocessing[Text Preprocessing&lt;br/&gt;NLTK]\n        Topics[Topic Extraction&lt;br/&gt;TF-IDF]\n        RAG[RAG Service&lt;br/&gt;ChromaDB + Embeddings]\n    end\n\n    subgraph \"Data Layer\"\n        Speeches[Demo Dataset&lt;br/&gt;Political Speeches]\n        VectorDB[(ChromaDB&lt;br/&gt;Vector Store)]\n        Models[ML Models&lt;br/&gt;Transformers]\n    end\n\n    Client --&gt;|HTTP Requests| Frontend\n    Frontend --&gt;|API Calls| API\n    API --&gt; Sentiment\n    API --&gt; Preprocessing\n    API --&gt; Topics\n    API --&gt; RAG\n\n    Sentiment --&gt; Models\n    RAG --&gt; VectorDB\n    RAG --&gt; Models\n    Preprocessing --&gt; Speeches\n    Topics --&gt; Speeches</code></pre>"},{"location":"reference/architecture/#component-architecture","title":"Component Architecture","text":""},{"location":"reference/architecture/#1-api-layer-srcapipy","title":"1. API Layer (<code>src/api.py</code>)","text":"<p>FastAPI application serving as the main entry point.</p> <p>Responsibilities: - HTTP request handling - Input validation (Pydantic models) - Error handling and logging - CORS middleware - Static file serving - Service orchestration</p> <p>Endpoints: - <code>/analyze/sentiment</code> - Sentiment analysis - <code>/analyze/words</code> - Word frequency analysis - <code>/analyze/topics</code> - Topic extraction - <code>/analyze/ngrams</code> - N-gram analysis - <code>/text/clean</code> - Text preprocessing - <code>/rag/ask</code> - RAG question answering - <code>/rag/search</code> - Semantic search - <code>/rag/stats</code> - Collection statistics - <code>/rag/index</code> - Document indexing - <code>/speeches/stats</code> - Dataset statistics - <code>/speeches/list</code> - List all speeches - <code>/health</code> - Health check</p>"},{"location":"reference/architecture/#2-sentiment-analysis-srcmodelspy","title":"2. Sentiment Analysis (<code>src/models.py</code>)","text":"<p>Transformer-based sentiment classification using FinBERT.</p> <p>Key Features: - Pre-trained FinBERT model (ProsusAI/finbert) - Automatic text chunking for long documents - Confidence scoring - Three-class classification (positive/negative/neutral)</p> <p>Processing Flow: <pre><code>graph LR\n    Input[Raw Text] --&gt; Chunk[Text Chunking&lt;br/&gt;512 tokens max]\n    Chunk --&gt; Tokenize[Tokenization&lt;br/&gt;BERT Tokenizer]\n    Tokenize --&gt; Model[FinBERT Model&lt;br/&gt;Inference]\n    Model --&gt; Aggregate[Score Aggregation&lt;br/&gt;Mean Pooling]\n    Aggregate --&gt; Output[Sentiment + Confidence]</code></pre></p>"},{"location":"reference/architecture/#3-rag-service-srcrag_servicepy","title":"3. RAG Service (<code>src/rag_service.py</code>)","text":"<p>Advanced Retrieval-Augmented Generation for intelligent question answering.</p> <p>Components: - Vector Store: ChromaDB with persistent SQLite storage - Embeddings: sentence-transformers (all-mpnet-base-v2, 768 dimensions) - Hybrid Search: Semantic (dense) + BM25 (sparse) retrieval - Reranking: Cross-encoder for precision optimization - Text Splitter: LangChain RecursiveCharacterTextSplitter - Chunking: 2048 chars with 150 char overlap (~512-768 tokens) - LLM: Google Gemini for answer generation</p> <p>Capabilities: - Document loading and indexing with progress tracking - Hybrid semantic + keyword search - Multi-factor confidence scoring - Entity extraction and analytics - Sentiment analysis for entities - Context-aware answer generation with Gemini</p>"},{"location":"reference/architecture/#4-text-preprocessing-srcpreprocessingpy","title":"4. Text Preprocessing (<code>src/preprocessing.py</code>)","text":"<p>Text cleaning and normalization utilities.</p> <p>Functions: - Stopword removal (NLTK) - Tokenization - Special character removal - URL removal - N-gram extraction</p>"},{"location":"reference/architecture/#5-utilities-srcutilspy","title":"5. Utilities (<code>src/utils.py</code>)","text":"<p>Data loading and analysis helpers.</p> <p>Functions: - Speech loading from directory - Word frequency statistics - Topic extraction (TF-IDF) - Dataset statistics calculation</p>"},{"location":"reference/architecture/#rag-pipeline","title":"RAG Pipeline","text":"<p>Detailed architecture of the Retrieval-Augmented Generation system.</p> <pre><code>graph TB\n    subgraph \"Indexing Phase (Startup)\"\n        Docs[Text Documents&lt;br/&gt;*.txt files]\n        Load[Document Loader]\n        Split[Text Splitter&lt;br/&gt;500 chars, 50 overlap]\n        Embed[Embedding Model&lt;br/&gt;all-MiniLM-L6-v2]\n        Store[(ChromaDB&lt;br/&gt;Vector Store)]\n\n        Docs --&gt; Load\n        Load --&gt; Split\n        Split --&gt;|Text Chunks| Embed\n        Embed --&gt;|384-dim Vectors| Store\n    end\n\n    subgraph \"Query Phase (Runtime)\"\n        Question[User Question]\n        QEmbed[Query Embedding]\n        Search[Similarity Search&lt;br/&gt;Cosine Distance]\n        Retrieve[Top-K Retrieval]\n        Generate[Answer Generation&lt;br/&gt;Context-based]\n        Response[Answer + Context]\n\n        Question --&gt; QEmbed\n        QEmbed --&gt; Search\n        Store -.-&gt;|Vector Lookup| Search\n        Search --&gt; Retrieve\n        Retrieve --&gt; Generate\n        Generate --&gt; Response\n    end</code></pre>"},{"location":"reference/architecture/#rag-workflow-details","title":"RAG Workflow Details","text":"<p>1. Indexing (One-time or on-demand): <pre><code>1. Load documents from directory\n2. Split into chunks (RecursiveCharacterTextSplitter)\n   - chunk_size: 2048 characters (~512-768 tokens)\n   - chunk_overlap: 150 characters (~100-150 tokens)\n3. Generate embeddings (sentence-transformers)\n   - Model: all-mpnet-base-v2\n   - Dimension: 768\n4. Store in ChromaDB with metadata:\n   - source: filename\n   - chunk_index: position in document\n   - total_chunks: document length\n</code></pre></p> <p>2. Querying: <pre><code>1. Receive question from user\n2. Extract entities from question (capitalized words)\n3. Generate question embedding (same model)\n4. Hybrid search:\n   a. Semantic search: cosine similarity on embeddings\n   b. BM25 search: keyword matching\n   c. Combine results with configurable weights\n5. Cross-encoder reranking for precision\n6. Retrieve top-k most relevant chunks (default k=5, max k=15)\n7. Calculate multi-factor confidence score:\n   - Retrieval quality (40%): semantic similarity\n   - Score consistency (25%): low variance = higher confidence\n   - Coverage (20%): number of supporting chunks\n   - Entity coverage (15%): mention frequency in results\n8. Generate entity statistics (mentions, sentiment, associations)\n9. Build context-aware prompt with entity focus\n10. Generate answer using Gemini LLM\n11. Return answer with:\n    - Generated text\n    - Confidence score and explanation\n    - Supporting context chunks\n    - Source attribution\n    - Entity analytics (if applicable)\n</code></pre></p> <p>3. Confidence Scoring:</p> <p>Multi-factor calculation combining: - Retrieval Quality (40%): Average semantic similarity (0-1) - Consistency (25%): Score variance (low variance = high confidence) - Coverage (20%): Number of supporting chunks (normalized) - Entity Coverage (15%): % of chunks mentioning query entities</p> <p>Confidence Levels: - High: combined_score \u2265 0.7 - Medium: 0.4 \u2264 combined_score &lt; 0.7 - Low: combined_score &lt; 0.4</p>"},{"location":"reference/architecture/#data-flow","title":"Data Flow","text":""},{"location":"reference/architecture/#sentiment-analysis-flow","title":"Sentiment Analysis Flow","text":"<pre><code>sequenceDiagram\n    participant User\n    participant Frontend\n    participant API\n    participant SentimentAnalyzer\n    participant FinBERT\n\n    User-&gt;&gt;Frontend: Enter text\n    Frontend-&gt;&gt;API: POST /analyze/sentiment\n    API-&gt;&gt;SentimentAnalyzer: analyze_sentiment(text)\n    SentimentAnalyzer-&gt;&gt;SentimentAnalyzer: Chunk text (512 tokens)\n    loop For each chunk\n        SentimentAnalyzer-&gt;&gt;FinBERT: Classify chunk\n        FinBERT--&gt;&gt;SentimentAnalyzer: Scores\n    end\n    SentimentAnalyzer-&gt;&gt;SentimentAnalyzer: Aggregate scores\n    SentimentAnalyzer--&gt;&gt;API: Sentiment + Confidence\n    API--&gt;&gt;Frontend: JSON Response\n    Frontend--&gt;&gt;User: Display results</code></pre>"},{"location":"reference/architecture/#rag-question-answering-flow","title":"RAG Question Answering Flow","text":"<pre><code>sequenceDiagram\n    participant User\n    participant Frontend\n    participant API\n    participant RAGService\n    participant ChromaDB\n    participant Embeddings\n\n    User-&gt;&gt;Frontend: Ask question\n    Frontend-&gt;&gt;API: POST /rag/ask\n    API-&gt;&gt;RAGService: ask(question)\n    RAGService-&gt;&gt;Embeddings: Encode question\n    Embeddings--&gt;&gt;RAGService: Query vector\n    RAGService-&gt;&gt;ChromaDB: Search similar vectors\n    ChromaDB--&gt;&gt;RAGService: Top-k chunks\n    RAGService-&gt;&gt;RAGService: Generate answer\n    RAGService--&gt;&gt;API: Answer + Context + Sources\n    API--&gt;&gt;Frontend: JSON Response\n    Frontend--&gt;&gt;User: Display answer &amp; context</code></pre>"},{"location":"reference/architecture/#api-architecture","title":"API Architecture","text":""},{"location":"reference/architecture/#requestresponse-models-pydantic","title":"Request/Response Models (Pydantic)","text":"<pre><code># Input Models\nTextInput\nNGramRequest\nRAGQueryRequest\nRAGSearchRequest\n\n# Response Models\nSentimentResponse\nWordFrequencyResponse\nTopicResponse\nStatsResponse\nRAGAnswerResponse\nRAGStatsResponse\n</code></pre>"},{"location":"reference/architecture/#middleware-stack","title":"Middleware Stack","text":"<pre><code>User Request\n    \u2193\nCORS Middleware (allow all origins in dev)\n    \u2193\nFastAPI Routing\n    \u2193\nPydantic Validation\n    \u2193\nEndpoint Handler\n    \u2193\nBusiness Logic (Services)\n    \u2193\nResponse Serialization\n    \u2193\nHTTP Response\n</code></pre>"},{"location":"reference/architecture/#error-handling-strategy","title":"Error Handling Strategy","text":"<pre><code>try:\n    # Business logic\nexcept SpecificError:\n    # Handle known errors\n    raise HTTPException(status_code=4xx)\nexcept Exception as e:\n    # Log unexpected errors\n    logger.error(f\"Error: {e}\")\n    raise HTTPException(status_code=500)\n</code></pre>"},{"location":"reference/architecture/#deployment-architecture","title":"Deployment Architecture","text":""},{"location":"reference/architecture/#docker-multi-stage-build","title":"Docker Multi-Stage Build","text":"<pre><code>graph TB\n    subgraph \"Stage 1: Builder\"\n        UV[uv Package Manager]\n        Deps[Install Dependencies]\n        UV --&gt; Deps\n    end\n\n    subgraph \"Stage 2: Runtime\"\n        Slim[Python 3.12-slim]\n        Copy[Copy Dependencies]\n        App[Copy Application Code]\n        Models[Download Models&lt;br/&gt;NLTK + Transformers]\n\n        Slim --&gt; Copy\n        Copy --&gt; App\n        App --&gt; Models\n    end\n\n    Deps -.-&gt;|Python packages| Copy</code></pre>"},{"location":"reference/architecture/#deployment-options","title":"Deployment Options","text":""},{"location":"reference/architecture/#option-1-render-via-docker-hub","title":"Option 1: Render (via Docker Hub)","text":"<pre><code>graph LR\n    GH[GitHub Actions] --&gt;|Build &amp; Push| DH[Docker Hub]\n    DH --&gt;|Auto-deploy| Render[Render Platform]\n    Render --&gt;|Serve| Users[End Users]</code></pre> <p>Flow: 1. Push to <code>main</code> branch 2. GitHub Actions builds Docker image 3. Push to Docker Hub (<code>trump-speeches-nlp-chatbot:latest</code>) 4. Render detects new image 5. Render pulls and deploys 6. Health check <code>/health</code></p>"},{"location":"reference/architecture/#option-2-azure-web-app","title":"Option 2: Azure Web App","text":"<p>Via ACR: <pre><code>graph LR\n    GH[GitHub Actions] --&gt;|Build &amp; Push| ACR[Azure Container&lt;br/&gt;Registry]\n    ACR --&gt;|Deploy| Azure[Azure Web App]\n    Azure --&gt;|Serve| Users[End Users]</code></pre></p> <p>Via Docker Hub: <pre><code>graph LR\n    GH[GitHub Actions] --&gt;|Build &amp; Push| DH[Docker Hub]\n    DH --&gt;|Deploy| Azure[Azure Web App]\n    Azure --&gt;|Serve| Users[End Users]</code></pre></p>"},{"location":"reference/architecture/#cicd-pipeline","title":"CI/CD Pipeline","text":"<pre><code>graph TB\n    Push[Push to main] --&gt; CI[CI Workflow]\n    Push --&gt; Security[Security Scan]\n\n    CI --&gt; Tests[Unit Tests&lt;br/&gt;Pytest]\n    CI --&gt; Lint[Code Quality&lt;br/&gt;flake8, black, mypy]\n\n    Security --&gt; PipAudit[pip-audit&lt;br/&gt;Dependency Check]\n    Security --&gt; Bandit[bandit&lt;br/&gt;Security Analysis]\n\n    Tests --&gt; Build[Build Docker Image]\n    Lint --&gt; Build\n    PipAudit --&gt; Build\n    Bandit --&gt; Build\n\n    Build --&gt; DHPush[Push to Docker Hub]\n    Build --&gt; ACRPush[Push to ACR]\n\n    DHPush --&gt; RenderDeploy[Deploy to Render]\n    ACRPush --&gt; AzureDeploy[Deploy to Azure]</code></pre>"},{"location":"reference/architecture/#technology-stack","title":"Technology Stack","text":""},{"location":"reference/architecture/#core-technologies","title":"Core Technologies","text":"Layer Technology Purpose API Framework FastAPI 0.116+ High-performance async API Web Server Uvicorn ASGI server LLM Integration Google Gemini 2.5 Flash Answer generation ML Framework PyTorch 2.5+ Deep learning backend NLP Library Transformers 4.57+ Pre-trained models Text Processing NLTK 3.9+ Tokenization, stopwords Vector DB ChromaDB 0.5+ Persistent embeddings storage Embeddings sentence-transformers 3.3+ Semantic embeddings (MPNet) Reranking Cross-encoder Precision optimization Keyword Search rank-bm25 Sparse retrieval RAG Framework LangChain 0.3+ Text splitting utilities"},{"location":"reference/architecture/#supporting-technologies","title":"Supporting Technologies","text":"Category Technology Version Dependency Mgmt uv Latest Containerization Docker Latest CI/CD GitHub Actions - Testing pytest 8.3+ Code Quality black, flake8, mypy, isort Latest Security pip-audit, bandit Latest"},{"location":"reference/architecture/#model-details","title":"Model Details","text":"Model Task Source Size Gemini 2.5 Flash Answer Generation Google AI API-based FinBERT Sentiment Analysis ProsusAI/finbert ~440MB all-mpnet-base-v2 Embeddings (768d) sentence-transformers ~420MB ms-marco-MiniLM Reranking cross-encoder ~80MB"},{"location":"reference/architecture/#scalability-considerations","title":"Scalability Considerations","text":""},{"location":"reference/architecture/#current-architecture","title":"Current Architecture","text":"<ul> <li>Compute: Single-instance deployment</li> <li>Storage: Local filesystem + ChromaDB</li> <li>Concurrency: Async FastAPI (handles concurrent requests)</li> </ul>"},{"location":"reference/architecture/#scaling-strategies","title":"Scaling Strategies","text":""},{"location":"reference/architecture/#1-horizontal-scaling","title":"1. Horizontal Scaling","text":"<pre><code>graph TB\n    LB[Load Balancer]\n    API1[API Instance 1]\n    API2[API Instance 2]\n    API3[API Instance 3]\n    SharedDB[(Shared ChromaDB&lt;br/&gt;Postgres pgvector)]\n\n    LB --&gt; API1\n    LB --&gt; API2\n    LB --&gt; API3\n\n    API1 --&gt; SharedDB\n    API2 --&gt; SharedDB\n    API3 --&gt; SharedDB</code></pre> <p>Required Changes: - Replace ChromaDB with pgvector (Postgres) or Pinecone - Use shared model storage (S3/Azure Blob) - Add Redis for caching</p>"},{"location":"reference/architecture/#2-vertical-scaling","title":"2. Vertical Scaling","text":"<p>Current Requirements: - RAM: ~2GB (models + API) - CPU: 1-2 cores - Storage: ~1GB (models + data)</p> <p>Optimized for: - RAM: 4-8GB for concurrent requests - CPU: 4+ cores for parallel processing - Storage: 5GB+ for larger datasets</p>"},{"location":"reference/architecture/#3-performance-optimizations","title":"3. Performance Optimizations","text":"<p>Already Implemented: - Multi-stage Docker builds - Model pre-loading on startup - Async request handling - Efficient text chunking</p> <p>Future Improvements: - Model quantization (reduce size) - GPU acceleration (CUDA support) - Response caching (Redis) - CDN for static files - Database connection pooling - Background task queues (Celery)</p>"},{"location":"reference/architecture/#resource-usage","title":"Resource Usage","text":"Component RAM CPU Storage FastAPI ~100MB Low - FinBERT ~1GB Medium 440MB all-MiniLM-L6-v2 ~200MB Low 80MB ChromaDB ~100MB Low Variable NLTK Data ~50MB Low 50MB Total ~2GB 1-2 cores ~1GB"},{"location":"reference/architecture/#security-architecture","title":"Security Architecture","text":""},{"location":"reference/architecture/#current-security-measures","title":"Current Security Measures","text":"<ol> <li>Dependency Scanning: pip-audit (weekly)</li> <li>Code Analysis: bandit</li> <li>Input Validation: Pydantic models</li> <li>Non-root Container: User <code>appuser</code> (UID 1000)</li> <li>Health Checks: <code>/health</code> endpoint</li> </ol>"},{"location":"reference/architecture/#production-recommendations","title":"Production Recommendations","text":"<ol> <li>Authentication: Add API key validation</li> <li>Rate Limiting: Implement per-IP limits</li> <li>HTTPS: Use reverse proxy (Nginx)</li> <li>CORS: Restrict to specific origins</li> <li>Secrets Management: Use environment variables</li> <li>Logging: Centralized logging (ELK stack)</li> <li>Monitoring: Prometheus + Grafana</li> </ol>"},{"location":"reference/architecture/#monitoring-observability","title":"Monitoring &amp; Observability","text":""},{"location":"reference/architecture/#recommended-metrics","title":"Recommended Metrics","text":"<p>Application Metrics: - Request count (by endpoint) - Response time (p50, p95, p99) - Error rate (4xx, 5xx) - Model inference time</p> <p>System Metrics: - CPU usage - Memory usage - Disk I/O - Network I/O</p> <p>Business Metrics: - Total analyses performed - Most used endpoints - Average sentiment scores - RAG query accuracy</p>"},{"location":"reference/architecture/#implementation-example-prometheus","title":"Implementation Example (Prometheus)","text":"<pre><code>from prometheus_client import Counter, Histogram\n\nrequest_count = Counter('api_requests_total', 'Total requests', ['endpoint'])\nrequest_duration = Histogram('api_request_duration_seconds', 'Request duration')\n</code></pre>"},{"location":"reference/architecture/#future-architecture-enhancements","title":"Future Architecture Enhancements","text":""},{"location":"reference/architecture/#1-advanced-rag-features","title":"1. Advanced RAG Features","text":"<ul> <li>Query Caching: Redis layer for common questions</li> <li>Multi-modal: Support PDFs, images, audio transcripts</li> <li>Temporal Analysis: Sentiment trends over time</li> <li>Entity Relationships: Knowledge graph visualization</li> <li>Fine-tuned Embeddings: Domain-specific embedding models</li> </ul>"},{"location":"reference/architecture/#2-performance-optimizations","title":"2. Performance Optimizations","text":"<ul> <li>Async Processing: Background tasks for entity analytics</li> <li>GPU Acceleration: CUDA support for faster inference</li> <li>Model Quantization: Reduce model sizes</li> <li>Response Streaming: WebSocket support for real-time answers</li> </ul>"},{"location":"reference/architecture/#3-enhanced-nlp","title":"3. Enhanced NLP","text":"<ul> <li>Proper NER: spaCy or Hugging Face transformers for entity extraction</li> <li>Text Summarization: Automatic speech summarization</li> <li>Topic Modeling: LDA or BERTopic for theme discovery</li> <li>Fact Extraction: Structured information extraction</li> </ul>"},{"location":"reference/architecture/#4-deployment-scale","title":"4. Deployment &amp; Scale","text":"<ul> <li>Kubernetes: Container orchestration</li> <li>Auto-scaling: Based on request volume</li> <li>Multi-region: Global deployment</li> <li>CDN: Static asset delivery</li> </ul>"},{"location":"reference/architecture/#development-workflow","title":"Development Workflow","text":"<pre><code>graph LR\n    Dev[Local Development] --&gt;|Test| Test[pytest]\n    Test --&gt;|Lint| Lint[black, flake8, mypy]\n    Lint --&gt;|Commit| Git[Git Push]\n    Git --&gt;|Trigger| CI[GitHub Actions]\n    CI --&gt;|Build| Docker[Docker Build]\n    Docker --&gt;|Deploy| Env[Render/Azure]</code></pre>"},{"location":"reference/architecture/#references","title":"References","text":"<ul> <li>FastAPI Documentation</li> <li>Transformers Documentation</li> <li>ChromaDB Documentation</li> <li>LangChain Documentation</li> <li>Docker Best Practices</li> </ul> <p>Last Updated: October 2025 Version: 0.1.0 Maintainer: Kristiyan Bonev</p>"},{"location":"reference/configuration/","title":"Configuration Guide","text":"<p>This project uses Pydantic Settings for type-safe, environment-based configuration. This is the industry-standard approach for modern Python applications, especially those deployed to cloud platforms like Azure.</p>"},{"location":"reference/configuration/#configuration-architecture","title":"Configuration Architecture","text":""},{"location":"reference/configuration/#core-components","title":"Core Components","text":"<ol> <li><code>src/config.py</code> - Central configuration module with <code>Settings</code> class</li> <li><code>.env</code> file - Environment variables for local/cloud deployment</li> <li>Validation - Automatic type checking and validation via Pydantic</li> </ol>"},{"location":"reference/configuration/#benefits","title":"Benefits","text":"<ul> <li>\u2705 Type-safe - Compile-time checking of configuration values</li> <li>\u2705 Environment-aware - Different configs for dev/staging/prod</li> <li>\u2705 Cloud-friendly - Works seamlessly with Azure, AWS, GCP</li> <li>\u2705 Validated - Invalid configs fail fast with clear error messages</li> <li>\u2705 Documented - Self-documenting with type hints and descriptions</li> </ul>"},{"location":"reference/configuration/#quick-start","title":"Quick Start","text":""},{"location":"reference/configuration/#1-create-your-env-file","title":"1. Create Your <code>.env</code> File","text":"<p>Copy the example file:</p> <pre><code>cp .env.example .env\n</code></pre>"},{"location":"reference/configuration/#2-set-your-api-key","title":"2. Set Your API Key","text":"<p>Edit <code>.env</code> and add your Gemini API key:</p> <pre><code>GEMINI_API_KEY=your_actual_api_key_here\n</code></pre> <p>Get a free key at: https://ai.google.dev/</p>"},{"location":"reference/configuration/#3-run-the-application","title":"3. Run the Application","text":"<pre><code>uv run uvicorn src.api:app --host 0.0.0.0 --port 8000 --reload\n</code></pre> <p>The app will automatically: - Load settings from <code>.env</code> - Validate all configuration values - Initialize services with configured parameters - Display startup configuration in logs</p>"},{"location":"reference/configuration/#configuration-options","title":"Configuration Options","text":""},{"location":"reference/configuration/#application-settings","title":"Application Settings","text":"<pre><code>APP_NAME=\"Trump Speeches NLP Chatbot API\"\nAPP_VERSION=\"0.1.0\"\nENVIRONMENT=\"development\"  # development, staging, production\nLOG_LEVEL=\"INFO\"           # DEBUG, INFO, WARNING, ERROR, CRITICAL\n</code></pre>"},{"location":"reference/configuration/#llm-provider","title":"LLM Provider","text":"<p>Configure which LLM to use for answer generation:</p> <pre><code>LLM_PROVIDER=\"gemini\"      # gemini, openai, anthropic, none\nLLM_ENABLED=\"true\"\n</code></pre>"},{"location":"reference/configuration/#gemini-configuration","title":"Gemini Configuration","text":"<pre><code>GEMINI_API_KEY=\"your_key\"\nGEMINI_MODEL_NAME=\"gemini-2.5-flash\"  # or gemini-1.5-pro\nGEMINI_TEMPERATURE=\"0.3\"              # 0.0-1.0 (lower = more focused)\nGEMINI_MAX_OUTPUT_TOKENS=\"1024\"\n</code></pre>"},{"location":"reference/configuration/#openai-configuration-future","title":"OpenAI Configuration (Future)","text":"<pre><code>OPENAI_API_KEY=\"your_key\"\nOPENAI_MODEL_NAME=\"gpt-4o-mini\"\n</code></pre>"},{"location":"reference/configuration/#ml-models","title":"ML Models","text":"<p>Configure which models to use for different tasks:</p> <pre><code># Sentiment Analysis\nSENTIMENT_MODEL_NAME=\"ProsusAI/finbert\"\n\n# Embeddings for RAG\nEMBEDDING_MODEL_NAME=\"all-mpnet-base-v2\"\n\n# Re-ranking\nRERANKER_MODEL_NAME=\"cross-encoder/ms-marco-MiniLM-L-6-v2\"\n</code></pre>"},{"location":"reference/configuration/#rag-configuration","title":"RAG Configuration","text":"<pre><code># ChromaDB\nCHROMADB_PERSIST_DIRECTORY=\"./data/chromadb\"\nCHROMADB_COLLECTION_NAME=\"speeches\"\n\n# Text Chunking\nCHUNK_SIZE=\"2048\"\nCHUNK_OVERLAP=\"150\"\n\n# Search\nDEFAULT_TOP_K=\"5\"\nUSE_RERANKING=\"true\"\nUSE_HYBRID_SEARCH=\"true\"\n</code></pre>"},{"location":"reference/configuration/#data-directories","title":"Data Directories","text":"<pre><code>DATA_ROOT_DIRECTORY=\"./data\"\nSPEECHES_DIRECTORY=\"./data/Donald Trump Rally Speeches\"\n</code></pre>"},{"location":"reference/configuration/#api-settings","title":"API Settings","text":"<pre><code>API_HOST=\"0.0.0.0\"\nAPI_PORT=\"8000\"\nAPI_RELOAD=\"false\"  # true in development\nCORS_ORIGINS=\"*\"    # comma-separated list in production\n</code></pre>"},{"location":"reference/configuration/#environment-specific-configs","title":"Environment-Specific Configs","text":""},{"location":"reference/configuration/#development","title":"Development","text":"<pre><code>ENVIRONMENT=\"development\"\nLOG_LEVEL=\"DEBUG\"\nAPI_RELOAD=\"true\"\nCORS_ORIGINS=\"*\"\n</code></pre>"},{"location":"reference/configuration/#production-azure","title":"Production (Azure)","text":"<pre><code>ENVIRONMENT=\"production\"\nLOG_LEVEL=\"INFO\"\nAPI_RELOAD=\"false\"\nCORS_ORIGINS=\"https://yourdomain.com\"\n</code></pre>"},{"location":"reference/configuration/#using-configuration-in-code","title":"Using Configuration in Code","text":""},{"location":"reference/configuration/#accessing-settings","title":"Accessing Settings","text":"<pre><code>from src.config import get_settings\n\nsettings = get_settings()\n\n# Access values\nprint(settings.gemini_api_key)\nprint(settings.chunk_size)\nprint(settings.log_level)\n</code></pre>"},{"location":"reference/configuration/#type-safe-access","title":"Type-Safe Access","text":"<pre><code># All settings are type-checked\nsettings.chunk_size  # int\nsettings.gemini_temperature  # float\nsettings.use_reranking  # bool\nsettings.llm_provider  # Literal[\"gemini\", \"openai\", \"anthropic\", \"none\"]\n</code></pre>"},{"location":"reference/configuration/#helper-methods","title":"Helper Methods","text":"<pre><code># Check if LLM is configured\nif settings.is_llm_configured():\n    api_key = settings.get_llm_api_key()\n    model = settings.get_llm_model_name()\n\n# Get Path objects\nspeeches_path = settings.get_speeches_path()\nchromadb_path = settings.get_chromadb_path()\n\n# Setup logging\nsettings.setup_logging()\n</code></pre>"},{"location":"reference/configuration/#logging-configuration","title":"Logging Configuration","text":"<p>The project uses <code>src/logging_config.py</code> for production-ready logging with automatic format detection.</p>"},{"location":"reference/configuration/#log-levels","title":"Log Levels","text":"<ul> <li>DEBUG: Detailed diagnostic information for troubleshooting</li> <li>INFO: Important application events (default, recommended for production)</li> <li>WARNING: Unexpected but recoverable situations</li> <li>ERROR: Application errors requiring attention</li> <li>CRITICAL: System-critical failures</li> </ul>"},{"location":"reference/configuration/#log-formats","title":"Log Formats","text":""},{"location":"reference/configuration/#development-colored","title":"Development (Colored)","text":"<p>Automatically enabled when <code>ENVIRONMENT=development</code>:</p> <pre><code>2025-11-04 12:34:56 | INFO     | src.api              | Application startup complete\n2025-11-04 12:34:57 | DEBUG    | src.rag_service      | Performing hybrid search\n</code></pre> <ul> <li>ANSI colors by level (green=INFO, red=ERROR, etc.)</li> <li>Human-readable timestamps</li> <li>Module names right-aligned</li> </ul>"},{"location":"reference/configuration/#production-json","title":"Production (JSON)","text":"<p>Automatically enabled when <code>ENVIRONMENT=production</code>:</p> <pre><code>{\"timestamp\": \"2025-11-04 12:34:56\", \"level\": \"INFO\", \"name\": \"src.api\", \"message\": \"Application startup complete\"}\n{\"timestamp\": \"2025-11-04 12:34:57\", \"level\": \"DEBUG\", \"name\": \"src.rag_service\", \"message\": \"Performing hybrid search\"}\n</code></pre> <ul> <li>Machine-parseable JSON</li> <li>Compatible with Azure Application Insights, CloudWatch, ELK stack</li> <li>Automatic exception field for errors</li> </ul>"},{"location":"reference/configuration/#changing-log-settings","title":"Changing Log Settings","text":"<p>Edit <code>.env</code>:</p> <pre><code># Log level\nLOG_LEVEL=\"INFO\"   # Recommended for production\nLOG_LEVEL=\"DEBUG\"  # Verbose for debugging\n\n# Environment (affects format)\nENVIRONMENT=\"development\"  # Colored logs\nENVIRONMENT=\"production\"   # JSON logs\n</code></pre> <p>The logging system automatically: - Detects environment and chooses appropriate format - Suppresses noisy third-party loggers (chromadb, httpx, transformers) - Configures uvicorn logs - Filters ChromaDB telemetry errors</p> <p>For detailed logging documentation, see <code>docs/howto/logging.md</code>.</p>"},{"location":"reference/configuration/#azure-deployment","title":"Azure Deployment","text":"<p>Azure App Service automatically loads environment variables. Configure them in:</p> <ol> <li>Azure Portal: App Service \u2192 Configuration \u2192 Application Settings</li> <li>Azure CLI:    <pre><code>az webapp config appsettings set --name myapp --resource-group mygroup \\\n  --settings GEMINI_API_KEY=\"your_key\" LOG_LEVEL=\"INFO\"\n</code></pre></li> </ol>"},{"location":"reference/configuration/#docker-deployment","title":"Docker Deployment","text":""},{"location":"reference/configuration/#using-env-file","title":"Using .env file","text":"<pre><code>docker run --env-file .env -p 8000:8000 myapp\n</code></pre>"},{"location":"reference/configuration/#using-environment-variables","title":"Using environment variables","text":"<pre><code>docker run \\\n  -e GEMINI_API_KEY=\"your_key\" \\\n  -e LOG_LEVEL=\"INFO\" \\\n  -p 8000:8000 \\\n  myapp\n</code></pre>"},{"location":"reference/configuration/#docker-compose","title":"Docker Compose","text":"<pre><code>services:\n  api:\n    build: .\n    environment:\n      - GEMINI_API_KEY=${GEMINI_API_KEY}\n      - LOG_LEVEL=${LOG_LEVEL:-INFO}\n    env_file:\n      - .env\n    ports:\n      - \"8000:8000\"\n</code></pre>"},{"location":"reference/configuration/#validation","title":"Validation","text":"<p>Pydantic automatically validates configuration:</p>"},{"location":"reference/configuration/#example-validation-errors","title":"Example Validation Errors","text":"<pre><code># Invalid log level\nLOG_LEVEL=\"INVALID\"\n# \u274c Error: Invalid log level. Must be one of: DEBUG, INFO, WARNING, ERROR, CRITICAL\n\n# Invalid chunk size\nCHUNK_SIZE=\"not_a_number\"\n# \u274c Error: Input should be a valid integer\n\n# Missing required API key (when LLM enabled)\nLLM_ENABLED=\"true\"\nGEMINI_API_KEY=\"\"\n# \u274c Error: API key appears to be too short\n</code></pre>"},{"location":"reference/configuration/#best-practices","title":"Best Practices","text":"<ol> <li>Never commit <code>.env</code> - Add to <code>.gitignore</code></li> <li>Use <code>.env.example</code> - Document all available options</li> <li>Validate early - Settings load at startup, fail fast</li> <li>Environment-specific - Different configs for dev/prod</li> <li>Security - Use Azure Key Vault for sensitive values in production</li> <li>Logging - Use appropriate log levels for each environment</li> </ol>"},{"location":"reference/configuration/#troubleshooting","title":"Troubleshooting","text":""},{"location":"reference/configuration/#settings-not-loading","title":"Settings not loading","text":"<p>Check: 1. <code>.env</code> file exists in project root 2. File encoding is UTF-8 3. No syntax errors in <code>.env</code></p>"},{"location":"reference/configuration/#invalid-configuration","title":"Invalid configuration","text":"<p>Check logs at startup: <pre><code>ERROR: ValidationError: 1 validation error for Settings\n  Invalid log level. Must be one of: DEBUG, INFO, WARNING, ERROR, CRITICAL\n</code></pre></p>"},{"location":"reference/configuration/#api-key-issues","title":"API key issues","text":"<pre><code># Check if API key is set\npython -c \"from src.config import get_settings; print(get_settings().gemini_api_key)\"\n</code></pre>"},{"location":"reference/configuration/#migration-from-old-code","title":"Migration from Old Code","text":"<p>If you were using environment variables directly:</p> <p>Before: <pre><code>import os\napi_key = os.getenv(\"GEMINI_API_KEY\")\n</code></pre></p> <p>After: <pre><code>from src.config import get_settings\nsettings = get_settings()\napi_key = settings.gemini_api_key  # Type-safe!\n</code></pre></p>"},{"location":"reference/configuration/#further-reading","title":"Further Reading","text":"<ul> <li>Pydantic Settings Docs</li> <li>12-Factor App Config</li> <li>Azure App Service Configuration</li> </ul>"},{"location":"reference/rag-features/","title":"RAG System Features","text":""},{"location":"reference/rag-features/#overview","title":"Overview","text":"<p>This project implements a production-grade Retrieval-Augmented Generation (RAG) system for question-answering over a corpus of 35 political speeches (300,000+ words).</p>"},{"location":"reference/rag-features/#core-architecture","title":"Core Architecture","text":""},{"location":"reference/rag-features/#vector-database","title":"Vector Database","text":"<ul> <li>ChromaDB with persistent storage</li> <li>MPNet embeddings (768 dimensions) for semantic understanding</li> <li>Hybrid search combining dense embeddings with BM25 sparse retrieval</li> <li>Cross-encoder reranking for precision optimization</li> </ul>"},{"location":"reference/rag-features/#llm-integration","title":"LLM Integration","text":"<ul> <li>Google Gemini (gemini-2.5-flash) for answer generation</li> <li>Context-aware prompt engineering</li> <li>Entity-focused generation for targeted queries</li> <li>Fallback extraction for robustness</li> </ul>"},{"location":"reference/rag-features/#advanced-features","title":"Advanced Features","text":"<ul> <li>Multi-factor confidence scoring</li> <li>Entity extraction and analytics</li> <li>Sentiment analysis for entities</li> <li>Co-occurrence analysis</li> <li>Source attribution with citations</li> </ul>"},{"location":"reference/rag-features/#key-features","title":"Key Features","text":""},{"location":"reference/rag-features/#1-intelligent-question-answering","title":"1. Intelligent Question Answering","text":"<p>Ask natural language questions and receive AI-generated answers with supporting evidence.</p> <p>Example: <pre><code>response = rag.ask(\"What economic policies were discussed?\", top_k=5)\n</code></pre></p> <p>Response includes: - Generated answer from Gemini - 5 supporting context chunks - Confidence score with explanation - Source document attribution - Entity statistics (if applicable)</p>"},{"location":"reference/rag-features/#2-multi-factor-confidence-scoring","title":"2. Multi-Factor Confidence Scoring","text":"<p>Sophisticated confidence assessment considering: - Retrieval Quality (40%) \u2014 Semantic similarity of retrieved chunks - Consistency (25%) \u2014 Low variance in scores = higher confidence - Coverage (20%) \u2014 Number of supporting chunks - Entity Coverage (15%) \u2014 For entity queries, mention frequency</p> <p>Example output: <pre><code>{\n  \"confidence\": \"high\",\n  \"confidence_score\": 0.87,\n  \"confidence_explanation\": \"Overall confidence is HIGH (score: 0.87) based on excellent semantic match (similarity: 0.91), very consistent results (consistency: 0.93), 5 supporting context chunks\",\n  \"confidence_factors\": {\n    \"retrieval_score\": 0.91,\n    \"consistency\": 0.93,\n    \"chunk_coverage\": 5,\n    \"entity_coverage\": 0.84\n  }\n}\n</code></pre></p>"},{"location":"reference/rag-features/#3-entity-analytics","title":"3. Entity Analytics","text":"<p>Automatic entity detection with comprehensive statistics:</p> <ul> <li>Mention counts across entire corpus</li> <li>Speech coverage \u2014 which documents mention the entity</li> <li>Sentiment analysis \u2014 average sentiment toward entity</li> <li>Co-occurrence analysis \u2014 most common associated terms</li> </ul> <p>Example output: <pre><code>{\n  \"entity_statistics\": {\n    \"Biden\": {\n      \"mention_count\": 524,\n      \"speech_count\": 30,\n      \"corpus_percentage\": 25.03,\n      \"speeches\": [\"OhioSep21_2020.txt\", \"BemidjiSep18_2020.txt\", ...],\n      \"sentiment\": {\n        \"average_score\": -0.15,\n        \"classification\": \"Neutral\",\n        \"sample_size\": 50\n      },\n      \"associations\": [\"people\", \"our\", \"right\", \"about\", \"say\"]\n    }\n  }\n}\n</code></pre></p>"},{"location":"reference/rag-features/#4-hybrid-search","title":"4. Hybrid Search","text":"<p>Combines semantic and keyword search for optimal retrieval:</p> <ul> <li>Semantic search \u2014 Dense embeddings capture meaning and context</li> <li>BM25 keyword search \u2014 Ensures exact term matches aren't missed</li> <li>Cross-encoder reranking \u2014 Final precision optimization</li> <li>Configurable weights \u2014 Adjust semantic vs keyword importance</li> </ul>"},{"location":"reference/rag-features/#5-optimized-chunking","title":"5. Optimized Chunking","text":"<ul> <li>2048 character chunks (~512-768 tokens) for complete context</li> <li>150 character overlap to preserve continuity</li> <li>Smart splitting with RecursiveCharacterTextSplitter</li> <li>Maintains coherent context boundaries</li> </ul>"},{"location":"reference/rag-features/#api-usage","title":"API Usage","text":""},{"location":"reference/rag-features/#basic-question","title":"Basic Question","text":"<pre><code>import requests\n\nresponse = requests.post(\n    \"http://localhost:8000/rag/ask\",\n    json={\"question\": \"What was said about the economy?\", \"top_k\": 5}\n)\n\nresult = response.json()\nprint(result[\"answer\"])\nprint(f\"Confidence: {result['confidence']} ({result['confidence_score']:.2f})\")\n</code></pre>"},{"location":"reference/rag-features/#entity-query","title":"Entity Query","text":"<pre><code>response = requests.post(\n    \"http://localhost:8000/rag/ask\",\n    json={\"question\": \"What did Trump say about Biden?\", \"top_k\": 10}\n)\n\nresult = response.json()\n\n# View entity statistics\nif \"entity_statistics\" in result:\n    for entity, stats in result[\"entity_statistics\"].items():\n        print(f\"\\n{entity}:\")\n        print(f\"  Mentions: {stats['mention_count']}\")\n        print(f\"  Sentiment: {stats['sentiment']['classification']}\")\n        print(f\"  Associated: {', '.join(stats['associations'][:3])}\")\n</code></pre>"},{"location":"reference/rag-features/#semantic-search","title":"Semantic Search","text":"<pre><code>response = requests.post(\n    \"http://localhost:8000/rag/search\",\n    json={\"query\": \"immigration policy\", \"top_k\": 5}\n)\n\nresults = response.json()[\"results\"]\nfor i, result in enumerate(results, 1):\n    print(f\"\\n{i}. Source: {result['source']}\")\n    print(f\"   Similarity: {result['similarity']:.3f}\")\n    print(f\"   Preview: {result['text'][:100]}...\")\n</code></pre>"},{"location":"reference/rag-features/#configuration","title":"Configuration","text":""},{"location":"reference/rag-features/#ragservice-parameters","title":"RAGService Parameters","text":"<pre><code>from src.rag_service import RAGService\n\nrag = RAGService(\n    collection_name=\"speeches\",\n    persist_directory=\"./data/chromadb\",\n    embedding_model=\"all-mpnet-base-v2\",      # 768d embeddings\n    reranker_model=\"cross-encoder/ms-marco-MiniLM-L-6-v2\",\n    chunk_size=2048,                          # ~512-768 tokens\n    chunk_overlap=150,                        # ~100-150 tokens\n    use_llm=True,                             # Enable Gemini\n    use_reranking=True,                       # Enable cross-encoder\n    use_hybrid_search=True                    # Enable BM25 + semantic\n)\n</code></pre>"},{"location":"reference/rag-features/#api-endpoint-configuration","title":"API Endpoint Configuration","text":"<ul> <li>Default <code>top_k</code>: 5 chunks</li> <li>Maximum <code>top_k</code>: 15 chunks</li> <li>Increase for complex/entity queries</li> </ul>"},{"location":"reference/rag-features/#performance","title":"Performance","text":""},{"location":"reference/rag-features/#first-request","title":"First Request","text":"<ul> <li>~30-60 seconds (model downloads + document indexing)</li> <li>Downloads ~1-2 GB of models (one-time)</li> </ul>"},{"location":"reference/rag-features/#subsequent-requests","title":"Subsequent Requests","text":"<ul> <li>~1-3 seconds for typical queries</li> <li>~2-5 seconds for entity analytics (sentiment analysis)</li> </ul>"},{"location":"reference/rag-features/#optimization-opportunities","title":"Optimization Opportunities","text":"<ul> <li>Cache entity statistics</li> <li>Pre-compute embeddings</li> <li>Async sentiment analysis</li> <li>Redis for query caching</li> </ul>"},{"location":"reference/rag-features/#technical-details","title":"Technical Details","text":""},{"location":"reference/rag-features/#models-used","title":"Models Used","text":"<ul> <li>Embeddings: <code>sentence-transformers/all-mpnet-base-v2</code> (768d)</li> <li>Reranker: <code>cross-encoder/ms-marco-MiniLM-L-6-v2</code></li> <li>LLM: Google Gemini 2.5 Flash</li> <li>Sentiment: ProsusAI/finbert</li> </ul>"},{"location":"reference/rag-features/#database","title":"Database","text":"<ul> <li>ChromaDB 0.5.0 with SQLite persistence</li> <li>Vector index: HNSW for efficient similarity search</li> <li>Metadata filtering: Source, chunk index, timestamps</li> </ul>"},{"location":"reference/rag-features/#prompt-engineering","title":"Prompt Engineering","text":"<ul> <li>Context-limited to 4000 characters max</li> <li>Source attribution in context</li> <li>Entity-focused instructions when entities detected</li> <li>Structured output format</li> <li>Safety settings for political content</li> </ul>"},{"location":"reference/rag-features/#limitations-future-work","title":"Limitations &amp; Future Work","text":""},{"location":"reference/rag-features/#current-limitations","title":"Current Limitations","text":"<ul> <li>Entity extraction uses simple heuristics (capitalization)</li> <li>Sentiment analysis may show neutral for complex political text</li> <li>No query caching (every request recomputes)</li> <li>Synchronous processing (no async optimization)</li> </ul>"},{"location":"reference/rag-features/#future-enhancements","title":"Future Enhancements","text":"<ul> <li>Integrate proper NER (spaCy or Hugging Face)</li> <li>Add query caching layer (Redis)</li> <li>Implement async processing</li> <li>Add temporal analysis (sentiment over time)</li> <li>Entity relationship graphs</li> <li>Fine-tune embeddings on domain data</li> </ul>"},{"location":"reference/rag-features/#migration","title":"Migration","text":"<p>If you're upgrading from a previous version with different embeddings:</p> <pre><code>poetry run python scripts/migrate_rag_embeddings.py\n</code></pre> <p>This will: 1. Clear existing ChromaDB collection 2. Reload documents with new embeddings 3. Re-index all 35 speeches (~1082 chunks)</p>"},{"location":"reference/rag-features/#testing","title":"Testing","text":"<pre><code># Run RAG tests\npoetry run pytest tests/test_rag_service.py -v\n\n# Run all tests\npoetry run pytest -v\n</code></pre>"},{"location":"reference/rag-features/#documentation","title":"Documentation","text":"<ul> <li>API Reference: http://localhost:8000/docs</li> <li>Quick Start: <code>docs/QUICKSTART.md</code></li> <li>Deployment: <code>docs/DEPLOYMENT.md</code></li> <li>Testing: <code>docs/TESTING.md</code></li> </ul> <p>This RAG system demonstrates production-ready AI engineering with vector databases, LLM integration, and sophisticated retrieval techniques.</p>"}]}